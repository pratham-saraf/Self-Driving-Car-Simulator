{
 "cells": [
  {
   "attachments": {
    "81e2707f-27e3-4431-a9db-c2bb3e73cfbd.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c965ba05",
   "metadata": {
    "papermill": {
     "duration": 0.010968,
     "end_time": "2022-08-30T21:01:43.564434",
     "exception": false,
     "start_time": "2022-08-30T21:01:43.553466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Automating car driving using CNNs\n",
    "[Link to reference research paper](https://developer.nvidia.com/blog/deep-learning-self-driving-cars/)\n",
    "\n",
    "Some call it a technical marvel. Some people call it a frenzy. Autonomous vehicles are constantly in the headlines. These self-driving cars, which are designed to carry people from point A to point B without the need for human involvement, are projected to boost mobility, reduce traffic congestion and fuel consumption, and create safer highways.\n",
    "\n",
    "> ### The DAVE-2 System\n",
    "> \n",
    "> It relies on the three cameras that are mounted behind the windshield of the data-acquisition car, and timestamped video from the cameras is captured simultaneously with the steering angle applied by the human driver. The steering command is obtained by tapping into the vehicles Controller Area Network (CAN) bus. In order to make our system independent of the car geometry, we represent the steering command as 1/r, where r is the turning radius in meters. We use 1/r instead of r to prevent a singularity when driving straight (the turning radius for driving straight is infinity). 1/r smoothly transitions through zero from left turns (negative values) to right turns (positive values).\n",
    "\n",
    "Model used is based on \n",
    "<br>\n",
    "![image.png](attachment:81e2707f-27e3-4431-a9db-c2bb3e73cfbd.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f73ed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:43.584927Z",
     "iopub.status.busy": "2022-08-30T21:01:43.584332Z",
     "iopub.status.idle": "2022-08-30T21:01:45.819380Z",
     "shell.execute_reply": "2022-08-30T21:01:45.818187Z"
    },
    "papermill": {
     "duration": 2.248315,
     "end_time": "2022-08-30T21:01:45.822501",
     "exception": false,
     "start_time": "2022-08-30T21:01:43.574186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "tqdm.monitor_interval = 0\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "# !pip install tensorboard-logger\n",
    "# from tensorboard_logger import configure, log_value\n",
    "from torch.utils.data import Dataset\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6fdde",
   "metadata": {
    "papermill": {
     "duration": 0.005611,
     "end_time": "2022-08-30T21:01:45.834221",
     "exception": false,
     "start_time": "2022-08-30T21:01:45.828610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lets start by importing the Required modules\n",
    "this version of code uses *Pytorch with cuda* to utilise the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cd2baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:45.848414Z",
     "iopub.status.busy": "2022-08-30T21:01:45.847724Z",
     "iopub.status.idle": "2022-08-30T21:01:45.860187Z",
     "shell.execute_reply": "2022-08-30T21:01:45.858964Z"
    },
    "papermill": {
     "duration": 0.021991,
     "end_time": "2022-08-30T21:01:45.862539",
     "exception": false,
     "start_time": "2022-08-30T21:01:45.840548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        self.conv_0 = nn.Conv2d(3, 24, 5, stride=2)\n",
    "        self.conv_1 = nn.Conv2d(24, 36, kernel_size=5, stride=2)\n",
    "        self.conv_2 = nn.Conv2d(36, 48, kernel_size=5, stride=2) #384 kernels, size 3x3\n",
    "        self.conv_3 = nn.Conv2d(48, 64, kernel_size=3) # 384 kernels size 3x3\n",
    "        self.conv_4 = nn.Conv2d(64, 64, kernel_size=3) # 256 kernels, size 3x3\n",
    "\n",
    "        self.fc0 = nn.Linear(1152, 100)\n",
    "        self.fc1 = nn.Linear(100,50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input/127.5-1.0\n",
    "        input = self.elu(self.conv_0(input))\n",
    "        input = self.elu(self.conv_1(input))\n",
    "        input = self.elu(self.conv_2(input))\n",
    "        input = self.elu(self.conv_3(input))\n",
    "        input = self.elu(self.conv_4(input))\n",
    "        input = self.dropout(input)\n",
    "\n",
    "        input = input.flatten()\n",
    "        input = self.elu(self.fc0(input))\n",
    "        input = self.elu(self.fc1(input))\n",
    "        input = self.elu(self.fc2(input))\n",
    "        input = self.fc3(input)\n",
    "\n",
    "        return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4cb82f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:45.875999Z",
     "iopub.status.busy": "2022-08-30T21:01:45.875572Z",
     "iopub.status.idle": "2022-08-30T21:01:49.156931Z",
     "shell.execute_reply": "2022-08-30T21:01:49.155932Z"
    },
    "papermill": {
     "duration": 3.290913,
     "end_time": "2022-08-30T21:01:49.159215",
     "exception": false,
     "start_time": "2022-08-30T21:01:45.868302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_Model(\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (conv_0): Conv2d(3, 24, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv_1): Conv2d(24, 36, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv_2): Conv2d(36, 48, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv_3): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv_4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc0): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc1): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN_Model()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf59638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.171429Z",
     "iopub.status.busy": "2022-08-30T21:01:49.170429Z",
     "iopub.status.idle": "2022-08-30T21:01:49.179045Z",
     "shell.execute_reply": "2022-08-30T21:01:49.178189Z"
    },
    "papermill": {
     "duration": 0.016427,
     "end_time": "2022-08-30T21:01:49.180963",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.164536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Features(Dataset):\n",
    "    def __init__(self, path_to_csv):\n",
    "        path_to_csv = os.path.join(path_to_csv, 'driving_log.csv')\n",
    "        print(path_to_csv)\n",
    "        self.csv_data = self.load_csv_file(path_to_csv)\n",
    "\n",
    "    def load_csv_file(self, path_to_csv):\n",
    "        data = []\n",
    "        with open(path_to_csv, 'r') as csvfile:\n",
    "            data_reader = csv.reader(csvfile, delimiter=',')\n",
    "            for row in data_reader:\n",
    "                data.append(row)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def get_csv_data(self):\n",
    "        return self.csv_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        data_entry = self.csv_data[i]\n",
    "\n",
    "        to_return = {\n",
    "            'img_center_pth': data_entry[0],\n",
    "            'img_left_pth': data_entry[1],\n",
    "            'img_right_pth': data_entry[2],\n",
    "            'steering_angle': data_entry[3],\n",
    "            'throttle': data_entry[4],\n",
    "            'brake': data_entry[5],\n",
    "            'speed': data_entry[6]\n",
    "        }\n",
    "\n",
    "        return to_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105f01c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.192560Z",
     "iopub.status.busy": "2022-08-30T21:01:49.192002Z",
     "iopub.status.idle": "2022-08-30T21:01:49.329139Z",
     "shell.execute_reply": "2022-08-30T21:01:49.327729Z"
    },
    "papermill": {
     "duration": 0.145422,
     "end_time": "2022-08-30T21:01:49.331500",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.186078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/car-simulator-dataset/driving_log.csv\n"
     ]
    }
   ],
   "source": [
    " dataset = Features(\"../input/car-simulator-dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fece8a",
   "metadata": {
    "papermill": {
     "duration": 0.004904,
     "end_time": "2022-08-30T21:01:49.341896",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.336992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We need to split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26473394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.354825Z",
     "iopub.status.busy": "2022-08-30T21:01:49.353358Z",
     "iopub.status.idle": "2022-08-30T21:01:49.363881Z",
     "shell.execute_reply": "2022-08-30T21:01:49.362918Z"
    },
    "papermill": {
     "duration": 0.019029,
     "end_time": "2022-08-30T21:01:49.365855",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.346826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data , val_data = torch.utils.data.dataset.random_split(dataset,[train_size,val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca5e8e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.377640Z",
     "iopub.status.busy": "2022-08-30T21:01:49.377366Z",
     "iopub.status.idle": "2022-08-30T21:01:49.382479Z",
     "shell.execute_reply": "2022-08-30T21:01:49.381318Z"
    },
    "papermill": {
     "duration": 0.01518,
     "end_time": "2022-08-30T21:01:49.386114",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.370934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x7fc38ce57910>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ff09b",
   "metadata": {
    "papermill": {
     "duration": 0.005166,
     "end_time": "2022-08-30T21:01:49.396132",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.390966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## **Augmentation**\n",
    "After selecting the final set of frames, we augment the data by adding artificial shifts and rotations to teach the network how to recover from a poor position or orientation. The magnitude of these perturbations is chosen randomly from a normal distribution. The distribution has zero mean, and the standard deviation is twice the standard deviation that we measured with human drivers. Artificially augmenting the data does add undesirable artifacts as the magnitude increases (as mentioned previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e13a009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.407902Z",
     "iopub.status.busy": "2022-08-30T21:01:49.407600Z",
     "iopub.status.idle": "2022-08-30T21:01:49.430873Z",
     "shell.execute_reply": "2022-08-30T21:01:49.429847Z"
    },
    "papermill": {
     "duration": 0.031978,
     "end_time": "2022-08-30T21:01:49.433086",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.401108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#template used from https://github.com/naokishibuya/car-behavioral-cloning/blob/master/utils.py\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3\n",
    "INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n",
    "\n",
    "\n",
    "def load_image(data_dir, image_file):\n",
    "    \"\"\"\n",
    "    Load RGB images from a file\n",
    "    \"\"\"\n",
    "    return mpimg.imread(os.path.join(data_dir, image_file.strip()))\n",
    "\n",
    "\n",
    "def crop(image):\n",
    "    \"\"\"\n",
    "    Crop the image (removing the sky at the top and the car front at the bottom)\n",
    "    \"\"\"\n",
    "    return image[60:-25, :, :] # remove the sky and the car front\n",
    "\n",
    "\n",
    "def resize(image):\n",
    "    \"\"\"\n",
    "    Resize the image to the input shape used by the network model\n",
    "    \"\"\"\n",
    "    return cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def rgb2yuv(image):\n",
    "    \"\"\"\n",
    "    Convert the image from RGB to YUV (This is what the NVIDIA model does)\n",
    "    \"\"\"\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    \"\"\"\n",
    "    Combine all preprocess functions into one\n",
    "    \"\"\"\n",
    "    image = crop(image)\n",
    "    image = resize(image)\n",
    "    image = rgb2yuv(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def choose_image(steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly choose an image from the center, left or right, and adjust\n",
    "    the steering angle.\n",
    "    \"\"\"\n",
    "    choice = np.random.choice(3)\n",
    "    if choice == 0:\n",
    "        return \"img_left_pth\", float(steering_angle) + 0.2\n",
    "    elif choice == 1:\n",
    "        return \"img_right_pth\", float(steering_angle) - 0.2\n",
    "    return \"img_center_pth\", float(steering_angle)\n",
    "\n",
    "\n",
    "def random_flip(image, steering_angle):\n",
    "    \"\"\"\n",
    "    Randomly flipt the image left <-> right, and adjust the steering angle.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "        steering_angle = -steering_angle\n",
    "    return image, steering_angle\n",
    "\n",
    "\n",
    "def random_translate(image, steering_angle, range_x, range_y):\n",
    "    \"\"\"\n",
    "    Randomly shift the image virtially and horizontally (translation).\n",
    "    \"\"\"\n",
    "    trans_x = range_x * (np.random.rand() - 0.5)\n",
    "    trans_y = range_y * (np.random.rand() - 0.5)\n",
    "    steering_angle += trans_x * 0.002\n",
    "    trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]])\n",
    "    height, width = image.shape[:2]\n",
    "    image = cv2.warpAffine(image, trans_m, (width, height))\n",
    "    return image, steering_angle\n",
    "\n",
    "\n",
    "def random_shadow(image):\n",
    "    \"\"\"\n",
    "    Generates and adds random shadow\n",
    "    \"\"\"\n",
    "    # (x1, y1) and (x2, y2) forms a line\n",
    "    # xm, ym gives all the locations of the image\n",
    "    x1, y1 = IMAGE_WIDTH * np.random.rand(), 0\n",
    "    x2, y2 = IMAGE_WIDTH * np.random.rand(), IMAGE_HEIGHT\n",
    "    xm, ym = np.mgrid[0:IMAGE_HEIGHT, 0:IMAGE_WIDTH]\n",
    "\n",
    "    # mathematically speaking, we want to set 1 below the line and zero otherwise\n",
    "    # Our coordinate is up side down.  So, the above the line:\n",
    "    # (ym-y1)/(xm-x1) > (y2-y1)/(x2-x1)\n",
    "    # as x2 == x1 causes zero-division problem, we'll write it in the below form:\n",
    "    # (ym-y1)*(x2-x1) - (y2-y1)*(xm-x1) > 0\n",
    "    mask = np.zeros_like(image[:, :, 1])\n",
    "    mask[np.where((ym - y1) * (x2 - x1) - (y2 - y1) * (xm - x1) > 0)] = 1\n",
    "\n",
    "    # choose which side should have shadow and adjust saturation\n",
    "    cond = mask == np.random.randint(2)\n",
    "    s_ratio = np.random.uniform(low=0.2, high=0.5)\n",
    "\n",
    "    # adjust Saturation in HLS(Hue, Light, Saturation)\n",
    "    hls = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "    hls[:, :, 1][cond] = hls[:, :, 1][cond] * s_ratio\n",
    "    return cv2.cvtColor(hls, cv2.COLOR_HLS2RGB)\n",
    "\n",
    "\n",
    "def random_brightness(image):\n",
    "    \"\"\"\n",
    "    Randomly adjust brightness of the image.\n",
    "    \"\"\"\n",
    "    # HSV (Hue, Saturation, Value) is also called HSB ('B' for Brightness).\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    ratio = 1.0 + 0.4 * (np.random.rand() - 0.5)\n",
    "    hsv[:,:,2] =  hsv[:,:,2] * ratio\n",
    "    return cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "\n",
    "def augument(data_dir, center, left, right, steering_angle, range_x=100, range_y=10):\n",
    "    \"\"\"\n",
    "    Generate an augumented image and adjust steering angle.\n",
    "    (The steering angle is associated with the center image)\n",
    "    \"\"\"\n",
    "    image, steering_angle = choose_image(data_dir, center, left, right, steering_angle)\n",
    "    image, steering_angle = random_flip(image, steering_angle)\n",
    "    image, steering_angle = random_translate(image, steering_angle, range_x, range_y)\n",
    "    image = random_shadow(image)\n",
    "    image = random_brightness(image)\n",
    "    return image, steering_angle\n",
    "\n",
    "\n",
    "def batch_generator(data_dir, image_paths, steering_angles, batch_size, is_training):\n",
    "    \"\"\"\n",
    "    Generate training image give image paths and associated steering angles\n",
    "    \"\"\"\n",
    "    images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS])\n",
    "    steers = np.empty(batch_size)\n",
    "    while True:\n",
    "        i = 0\n",
    "        for index in np.random.permutation(image_paths.shape[0]):\n",
    "            center, left, right = image_paths[index]\n",
    "            steering_angle = steering_angles[index]\n",
    "            # argumentation\n",
    "            if is_training and np.random.rand() < 0.6:\n",
    "                image, steering_angle = augument(data_dir, center, left, right, steering_angle)\n",
    "            else:\n",
    "                image = load_image(data_dir, center)\n",
    "            # add the image and steering angle to the batch\n",
    "            images[i] = preprocess(image)\n",
    "            steers[i] = steering_angle\n",
    "            i += 1\n",
    "            if i == batch_size:\n",
    "                break\n",
    "        yield images, steers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85de189",
   "metadata": {
    "papermill": {
     "duration": 0.00488,
     "end_time": "2022-08-30T21:01:49.442884",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.438004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "defined a function to rectify the image path in data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982c6517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.454283Z",
     "iopub.status.busy": "2022-08-30T21:01:49.454013Z",
     "iopub.status.idle": "2022-08-30T21:01:49.458830Z",
     "shell.execute_reply": "2022-08-30T21:01:49.457825Z"
    },
    "papermill": {
     "duration": 0.013013,
     "end_time": "2022-08-30T21:01:49.460783",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.447770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rectify(data,img_pth):\n",
    "    val = data[img_pth].split(\"\\\\\")[-1]\n",
    "    val = (\"../input/car-simulator-dataset/img/IMG/\"+val)\n",
    "    data[img_pth] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8336b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.472517Z",
     "iopub.status.busy": "2022-08-30T21:01:49.472202Z",
     "iopub.status.idle": "2022-08-30T21:01:49.482076Z",
     "shell.execute_reply": "2022-08-30T21:01:49.480885Z"
    },
    "papermill": {
     "duration": 0.017732,
     "end_time": "2022-08-30T21:01:49.483996",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.466264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model,dataset,num_samples):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    step = 0\n",
    "    val_loss = 0\n",
    "    count = 0\n",
    "    sampler = RandomSampler(dataset)\n",
    "    torch.manual_seed(0)\n",
    "    for sample_id in tqdm(sampler):\n",
    "        if step==num_samples:\n",
    "            break\n",
    "\n",
    "        data = dataset[sample_id]\n",
    "        img_pth, label = choose_image(data['steering_angle'])\n",
    "        rectify(data,img_pth)\n",
    "        \n",
    "        img = cv2.imread(data[img_pth])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = preprocess(img)\n",
    "        img, label = random_flip(img, label)\n",
    "        img, label = random_translate(img, label, 100, 10)\n",
    "        img = random_shadow(img)\n",
    "        img = random_brightness(img)\n",
    "        img = Variable(torch.cuda.FloatTensor([img]))\n",
    "        img = img.permute(0,3,1,2)\n",
    "        label = np.array([label]).astype(float)\n",
    "        label = Variable(torch.cuda.FloatTensor(label))\n",
    "\n",
    "        out_vec = model(img)\n",
    "\n",
    "        loss = criterion(out_vec,label)\n",
    "\n",
    "        batch_size = 4\n",
    "        val_loss += loss.data.item()\n",
    "        count += batch_size\n",
    "        step += 1\n",
    "\n",
    "    val_loss = val_loss / float(count)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd89b3",
   "metadata": {
    "papermill": {
     "duration": 0.005177,
     "end_time": "2022-08-30T21:01:49.494053",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.488876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training\n",
    "The model saves the file after every 5000 \n",
    "so that if GPU crashes we wont lose the models trained so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c33c875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-30T21:01:49.505939Z",
     "iopub.status.busy": "2022-08-30T21:01:49.505327Z",
     "iopub.status.idle": "2022-08-30T22:31:54.949674Z",
     "shell.execute_reply": "2022-08-30T22:31:54.948566Z"
    },
    "papermill": {
     "duration": 5405.453561,
     "end_time": "2022-08-30T22:31:54.952496",
     "exception": false,
     "start_time": "2022-08-30T21:01:49.498935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:207.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 0 | Step: 0 | Train Loss: 0.01387499 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:51, 40.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 0 | Step: 0 | Val Loss: 0.02626854\n",
      "Epoch: 0 | Iter: 100 | Step: 100 | Train Loss: 0.05328948 |\n",
      "Epoch: 0 | Iter: 200 | Step: 200 | Train Loss: 0.00243725 |\n",
      "Epoch: 0 | Iter: 300 | Step: 300 | Train Loss: 0.00855983 |\n",
      "Epoch: 0 | Iter: 400 | Step: 400 | Train Loss: 0.03181834 |\n",
      "Epoch: 0 | Iter: 500 | Step: 500 | Train Loss: 0.00558425 |\n",
      "Epoch: 0 | Iter: 600 | Step: 600 | Train Loss: 0.00276919 |\n",
      "Epoch: 0 | Iter: 700 | Step: 700 | Train Loss: 0.08707961 |\n",
      "Epoch: 0 | Iter: 800 | Step: 800 | Train Loss: 0.11517500 |\n",
      "Epoch: 0 | Iter: 900 | Step: 900 | Train Loss: 0.00115327 |\n",
      "Epoch: 0 | Iter: 1000 | Step: 1000 | Train Loss: 0.10835119 |\n",
      "Epoch: 0 | Iter: 1100 | Step: 1100 | Train Loss: 0.02124577 |\n",
      "Epoch: 0 | Iter: 1200 | Step: 1200 | Train Loss: 0.00291905 |\n",
      "Epoch: 0 | Iter: 1300 | Step: 1300 | Train Loss: 0.00238849 |\n",
      "Epoch: 0 | Iter: 1400 | Step: 1400 | Train Loss: 0.13998096 |\n",
      "Epoch: 0 | Iter: 1500 | Step: 1500 | Train Loss: 0.12938750 |\n",
      "Epoch: 0 | Iter: 1600 | Step: 1600 | Train Loss: 0.05816229 |\n",
      "Epoch: 0 | Iter: 1700 | Step: 1700 | Train Loss: 0.07714666 |\n",
      "Epoch: 0 | Iter: 1800 | Step: 1800 | Train Loss: 0.00079456 |\n",
      "Epoch: 0 | Iter: 1900 | Step: 1900 | Train Loss: 0.06002687 |\n",
      "Epoch: 0 | Iter: 2000 | Step: 2000 | Train Loss: 0.00647513 |\n",
      "Epoch: 0 | Iter: 2100 | Step: 2100 | Train Loss: 0.00074197 |\n",
      "Epoch: 0 | Iter: 2200 | Step: 2200 | Train Loss: 0.02122258 |\n",
      "Epoch: 0 | Iter: 2300 | Step: 2300 | Train Loss: 0.00502274 |\n",
      "Epoch: 0 | Iter: 2400 | Step: 2400 | Train Loss: 0.00044987 |\n",
      "Epoch: 0 | Iter: 2500 | Step: 2500 | Train Loss: 0.04606213 |\n",
      "Epoch: 0 | Iter: 2600 | Step: 2600 | Train Loss: 0.05470535 |\n",
      "Epoch: 0 | Iter: 2700 | Step: 2700 | Train Loss: 0.00032033 |\n",
      "Epoch: 0 | Iter: 2800 | Step: 2800 | Train Loss: 0.01394428 |\n",
      "Epoch: 0 | Iter: 2900 | Step: 2900 | Train Loss: 0.55781299 |\n",
      "Epoch: 0 | Iter: 3000 | Step: 3000 | Train Loss: 0.64901364 |\n",
      "Epoch: 0 | Iter: 3100 | Step: 3100 | Train Loss: 0.00808041 |\n",
      "Epoch: 0 | Iter: 3200 | Step: 3200 | Train Loss: 0.00532251 |\n",
      "Epoch: 0 | Iter: 3300 | Step: 3300 | Train Loss: 0.07681366 |\n",
      "Epoch: 0 | Iter: 3400 | Step: 3400 | Train Loss: 0.06741951 |\n",
      "Epoch: 0 | Iter: 3500 | Step: 3500 | Train Loss: 0.32226655 |\n",
      "Epoch: 0 | Iter: 3600 | Step: 3600 | Train Loss: 0.05254628 |\n",
      "Epoch: 0 | Iter: 3700 | Step: 3700 | Train Loss: 0.06833942 |\n",
      "Epoch: 0 | Iter: 3800 | Step: 3800 | Train Loss: 0.02244155 |\n",
      "Epoch: 0 | Iter: 3900 | Step: 3900 | Train Loss: 0.02513268 |\n",
      "Epoch: 0 | Iter: 4000 | Step: 4000 | Train Loss: 0.00000211 |\n",
      "Epoch: 0 | Iter: 4100 | Step: 4100 | Train Loss: 0.00577042 |\n",
      "Epoch: 0 | Iter: 4200 | Step: 4200 | Train Loss: 0.26159799 |\n",
      "Epoch: 0 | Iter: 4300 | Step: 4300 | Train Loss: 0.00019977 |\n",
      "Epoch: 0 | Iter: 4400 | Step: 4400 | Train Loss: 0.00636769 |\n",
      "Epoch: 0 | Iter: 4500 | Step: 4500 | Train Loss: 0.00239189 |\n",
      "Epoch: 0 | Iter: 4600 | Step: 4600 | Train Loss: 0.00090033 |\n",
      "Epoch: 0 | Iter: 4700 | Step: 4700 | Train Loss: 0.00098888 |\n",
      "Epoch: 0 | Iter: 4800 | Step: 4800 | Train Loss: 0.01765478 |\n",
      "Epoch: 0 | Iter: 4900 | Step: 4900 | Train Loss: 0.06427786 |\n",
      "Epoch: 0 | Iter: 5000 | Step: 5000 | Train Loss: 0.01271950 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:47, 40.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 5000 | Step: 5000 | Val Loss: 0.02219207\n",
      "Epoch: 0 | Iter: 5100 | Step: 5100 | Train Loss: 0.03013483 |\n",
      "Epoch: 0 | Iter: 5200 | Step: 5200 | Train Loss: 0.05486815 |\n",
      "Epoch: 0 | Iter: 5300 | Step: 5300 | Train Loss: 0.00100160 |\n",
      "Epoch: 0 | Iter: 5400 | Step: 5400 | Train Loss: 0.00593287 |\n",
      "Epoch: 0 | Iter: 5500 | Step: 5500 | Train Loss: 0.02824558 |\n",
      "Epoch: 0 | Iter: 5600 | Step: 5600 | Train Loss: 0.00001944 |\n",
      "Epoch: 0 | Iter: 5700 | Step: 5700 | Train Loss: 0.04061214 |\n",
      "Epoch: 0 | Iter: 5800 | Step: 5800 | Train Loss: 0.05195013 |\n",
      "Epoch: 0 | Iter: 5900 | Step: 5900 | Train Loss: 0.02007226 |\n",
      "Epoch: 0 | Iter: 6000 | Step: 6000 | Train Loss: 0.03741752 |\n",
      "Epoch: 0 | Iter: 6100 | Step: 6100 | Train Loss: 0.00191617 |\n",
      "Epoch: 0 | Iter: 6200 | Step: 6200 | Train Loss: 0.05777198 |\n",
      "Epoch: 0 | Iter: 6300 | Step: 6300 | Train Loss: 0.17270292 |\n",
      "Epoch: 0 | Iter: 6400 | Step: 6400 | Train Loss: 0.02666191 |\n",
      "Epoch: 0 | Iter: 6500 | Step: 6500 | Train Loss: 0.00156973 |\n",
      "Epoch: 0 | Iter: 6600 | Step: 6600 | Train Loss: 0.00022223 |\n",
      "Epoch: 0 | Iter: 6700 | Step: 6700 | Train Loss: 0.78812212 |\n",
      "Epoch: 0 | Iter: 6800 | Step: 6800 | Train Loss: 0.00182363 |\n",
      "Epoch: 0 | Iter: 6900 | Step: 6900 | Train Loss: 0.17367685 |\n",
      "Epoch: 0 | Iter: 7000 | Step: 7000 | Train Loss: 0.04679671 |\n",
      "Epoch: 0 | Iter: 7100 | Step: 7100 | Train Loss: 1.11063039 |\n",
      "Epoch: 0 | Iter: 7200 | Step: 7200 | Train Loss: 0.08410499 |\n",
      "Epoch: 0 | Iter: 7300 | Step: 7300 | Train Loss: 1.18527973 |\n",
      "Epoch: 0 | Iter: 7400 | Step: 7400 | Train Loss: 0.00484241 |\n",
      "Epoch: 0 | Iter: 7500 | Step: 7500 | Train Loss: 1.46298158 |\n",
      "Epoch: 0 | Iter: 7600 | Step: 7600 | Train Loss: 0.02956348 |\n",
      "Epoch: 0 | Iter: 7700 | Step: 7700 | Train Loss: 0.00003894 |\n",
      "Epoch: 0 | Iter: 7800 | Step: 7800 | Train Loss: 0.02383605 |\n",
      "Epoch: 0 | Iter: 7900 | Step: 7900 | Train Loss: 0.01075362 |\n",
      "Epoch: 0 | Iter: 8000 | Step: 8000 | Train Loss: 0.00280387 |\n",
      "Epoch: 0 | Iter: 8100 | Step: 8100 | Train Loss: 0.00442000 |\n",
      "Epoch: 0 | Iter: 8200 | Step: 8200 | Train Loss: 0.00012380 |\n",
      "Epoch: 0 | Iter: 8300 | Step: 8300 | Train Loss: 0.16862234 |\n",
      "Epoch: 0 | Iter: 8400 | Step: 8400 | Train Loss: 0.05081121 |\n",
      "Epoch: 0 | Iter: 8500 | Step: 8500 | Train Loss: 0.02953601 |\n",
      "Epoch: 0 | Iter: 8600 | Step: 8600 | Train Loss: 0.03170098 |\n",
      "Epoch: 0 | Iter: 8700 | Step: 8700 | Train Loss: 0.02307586 |\n",
      "Epoch: 0 | Iter: 8800 | Step: 8800 | Train Loss: 0.04450241 |\n",
      "Epoch: 0 | Iter: 8900 | Step: 8900 | Train Loss: 0.10254621 |\n",
      "Epoch: 0 | Iter: 9000 | Step: 9000 | Train Loss: 0.06204538 |\n",
      "Epoch: 0 | Iter: 9100 | Step: 9100 | Train Loss: 0.00074092 |\n",
      "Epoch: 0 | Iter: 9200 | Step: 9200 | Train Loss: 0.55037892 |\n",
      "Epoch: 0 | Iter: 9300 | Step: 9300 | Train Loss: 0.08436952 |\n",
      "Epoch: 0 | Iter: 9400 | Step: 9400 | Train Loss: 0.06508998 |\n",
      "Epoch: 0 | Iter: 9500 | Step: 9500 | Train Loss: 0.04522959 |\n",
      "Epoch: 0 | Iter: 9600 | Step: 9600 | Train Loss: 0.01174243 |\n",
      "Epoch: 0 | Iter: 9700 | Step: 9700 | Train Loss: 0.06665616 |\n",
      "Epoch: 0 | Iter: 9800 | Step: 9800 | Train Loss: 0.04843001 |\n",
      "Epoch: 0 | Iter: 9900 | Step: 9900 | Train Loss: 0.03093460 |\n",
      "Epoch: 0 | Iter: 10000 | Step: 10000 | Train Loss: 0.03095840 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:10<03:59, 38.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 10000 | Step: 10000 | Val Loss: 0.02554410\n",
      "Epoch: 0 | Iter: 10100 | Step: 10100 | Train Loss: 0.03154308 |\n",
      "Epoch: 0 | Iter: 10200 | Step: 10200 | Train Loss: 0.03267326 |\n",
      "Epoch: 0 | Iter: 10300 | Step: 10300 | Train Loss: 0.65627867 |\n",
      "Epoch: 0 | Iter: 10400 | Step: 10400 | Train Loss: 0.00010877 |\n",
      "Epoch: 0 | Iter: 10500 | Step: 10500 | Train Loss: 0.03203322 |\n",
      "Epoch: 0 | Iter: 10600 | Step: 10600 | Train Loss: 0.00000352 |\n",
      "Epoch: 0 | Iter: 10700 | Step: 10700 | Train Loss: 0.04170057 |\n",
      "Epoch: 0 | Iter: 10800 | Step: 10800 | Train Loss: 0.02618948 |\n",
      "Epoch: 0 | Iter: 10900 | Step: 10900 | Train Loss: 0.03970001 |\n",
      "Epoch: 0 | Iter: 11000 | Step: 11000 | Train Loss: 0.00445525 |\n",
      "Epoch: 0 | Iter: 11100 | Step: 11100 | Train Loss: 0.02824979 |\n",
      "Epoch: 0 | Iter: 11200 | Step: 11200 | Train Loss: 0.03189955 |\n",
      "Epoch: 0 | Iter: 11300 | Step: 11300 | Train Loss: 0.00064244 |\n",
      "Epoch: 0 | Iter: 11400 | Step: 11400 | Train Loss: 0.04730679 |\n",
      "Epoch: 0 | Iter: 11500 | Step: 11500 | Train Loss: 0.03453113 |\n",
      "Epoch: 0 | Iter: 11600 | Step: 11600 | Train Loss: 0.00000131 |\n",
      "Epoch: 0 | Iter: 11700 | Step: 11700 | Train Loss: 0.00066362 |\n",
      "Epoch: 0 | Iter: 11800 | Step: 11800 | Train Loss: 0.03104039 |\n",
      "Epoch: 0 | Iter: 11900 | Step: 11900 | Train Loss: 0.01562408 |\n",
      "Epoch: 0 | Iter: 12000 | Step: 12000 | Train Loss: 0.01782644 |\n",
      "Epoch: 0 | Iter: 12100 | Step: 12100 | Train Loss: 0.45951974 |\n",
      "Epoch: 0 | Iter: 12200 | Step: 12200 | Train Loss: 0.00229413 |\n",
      "Epoch: 0 | Iter: 12300 | Step: 12300 | Train Loss: 0.69408214 |\n",
      "Epoch: 0 | Iter: 12400 | Step: 12400 | Train Loss: 0.01390888 |\n",
      "Epoch: 0 | Iter: 12500 | Step: 12500 | Train Loss: 0.05667335 |\n",
      "Epoch: 0 | Iter: 12600 | Step: 12600 | Train Loss: 0.03651785 |\n",
      "Epoch: 0 | Iter: 12700 | Step: 12700 | Train Loss: 0.12563336 |\n",
      "Epoch: 0 | Iter: 12800 | Step: 12800 | Train Loss: 0.01446045 |\n",
      "Epoch: 0 | Iter: 12900 | Step: 12900 | Train Loss: 0.55533427 |\n",
      "Epoch: 0 | Iter: 13000 | Step: 13000 | Train Loss: 0.03877116 |\n",
      "Epoch: 0 | Iter: 13100 | Step: 13100 | Train Loss: 0.01646221 |\n",
      "Epoch: 0 | Iter: 13200 | Step: 13200 | Train Loss: 0.04114838 |\n",
      "Epoch: 0 | Iter: 13300 | Step: 13300 | Train Loss: 0.00281355 |\n",
      "Epoch: 0 | Iter: 13400 | Step: 13400 | Train Loss: 0.00000003 |\n",
      "Epoch: 0 | Iter: 13500 | Step: 13500 | Train Loss: 0.01654754 |\n",
      "Epoch: 0 | Iter: 13600 | Step: 13600 | Train Loss: 0.04919390 |\n",
      "Epoch: 0 | Iter: 13700 | Step: 13700 | Train Loss: 0.35380796 |\n",
      "Epoch: 0 | Iter: 13800 | Step: 13800 | Train Loss: 0.03624342 |\n",
      "Epoch: 0 | Iter: 13900 | Step: 13900 | Train Loss: 0.05441297 |\n",
      "Epoch: 0 | Iter: 14000 | Step: 14000 | Train Loss: 0.00036627 |\n",
      "Epoch: 0 | Iter: 14100 | Step: 14100 | Train Loss: 0.00013019 |\n",
      "Epoch: 0 | Iter: 14200 | Step: 14200 | Train Loss: 1.52707529 |\n",
      "Epoch: 0 | Iter: 14300 | Step: 14300 | Train Loss: 0.06744657 |\n",
      "Epoch: 0 | Iter: 14400 | Step: 14400 | Train Loss: 0.10775717 |\n",
      "Epoch: 0 | Iter: 14500 | Step: 14500 | Train Loss: 0.04454393 |\n",
      "Epoch: 0 | Iter: 14600 | Step: 14600 | Train Loss: 0.00141512 |\n",
      "Epoch: 0 | Iter: 14700 | Step: 14700 | Train Loss: 0.00247474 |\n",
      "Epoch: 0 | Iter: 14800 | Step: 14800 | Train Loss: 0.03418228 |\n",
      "Epoch: 0 | Iter: 14900 | Step: 14900 | Train Loss: 0.08507841 |\n",
      "Epoch: 0 | Iter: 15000 | Step: 15000 | Train Loss: 0.03782111 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:38, 42.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Iter: 15000 | Step: 15000 | Val Loss: 0.02652589\n",
      "Epoch: 0 | Iter: 15100 | Step: 15100 | Train Loss: 0.00251016 |\n",
      "Epoch: 0 | Iter: 15200 | Step: 15200 | Train Loss: 0.03953616 |\n",
      "Epoch: 0 | Iter: 15300 | Step: 15300 | Train Loss: 0.00340441 |\n",
      "Epoch: 0 | Iter: 15400 | Step: 15400 | Train Loss: 0.20997928 |\n",
      "Epoch: 0 | Iter: 15500 | Step: 15500 | Train Loss: 0.00062004 |\n",
      "Epoch: 0 | Iter: 15600 | Step: 15600 | Train Loss: 0.04667440 |\n",
      "Epoch: 0 | Iter: 15700 | Step: 15700 | Train Loss: 0.00887914 |\n",
      "Epoch: 0 | Iter: 15800 | Step: 15800 | Train Loss: 0.03737038 |\n",
      "Epoch: 0 | Iter: 15900 | Step: 15900 | Train Loss: 0.00001665 |\n",
      "Epoch: 0 | Iter: 16000 | Step: 16000 | Train Loss: 0.06755079 |\n",
      "Epoch: 0 | Iter: 16100 | Step: 16100 | Train Loss: 0.00117045 |\n",
      "Epoch: 0 | Iter: 16200 | Step: 16200 | Train Loss: 0.00528002 |\n",
      "Epoch: 0 | Iter: 16300 | Step: 16300 | Train Loss: 0.00107412 |\n",
      "Epoch: 0 | Iter: 16400 | Step: 16400 | Train Loss: 0.04777538 |\n",
      "Epoch: 0 | Iter: 16500 | Step: 16500 | Train Loss: 0.49467668 |\n",
      "Epoch: 0 | Iter: 16600 | Step: 16600 | Train Loss: 0.14498685 |\n",
      "Epoch: 0 | Iter: 16700 | Step: 16700 | Train Loss: 0.00012753 |\n",
      "Epoch: 0 | Iter: 16800 | Step: 16800 | Train Loss: 0.02308866 |\n",
      "Epoch: 0 | Iter: 16900 | Step: 16900 | Train Loss: 0.00064811 |\n",
      "Epoch: 0 | Iter: 17000 | Step: 17000 | Train Loss: 0.00180148 |\n",
      "Epoch: 0 | Iter: 17100 | Step: 17100 | Train Loss: 0.08174680 |\n",
      "Epoch: 0 | Iter: 17200 | Step: 17200 | Train Loss: 0.02298076 |\n",
      "Epoch: 0 | Iter: 17300 | Step: 17300 | Train Loss: 0.02610941 |\n",
      "Epoch: 0 | Iter: 17400 | Step: 17400 | Train Loss: 0.27473581 |\n",
      "Epoch: 0 | Iter: 17500 | Step: 17500 | Train Loss: 0.03624433 |\n",
      "Epoch: 0 | Iter: 17600 | Step: 17600 | Train Loss: 0.00001786 |\n",
      "Epoch: 0 | Iter: 17700 | Step: 17700 | Train Loss: 0.02818188 |\n",
      "Epoch: 0 | Iter: 17800 | Step: 17800 | Train Loss: 0.04236095 |\n",
      "Epoch: 0 | Iter: 17900 | Step: 17900 | Train Loss: 0.02653154 |\n",
      "Epoch: 0 | Iter: 18000 | Step: 18000 | Train Loss: 0.02432267 |\n",
      "Epoch: 0 | Iter: 18100 | Step: 18100 | Train Loss: 0.04843952 |\n",
      "Epoch: 0 | Iter: 18200 | Step: 18200 | Train Loss: 0.08606285 |\n",
      "Epoch: 0 | Iter: 18300 | Step: 18300 | Train Loss: 0.01812354 |\n",
      "Epoch: 0 | Iter: 18400 | Step: 18400 | Train Loss: 0.04887848 |\n",
      "Epoch: 0 | Iter: 18500 | Step: 18500 | Train Loss: 0.04256683 |\n",
      "Epoch: 0 | Iter: 18600 | Step: 18600 | Train Loss: 0.62129349 |\n",
      "Epoch: 0 | Iter: 18700 | Step: 18700 | Train Loss: 0.02912163 |\n",
      "Epoch: 0 | Iter: 18800 | Step: 18800 | Train Loss: 0.09323437 |\n",
      "Epoch: 0 | Iter: 18900 | Step: 18900 | Train Loss: 0.01139225 |\n",
      "Epoch: 0 | Iter: 19000 | Step: 19000 | Train Loss: 0.03116852 |\n",
      "Epoch: 0 | Iter: 19100 | Step: 19100 | Train Loss: 0.03884494 |\n",
      "Epoch: 0 | Iter: 19200 | Step: 19200 | Train Loss: 0.04758190 |\n",
      "Epoch: 0 | Iter: 19300 | Step: 19300 | Train Loss: 0.06111735 |\n",
      "Epoch: 0 | Iter: 19400 | Step: 19400 | Train Loss: 0.00006078 |\n",
      "Epoch: 0 | Iter: 19500 | Step: 19500 | Train Loss: 0.00396325 |\n",
      "Epoch: 0 | Iter: 19600 | Step: 19600 | Train Loss: 0.01244851 |\n",
      "Epoch: 0 | Iter: 19700 | Step: 19700 | Train Loss: 0.01185321 |\n",
      "Epoch: 0 | Iter: 19800 | Step: 19800 | Train Loss: 0.00008846 |\n",
      "Epoch: 0 | Iter: 19900 | Step: 19900 | Train Loss: 0.00193216 |\n",
      "Epoch: 1 | Iter: 0 | Step: 20000 | Train Loss: 0.05568061 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:37, 42.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 0 | Step: 20000 | Val Loss: 0.02232528\n",
      "Epoch: 1 | Iter: 100 | Step: 20100 | Train Loss: 0.00021627 |\n",
      "Epoch: 1 | Iter: 200 | Step: 20200 | Train Loss: 0.02598961 |\n",
      "Epoch: 1 | Iter: 300 | Step: 20300 | Train Loss: 0.00379238 |\n",
      "Epoch: 1 | Iter: 400 | Step: 20400 | Train Loss: 0.05331425 |\n",
      "Epoch: 1 | Iter: 500 | Step: 20500 | Train Loss: 0.04972436 |\n",
      "Epoch: 1 | Iter: 600 | Step: 20600 | Train Loss: 0.09775529 |\n",
      "Epoch: 1 | Iter: 700 | Step: 20700 | Train Loss: 0.05171911 |\n",
      "Epoch: 1 | Iter: 800 | Step: 20800 | Train Loss: 0.04160829 |\n",
      "Epoch: 1 | Iter: 900 | Step: 20900 | Train Loss: 0.04111102 |\n",
      "Epoch: 1 | Iter: 1000 | Step: 21000 | Train Loss: 0.00001204 |\n",
      "Epoch: 1 | Iter: 1100 | Step: 21100 | Train Loss: 0.03849751 |\n",
      "Epoch: 1 | Iter: 1200 | Step: 21200 | Train Loss: 0.00218677 |\n",
      "Epoch: 1 | Iter: 1300 | Step: 21300 | Train Loss: 0.00094071 |\n",
      "Epoch: 1 | Iter: 1400 | Step: 21400 | Train Loss: 0.00026936 |\n",
      "Epoch: 1 | Iter: 1500 | Step: 21500 | Train Loss: 0.08379629 |\n",
      "Epoch: 1 | Iter: 1600 | Step: 21600 | Train Loss: 0.00429150 |\n",
      "Epoch: 1 | Iter: 1700 | Step: 21700 | Train Loss: 0.08970123 |\n",
      "Epoch: 1 | Iter: 1800 | Step: 21800 | Train Loss: 0.00029970 |\n",
      "Epoch: 1 | Iter: 1900 | Step: 21900 | Train Loss: 0.01737686 |\n",
      "Epoch: 1 | Iter: 2000 | Step: 22000 | Train Loss: 0.01026608 |\n",
      "Epoch: 1 | Iter: 2100 | Step: 22100 | Train Loss: 0.04750816 |\n",
      "Epoch: 1 | Iter: 2200 | Step: 22200 | Train Loss: 0.01118191 |\n",
      "Epoch: 1 | Iter: 2300 | Step: 22300 | Train Loss: 0.04669378 |\n",
      "Epoch: 1 | Iter: 2400 | Step: 22400 | Train Loss: 0.03504503 |\n",
      "Epoch: 1 | Iter: 2500 | Step: 22500 | Train Loss: 0.00005976 |\n",
      "Epoch: 1 | Iter: 2600 | Step: 22600 | Train Loss: 0.02310323 |\n",
      "Epoch: 1 | Iter: 2700 | Step: 22700 | Train Loss: 0.00027234 |\n",
      "Epoch: 1 | Iter: 2800 | Step: 22800 | Train Loss: 0.05884479 |\n",
      "Epoch: 1 | Iter: 2900 | Step: 22900 | Train Loss: 0.01814990 |\n",
      "Epoch: 1 | Iter: 3000 | Step: 23000 | Train Loss: 0.00181810 |\n",
      "Epoch: 1 | Iter: 3100 | Step: 23100 | Train Loss: 0.01957937 |\n",
      "Epoch: 1 | Iter: 3200 | Step: 23200 | Train Loss: 0.14770797 |\n",
      "Epoch: 1 | Iter: 3300 | Step: 23300 | Train Loss: 0.02885414 |\n",
      "Epoch: 1 | Iter: 3400 | Step: 23400 | Train Loss: 0.02996788 |\n",
      "Epoch: 1 | Iter: 3500 | Step: 23500 | Train Loss: 0.03179816 |\n",
      "Epoch: 1 | Iter: 3600 | Step: 23600 | Train Loss: 0.03714887 |\n",
      "Epoch: 1 | Iter: 3700 | Step: 23700 | Train Loss: 0.04732596 |\n",
      "Epoch: 1 | Iter: 3800 | Step: 23800 | Train Loss: 0.27238458 |\n",
      "Epoch: 1 | Iter: 3900 | Step: 23900 | Train Loss: 0.00576683 |\n",
      "Epoch: 1 | Iter: 4000 | Step: 24000 | Train Loss: 1.38476849 |\n",
      "Epoch: 1 | Iter: 4100 | Step: 24100 | Train Loss: 0.03761784 |\n",
      "Epoch: 1 | Iter: 4200 | Step: 24200 | Train Loss: 0.03687501 |\n",
      "Epoch: 1 | Iter: 4300 | Step: 24300 | Train Loss: 0.00004242 |\n",
      "Epoch: 1 | Iter: 4400 | Step: 24400 | Train Loss: 0.02837807 |\n",
      "Epoch: 1 | Iter: 4500 | Step: 24500 | Train Loss: 0.00511391 |\n",
      "Epoch: 1 | Iter: 4600 | Step: 24600 | Train Loss: 0.00102756 |\n",
      "Epoch: 1 | Iter: 4700 | Step: 24700 | Train Loss: 0.24605788 |\n",
      "Epoch: 1 | Iter: 4800 | Step: 24800 | Train Loss: 0.04833063 |\n",
      "Epoch: 1 | Iter: 4900 | Step: 24900 | Train Loss: 0.05477450 |\n",
      "Epoch: 1 | Iter: 5000 | Step: 25000 | Train Loss: 0.00005989 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:23, 45.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 5000 | Step: 25000 | Val Loss: 0.02494576\n",
      "Epoch: 1 | Iter: 5100 | Step: 25100 | Train Loss: 0.05287201 |\n",
      "Epoch: 1 | Iter: 5200 | Step: 25200 | Train Loss: 0.00264148 |\n",
      "Epoch: 1 | Iter: 5300 | Step: 25300 | Train Loss: 0.06691984 |\n",
      "Epoch: 1 | Iter: 5400 | Step: 25400 | Train Loss: 0.00439253 |\n",
      "Epoch: 1 | Iter: 5500 | Step: 25500 | Train Loss: 0.01886993 |\n",
      "Epoch: 1 | Iter: 5600 | Step: 25600 | Train Loss: 0.66470104 |\n",
      "Epoch: 1 | Iter: 5700 | Step: 25700 | Train Loss: 0.02279209 |\n",
      "Epoch: 1 | Iter: 5800 | Step: 25800 | Train Loss: 0.04966805 |\n",
      "Epoch: 1 | Iter: 5900 | Step: 25900 | Train Loss: 0.04462213 |\n",
      "Epoch: 1 | Iter: 6000 | Step: 26000 | Train Loss: 0.05197810 |\n",
      "Epoch: 1 | Iter: 6100 | Step: 26100 | Train Loss: 0.02811819 |\n",
      "Epoch: 1 | Iter: 6200 | Step: 26200 | Train Loss: 0.07229345 |\n",
      "Epoch: 1 | Iter: 6300 | Step: 26300 | Train Loss: 0.07266396 |\n",
      "Epoch: 1 | Iter: 6400 | Step: 26400 | Train Loss: 0.00118320 |\n",
      "Epoch: 1 | Iter: 6500 | Step: 26500 | Train Loss: 0.00424786 |\n",
      "Epoch: 1 | Iter: 6600 | Step: 26600 | Train Loss: 0.00405560 |\n",
      "Epoch: 1 | Iter: 6700 | Step: 26700 | Train Loss: 0.00026791 |\n",
      "Epoch: 1 | Iter: 6800 | Step: 26800 | Train Loss: 0.43564552 |\n",
      "Epoch: 1 | Iter: 6900 | Step: 26900 | Train Loss: 0.13239935 |\n",
      "Epoch: 1 | Iter: 7000 | Step: 27000 | Train Loss: 0.00075872 |\n",
      "Epoch: 1 | Iter: 7100 | Step: 27100 | Train Loss: 0.02224358 |\n",
      "Epoch: 1 | Iter: 7200 | Step: 27200 | Train Loss: 0.02618841 |\n",
      "Epoch: 1 | Iter: 7300 | Step: 27300 | Train Loss: 0.03465372 |\n",
      "Epoch: 1 | Iter: 7400 | Step: 27400 | Train Loss: 0.00879243 |\n",
      "Epoch: 1 | Iter: 7500 | Step: 27500 | Train Loss: 0.06017981 |\n",
      "Epoch: 1 | Iter: 7600 | Step: 27600 | Train Loss: 0.00305463 |\n",
      "Epoch: 1 | Iter: 7700 | Step: 27700 | Train Loss: 0.01332779 |\n",
      "Epoch: 1 | Iter: 7800 | Step: 27800 | Train Loss: 0.02112742 |\n",
      "Epoch: 1 | Iter: 7900 | Step: 27900 | Train Loss: 0.00259160 |\n",
      "Epoch: 1 | Iter: 8000 | Step: 28000 | Train Loss: 0.03013495 |\n",
      "Epoch: 1 | Iter: 8100 | Step: 28100 | Train Loss: 0.04236800 |\n",
      "Epoch: 1 | Iter: 8200 | Step: 28200 | Train Loss: 0.05778356 |\n",
      "Epoch: 1 | Iter: 8300 | Step: 28300 | Train Loss: 0.00036357 |\n",
      "Epoch: 1 | Iter: 8400 | Step: 28400 | Train Loss: 0.00013046 |\n",
      "Epoch: 1 | Iter: 8500 | Step: 28500 | Train Loss: 0.60164350 |\n",
      "Epoch: 1 | Iter: 8600 | Step: 28600 | Train Loss: 0.01555195 |\n",
      "Epoch: 1 | Iter: 8700 | Step: 28700 | Train Loss: 0.04276630 |\n",
      "Epoch: 1 | Iter: 8800 | Step: 28800 | Train Loss: 0.04183808 |\n",
      "Epoch: 1 | Iter: 8900 | Step: 28900 | Train Loss: 0.00279940 |\n",
      "Epoch: 1 | Iter: 9000 | Step: 29000 | Train Loss: 0.03368390 |\n",
      "Epoch: 1 | Iter: 9100 | Step: 29100 | Train Loss: 0.02860037 |\n",
      "Epoch: 1 | Iter: 9200 | Step: 29200 | Train Loss: 0.05584450 |\n",
      "Epoch: 1 | Iter: 9300 | Step: 29300 | Train Loss: 0.00012404 |\n",
      "Epoch: 1 | Iter: 9400 | Step: 29400 | Train Loss: 0.00953754 |\n",
      "Epoch: 1 | Iter: 9500 | Step: 29500 | Train Loss: 0.03359377 |\n",
      "Epoch: 1 | Iter: 9600 | Step: 29600 | Train Loss: 1.29949021 |\n",
      "Epoch: 1 | Iter: 9700 | Step: 29700 | Train Loss: 0.11516157 |\n",
      "Epoch: 1 | Iter: 9800 | Step: 29800 | Train Loss: 0.06375107 |\n",
      "Epoch: 1 | Iter: 9900 | Step: 29900 | Train Loss: 0.04102955 |\n",
      "Epoch: 1 | Iter: 10000 | Step: 30000 | Train Loss: 0.04383345 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:28, 44.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 10000 | Step: 30000 | Val Loss: 0.02481445\n",
      "Epoch: 1 | Iter: 10100 | Step: 30100 | Train Loss: 0.00025612 |\n",
      "Epoch: 1 | Iter: 10200 | Step: 30200 | Train Loss: 0.05348377 |\n",
      "Epoch: 1 | Iter: 10300 | Step: 30300 | Train Loss: 0.02800210 |\n",
      "Epoch: 1 | Iter: 10400 | Step: 30400 | Train Loss: 0.32411754 |\n",
      "Epoch: 1 | Iter: 10500 | Step: 30500 | Train Loss: 0.04094406 |\n",
      "Epoch: 1 | Iter: 10600 | Step: 30600 | Train Loss: 0.02919989 |\n",
      "Epoch: 1 | Iter: 10700 | Step: 30700 | Train Loss: 0.00002491 |\n",
      "Epoch: 1 | Iter: 10800 | Step: 30800 | Train Loss: 0.00686182 |\n",
      "Epoch: 1 | Iter: 10900 | Step: 30900 | Train Loss: 0.13796178 |\n",
      "Epoch: 1 | Iter: 11000 | Step: 31000 | Train Loss: 0.00036292 |\n",
      "Epoch: 1 | Iter: 11100 | Step: 31100 | Train Loss: 0.39557031 |\n",
      "Epoch: 1 | Iter: 11200 | Step: 31200 | Train Loss: 0.53308070 |\n",
      "Epoch: 1 | Iter: 11300 | Step: 31300 | Train Loss: 0.06446958 |\n",
      "Epoch: 1 | Iter: 11400 | Step: 31400 | Train Loss: 0.03494390 |\n",
      "Epoch: 1 | Iter: 11500 | Step: 31500 | Train Loss: 0.02662246 |\n",
      "Epoch: 1 | Iter: 11600 | Step: 31600 | Train Loss: 0.08111138 |\n",
      "Epoch: 1 | Iter: 11700 | Step: 31700 | Train Loss: 0.05972705 |\n",
      "Epoch: 1 | Iter: 11800 | Step: 31800 | Train Loss: 0.00083670 |\n",
      "Epoch: 1 | Iter: 11900 | Step: 31900 | Train Loss: 0.06477565 |\n",
      "Epoch: 1 | Iter: 12000 | Step: 32000 | Train Loss: 0.02629155 |\n",
      "Epoch: 1 | Iter: 12100 | Step: 32100 | Train Loss: 0.03147719 |\n",
      "Epoch: 1 | Iter: 12200 | Step: 32200 | Train Loss: 0.01074864 |\n",
      "Epoch: 1 | Iter: 12300 | Step: 32300 | Train Loss: 0.03381235 |\n",
      "Epoch: 1 | Iter: 12400 | Step: 32400 | Train Loss: 0.04047992 |\n",
      "Epoch: 1 | Iter: 12500 | Step: 32500 | Train Loss: 0.00657905 |\n",
      "Epoch: 1 | Iter: 12600 | Step: 32600 | Train Loss: 0.04876348 |\n",
      "Epoch: 1 | Iter: 12700 | Step: 32700 | Train Loss: 0.00502275 |\n",
      "Epoch: 1 | Iter: 12800 | Step: 32800 | Train Loss: 0.02850816 |\n",
      "Epoch: 1 | Iter: 12900 | Step: 32900 | Train Loss: 0.00965969 |\n",
      "Epoch: 1 | Iter: 13000 | Step: 33000 | Train Loss: 0.00202836 |\n",
      "Epoch: 1 | Iter: 13100 | Step: 33100 | Train Loss: 0.05491586 |\n",
      "Epoch: 1 | Iter: 13200 | Step: 33200 | Train Loss: 0.00423704 |\n",
      "Epoch: 1 | Iter: 13300 | Step: 33300 | Train Loss: 0.00099250 |\n",
      "Epoch: 1 | Iter: 13400 | Step: 33400 | Train Loss: 0.00102690 |\n",
      "Epoch: 1 | Iter: 13500 | Step: 33500 | Train Loss: 0.02180609 |\n",
      "Epoch: 1 | Iter: 13600 | Step: 33600 | Train Loss: 0.71996176 |\n",
      "Epoch: 1 | Iter: 13700 | Step: 33700 | Train Loss: 0.03474955 |\n",
      "Epoch: 1 | Iter: 13800 | Step: 33800 | Train Loss: 0.16091734 |\n",
      "Epoch: 1 | Iter: 13900 | Step: 33900 | Train Loss: 0.00242129 |\n",
      "Epoch: 1 | Iter: 14000 | Step: 34000 | Train Loss: 1.48709142 |\n",
      "Epoch: 1 | Iter: 14100 | Step: 34100 | Train Loss: 0.25508177 |\n",
      "Epoch: 1 | Iter: 14200 | Step: 34200 | Train Loss: 0.00000727 |\n",
      "Epoch: 1 | Iter: 14300 | Step: 34300 | Train Loss: 0.16009814 |\n",
      "Epoch: 1 | Iter: 14400 | Step: 34400 | Train Loss: 0.05634786 |\n",
      "Epoch: 1 | Iter: 14500 | Step: 34500 | Train Loss: 0.00713992 |\n",
      "Epoch: 1 | Iter: 14600 | Step: 34600 | Train Loss: 0.00509204 |\n",
      "Epoch: 1 | Iter: 14700 | Step: 34700 | Train Loss: 0.30436474 |\n",
      "Epoch: 1 | Iter: 14800 | Step: 34800 | Train Loss: 0.02577818 |\n",
      "Epoch: 1 | Iter: 14900 | Step: 34900 | Train Loss: 0.04290066 |\n",
      "Epoch: 1 | Iter: 15000 | Step: 35000 | Train Loss: 0.05547558 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:32, 43.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Iter: 15000 | Step: 35000 | Val Loss: 0.02541756\n",
      "Epoch: 1 | Iter: 15100 | Step: 35100 | Train Loss: 0.00718114 |\n",
      "Epoch: 1 | Iter: 15200 | Step: 35200 | Train Loss: 0.01811360 |\n",
      "Epoch: 1 | Iter: 15300 | Step: 35300 | Train Loss: 0.00048973 |\n",
      "Epoch: 1 | Iter: 15400 | Step: 35400 | Train Loss: 0.00063977 |\n",
      "Epoch: 1 | Iter: 15500 | Step: 35500 | Train Loss: 0.00266702 |\n",
      "Epoch: 1 | Iter: 15600 | Step: 35600 | Train Loss: 0.03803430 |\n",
      "Epoch: 1 | Iter: 15700 | Step: 35700 | Train Loss: 0.00284557 |\n",
      "Epoch: 1 | Iter: 15800 | Step: 35800 | Train Loss: 0.02082249 |\n",
      "Epoch: 1 | Iter: 15900 | Step: 35900 | Train Loss: 0.00165112 |\n",
      "Epoch: 1 | Iter: 16000 | Step: 36000 | Train Loss: 0.00022705 |\n",
      "Epoch: 1 | Iter: 16100 | Step: 36100 | Train Loss: 0.07370972 |\n",
      "Epoch: 1 | Iter: 16200 | Step: 36200 | Train Loss: 0.00009675 |\n",
      "Epoch: 1 | Iter: 16300 | Step: 36300 | Train Loss: 0.06373876 |\n",
      "Epoch: 1 | Iter: 16400 | Step: 36400 | Train Loss: 0.00089824 |\n",
      "Epoch: 1 | Iter: 16500 | Step: 36500 | Train Loss: 0.03549658 |\n",
      "Epoch: 1 | Iter: 16600 | Step: 36600 | Train Loss: 0.04330241 |\n",
      "Epoch: 1 | Iter: 16700 | Step: 36700 | Train Loss: 0.00035316 |\n",
      "Epoch: 1 | Iter: 16800 | Step: 36800 | Train Loss: 0.04492426 |\n",
      "Epoch: 1 | Iter: 16900 | Step: 36900 | Train Loss: 0.00019462 |\n",
      "Epoch: 1 | Iter: 17000 | Step: 37000 | Train Loss: 0.00595805 |\n",
      "Epoch: 1 | Iter: 17100 | Step: 37100 | Train Loss: 0.01364972 |\n",
      "Epoch: 1 | Iter: 17200 | Step: 37200 | Train Loss: 0.02278969 |\n",
      "Epoch: 1 | Iter: 17300 | Step: 37300 | Train Loss: 0.68038690 |\n",
      "Epoch: 1 | Iter: 17400 | Step: 37400 | Train Loss: 0.10092473 |\n",
      "Epoch: 1 | Iter: 17500 | Step: 37500 | Train Loss: 0.01123376 |\n",
      "Epoch: 1 | Iter: 17600 | Step: 37600 | Train Loss: 0.08297891 |\n",
      "Epoch: 1 | Iter: 17700 | Step: 37700 | Train Loss: 0.02279106 |\n",
      "Epoch: 1 | Iter: 17800 | Step: 37800 | Train Loss: 0.10080080 |\n",
      "Epoch: 1 | Iter: 17900 | Step: 37900 | Train Loss: 0.04582152 |\n",
      "Epoch: 1 | Iter: 18000 | Step: 38000 | Train Loss: 0.00202242 |\n",
      "Epoch: 1 | Iter: 18100 | Step: 38100 | Train Loss: 0.02563997 |\n",
      "Epoch: 1 | Iter: 18200 | Step: 38200 | Train Loss: 0.08045743 |\n",
      "Epoch: 1 | Iter: 18300 | Step: 38300 | Train Loss: 0.03326606 |\n",
      "Epoch: 1 | Iter: 18400 | Step: 38400 | Train Loss: 0.00036743 |\n",
      "Epoch: 1 | Iter: 18500 | Step: 38500 | Train Loss: 0.00242505 |\n",
      "Epoch: 1 | Iter: 18600 | Step: 38600 | Train Loss: 0.04913035 |\n",
      "Epoch: 1 | Iter: 18700 | Step: 38700 | Train Loss: 0.08335453 |\n",
      "Epoch: 1 | Iter: 18800 | Step: 38800 | Train Loss: 0.57411748 |\n",
      "Epoch: 1 | Iter: 18900 | Step: 38900 | Train Loss: 0.03817053 |\n",
      "Epoch: 1 | Iter: 19000 | Step: 39000 | Train Loss: 0.04532180 |\n",
      "Epoch: 1 | Iter: 19100 | Step: 39100 | Train Loss: 0.01436517 |\n",
      "Epoch: 1 | Iter: 19200 | Step: 39200 | Train Loss: 1.22054458 |\n",
      "Epoch: 1 | Iter: 19300 | Step: 39300 | Train Loss: 0.03830238 |\n",
      "Epoch: 1 | Iter: 19400 | Step: 39400 | Train Loss: 0.03538948 |\n",
      "Epoch: 1 | Iter: 19500 | Step: 39500 | Train Loss: 0.00043688 |\n",
      "Epoch: 1 | Iter: 19600 | Step: 39600 | Train Loss: 0.02456236 |\n",
      "Epoch: 1 | Iter: 19700 | Step: 39700 | Train Loss: 0.00118904 |\n",
      "Epoch: 1 | Iter: 19800 | Step: 39800 | Train Loss: 0.00020106 |\n",
      "Epoch: 1 | Iter: 19900 | Step: 39900 | Train Loss: 0.27116534 |\n",
      "Epoch: 2 | Iter: 0 | Step: 40000 | Train Loss: 0.00000729 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:19, 46.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Iter: 0 | Step: 40000 | Val Loss: 0.02670077\n",
      "Epoch: 2 | Iter: 100 | Step: 40100 | Train Loss: 0.01752030 |\n",
      "Epoch: 2 | Iter: 200 | Step: 40200 | Train Loss: 0.02115411 |\n",
      "Epoch: 2 | Iter: 300 | Step: 40300 | Train Loss: 0.04711604 |\n",
      "Epoch: 2 | Iter: 400 | Step: 40400 | Train Loss: 0.02487163 |\n",
      "Epoch: 2 | Iter: 500 | Step: 40500 | Train Loss: 0.07682990 |\n",
      "Epoch: 2 | Iter: 600 | Step: 40600 | Train Loss: 0.01235104 |\n",
      "Epoch: 2 | Iter: 700 | Step: 40700 | Train Loss: 0.03778712 |\n",
      "Epoch: 2 | Iter: 800 | Step: 40800 | Train Loss: 0.00158844 |\n",
      "Epoch: 2 | Iter: 900 | Step: 40900 | Train Loss: 0.00063832 |\n",
      "Epoch: 2 | Iter: 1000 | Step: 41000 | Train Loss: 0.00049582 |\n",
      "Epoch: 2 | Iter: 1100 | Step: 41100 | Train Loss: 0.00095954 |\n",
      "Epoch: 2 | Iter: 1200 | Step: 41200 | Train Loss: 0.04842978 |\n",
      "Epoch: 2 | Iter: 1300 | Step: 41300 | Train Loss: 0.00000322 |\n",
      "Epoch: 2 | Iter: 1400 | Step: 41400 | Train Loss: 0.04463355 |\n",
      "Epoch: 2 | Iter: 1500 | Step: 41500 | Train Loss: 0.00475270 |\n",
      "Epoch: 2 | Iter: 1600 | Step: 41600 | Train Loss: 0.00000687 |\n",
      "Epoch: 2 | Iter: 1700 | Step: 41700 | Train Loss: 0.00676795 |\n",
      "Epoch: 2 | Iter: 1800 | Step: 41800 | Train Loss: 0.00233090 |\n",
      "Epoch: 2 | Iter: 1900 | Step: 41900 | Train Loss: 0.04905465 |\n",
      "Epoch: 2 | Iter: 2000 | Step: 42000 | Train Loss: 0.00206014 |\n",
      "Epoch: 2 | Iter: 2100 | Step: 42100 | Train Loss: 0.00120786 |\n",
      "Epoch: 2 | Iter: 2200 | Step: 42200 | Train Loss: 0.03628105 |\n",
      "Epoch: 2 | Iter: 2300 | Step: 42300 | Train Loss: 0.04244694 |\n",
      "Epoch: 2 | Iter: 2400 | Step: 42400 | Train Loss: 0.00468393 |\n",
      "Epoch: 2 | Iter: 2500 | Step: 42500 | Train Loss: 0.01339190 |\n",
      "Epoch: 2 | Iter: 2600 | Step: 42600 | Train Loss: 0.03396701 |\n",
      "Epoch: 2 | Iter: 2700 | Step: 42700 | Train Loss: 0.03171862 |\n",
      "Epoch: 2 | Iter: 2800 | Step: 42800 | Train Loss: 0.00001513 |\n",
      "Epoch: 2 | Iter: 2900 | Step: 42900 | Train Loss: 0.02431507 |\n",
      "Epoch: 2 | Iter: 3000 | Step: 43000 | Train Loss: 0.05412124 |\n",
      "Epoch: 2 | Iter: 3100 | Step: 43100 | Train Loss: 0.02700175 |\n",
      "Epoch: 2 | Iter: 3200 | Step: 43200 | Train Loss: 0.00800612 |\n",
      "Epoch: 2 | Iter: 3300 | Step: 43300 | Train Loss: 0.00320392 |\n",
      "Epoch: 2 | Iter: 3400 | Step: 43400 | Train Loss: 0.00469139 |\n",
      "Epoch: 2 | Iter: 3500 | Step: 43500 | Train Loss: 0.08466064 |\n",
      "Epoch: 2 | Iter: 3600 | Step: 43600 | Train Loss: 0.01637182 |\n",
      "Epoch: 2 | Iter: 3700 | Step: 43700 | Train Loss: 0.00917315 |\n",
      "Epoch: 2 | Iter: 3800 | Step: 43800 | Train Loss: 0.07852530 |\n",
      "Epoch: 2 | Iter: 3900 | Step: 43900 | Train Loss: 0.19466589 |\n",
      "Epoch: 2 | Iter: 4000 | Step: 44000 | Train Loss: 1.55110919 |\n",
      "Epoch: 2 | Iter: 4100 | Step: 44100 | Train Loss: 0.02618532 |\n",
      "Epoch: 2 | Iter: 4200 | Step: 44200 | Train Loss: 0.06095690 |\n",
      "Epoch: 2 | Iter: 4300 | Step: 44300 | Train Loss: 0.02594115 |\n",
      "Epoch: 2 | Iter: 4400 | Step: 44400 | Train Loss: 0.04951870 |\n",
      "Epoch: 2 | Iter: 4500 | Step: 44500 | Train Loss: 0.01187680 |\n",
      "Epoch: 2 | Iter: 4600 | Step: 44600 | Train Loss: 0.00051205 |\n",
      "Epoch: 2 | Iter: 4700 | Step: 44700 | Train Loss: 0.00000001 |\n",
      "Epoch: 2 | Iter: 4800 | Step: 44800 | Train Loss: 0.00041261 |\n",
      "Epoch: 2 | Iter: 4900 | Step: 44900 | Train Loss: 0.02822360 |\n",
      "Epoch: 2 | Iter: 5000 | Step: 45000 | Train Loss: 0.09060730 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:27, 44.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Iter: 5000 | Step: 45000 | Val Loss: 0.02366420\n",
      "Epoch: 2 | Iter: 5100 | Step: 45100 | Train Loss: 0.03940463 |\n",
      "Epoch: 2 | Iter: 5200 | Step: 45200 | Train Loss: 0.00118048 |\n",
      "Epoch: 2 | Iter: 5300 | Step: 45300 | Train Loss: 0.02085676 |\n",
      "Epoch: 2 | Iter: 5400 | Step: 45400 | Train Loss: 0.05328076 |\n",
      "Epoch: 2 | Iter: 5500 | Step: 45500 | Train Loss: 0.04418397 |\n",
      "Epoch: 2 | Iter: 5600 | Step: 45600 | Train Loss: 0.73278683 |\n",
      "Epoch: 2 | Iter: 5700 | Step: 45700 | Train Loss: 0.02113064 |\n",
      "Epoch: 2 | Iter: 5800 | Step: 45800 | Train Loss: 0.02845131 |\n",
      "Epoch: 2 | Iter: 5900 | Step: 45900 | Train Loss: 0.04499878 |\n",
      "Epoch: 2 | Iter: 6000 | Step: 46000 | Train Loss: 0.07122887 |\n",
      "Epoch: 2 | Iter: 6100 | Step: 46100 | Train Loss: 0.00822797 |\n",
      "Epoch: 2 | Iter: 6200 | Step: 46200 | Train Loss: 0.06520497 |\n",
      "Epoch: 2 | Iter: 6300 | Step: 46300 | Train Loss: 0.02175295 |\n",
      "Epoch: 2 | Iter: 6400 | Step: 46400 | Train Loss: 0.03524933 |\n",
      "Epoch: 2 | Iter: 6500 | Step: 46500 | Train Loss: 0.00419937 |\n",
      "Epoch: 2 | Iter: 6600 | Step: 46600 | Train Loss: 0.00008358 |\n",
      "Epoch: 2 | Iter: 6700 | Step: 46700 | Train Loss: 0.03143550 |\n",
      "Epoch: 2 | Iter: 6800 | Step: 46800 | Train Loss: 0.22193827 |\n",
      "Epoch: 2 | Iter: 6900 | Step: 46900 | Train Loss: 0.62887675 |\n",
      "Epoch: 2 | Iter: 7000 | Step: 47000 | Train Loss: 0.14985622 |\n",
      "Epoch: 2 | Iter: 7100 | Step: 47100 | Train Loss: 0.00282308 |\n",
      "Epoch: 2 | Iter: 7200 | Step: 47200 | Train Loss: 0.00006322 |\n",
      "Epoch: 2 | Iter: 7300 | Step: 47300 | Train Loss: 0.00033795 |\n",
      "Epoch: 2 | Iter: 7400 | Step: 47400 | Train Loss: 0.02625358 |\n",
      "Epoch: 2 | Iter: 7500 | Step: 47500 | Train Loss: 0.00002428 |\n",
      "Epoch: 2 | Iter: 7600 | Step: 47600 | Train Loss: 0.04915695 |\n",
      "Epoch: 2 | Iter: 7700 | Step: 47700 | Train Loss: 0.00073708 |\n",
      "Epoch: 2 | Iter: 7800 | Step: 47800 | Train Loss: 0.00043855 |\n",
      "Epoch: 2 | Iter: 7900 | Step: 47900 | Train Loss: 0.00582035 |\n",
      "Epoch: 2 | Iter: 8000 | Step: 48000 | Train Loss: 0.02848195 |\n",
      "Epoch: 2 | Iter: 8100 | Step: 48100 | Train Loss: 0.03032472 |\n",
      "Epoch: 2 | Iter: 8200 | Step: 48200 | Train Loss: 0.01298260 |\n",
      "Epoch: 2 | Iter: 8300 | Step: 48300 | Train Loss: 0.02960307 |\n",
      "Epoch: 2 | Iter: 8400 | Step: 48400 | Train Loss: 0.03091946 |\n",
      "Epoch: 2 | Iter: 8500 | Step: 48500 | Train Loss: 1.33163023 |\n",
      "Epoch: 2 | Iter: 8600 | Step: 48600 | Train Loss: 0.15142003 |\n",
      "Epoch: 2 | Iter: 8700 | Step: 48700 | Train Loss: 0.02487576 |\n",
      "Epoch: 2 | Iter: 8800 | Step: 48800 | Train Loss: 0.00087170 |\n",
      "Epoch: 2 | Iter: 8900 | Step: 48900 | Train Loss: 0.04110811 |\n",
      "Epoch: 2 | Iter: 9000 | Step: 49000 | Train Loss: 0.00371357 |\n",
      "Epoch: 2 | Iter: 9100 | Step: 49100 | Train Loss: 0.04011352 |\n",
      "Epoch: 2 | Iter: 9200 | Step: 49200 | Train Loss: 0.00794693 |\n",
      "Epoch: 2 | Iter: 9300 | Step: 49300 | Train Loss: 0.04039177 |\n",
      "Epoch: 2 | Iter: 9400 | Step: 49400 | Train Loss: 0.21013376 |\n",
      "Epoch: 2 | Iter: 9500 | Step: 49500 | Train Loss: 0.01235379 |\n",
      "Epoch: 2 | Iter: 9600 | Step: 49600 | Train Loss: 0.93979168 |\n",
      "Epoch: 2 | Iter: 9700 | Step: 49700 | Train Loss: 0.01724850 |\n",
      "Epoch: 2 | Iter: 9800 | Step: 49800 | Train Loss: 0.00168491 |\n",
      "Epoch: 2 | Iter: 9900 | Step: 49900 | Train Loss: 0.05475152 |\n",
      "Epoch: 2 | Iter: 10000 | Step: 50000 | Train Loss: 0.04069562 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:17, 47.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Iter: 10000 | Step: 50000 | Val Loss: 0.02351099\n",
      "Epoch: 2 | Iter: 10100 | Step: 50100 | Train Loss: 0.01301706 |\n",
      "Epoch: 2 | Iter: 10200 | Step: 50200 | Train Loss: 0.00003373 |\n",
      "Epoch: 2 | Iter: 10300 | Step: 50300 | Train Loss: 0.03056487 |\n",
      "Epoch: 2 | Iter: 10400 | Step: 50400 | Train Loss: 0.36492354 |\n",
      "Epoch: 2 | Iter: 10500 | Step: 50500 | Train Loss: 0.02632892 |\n",
      "Epoch: 2 | Iter: 10600 | Step: 50600 | Train Loss: 0.01139277 |\n",
      "Epoch: 2 | Iter: 10700 | Step: 50700 | Train Loss: 0.01159056 |\n",
      "Epoch: 2 | Iter: 10800 | Step: 50800 | Train Loss: 0.05324670 |\n",
      "Epoch: 2 | Iter: 10900 | Step: 50900 | Train Loss: 0.49771830 |\n",
      "Epoch: 2 | Iter: 11000 | Step: 51000 | Train Loss: 0.10951795 |\n",
      "Epoch: 2 | Iter: 11100 | Step: 51100 | Train Loss: 0.17521700 |\n",
      "Epoch: 2 | Iter: 11200 | Step: 51200 | Train Loss: 0.50711286 |\n",
      "Epoch: 2 | Iter: 11300 | Step: 51300 | Train Loss: 0.06480640 |\n",
      "Epoch: 2 | Iter: 11400 | Step: 51400 | Train Loss: 0.07183496 |\n",
      "Epoch: 2 | Iter: 11500 | Step: 51500 | Train Loss: 0.01046907 |\n",
      "Epoch: 2 | Iter: 11600 | Step: 51600 | Train Loss: 0.07425388 |\n",
      "Epoch: 2 | Iter: 11700 | Step: 51700 | Train Loss: 0.01884679 |\n",
      "Epoch: 2 | Iter: 11800 | Step: 51800 | Train Loss: 0.03488388 |\n",
      "Epoch: 2 | Iter: 11900 | Step: 51900 | Train Loss: 0.04737147 |\n",
      "Epoch: 2 | Iter: 12000 | Step: 52000 | Train Loss: 0.00225378 |\n",
      "Epoch: 2 | Iter: 12100 | Step: 52100 | Train Loss: 0.03289770 |\n",
      "Epoch: 2 | Iter: 12200 | Step: 52200 | Train Loss: 0.07031562 |\n",
      "Epoch: 2 | Iter: 12300 | Step: 52300 | Train Loss: 0.00890304 |\n",
      "Epoch: 2 | Iter: 12400 | Step: 52400 | Train Loss: 0.00000919 |\n",
      "Epoch: 2 | Iter: 12500 | Step: 52500 | Train Loss: 0.06311288 |\n",
      "Epoch: 2 | Iter: 12600 | Step: 52600 | Train Loss: 0.04852162 |\n",
      "Epoch: 2 | Iter: 12700 | Step: 52700 | Train Loss: 0.00001118 |\n",
      "Epoch: 2 | Iter: 12800 | Step: 52800 | Train Loss: 0.05505184 |\n",
      "Epoch: 2 | Iter: 12900 | Step: 52900 | Train Loss: 0.06457891 |\n",
      "Epoch: 2 | Iter: 13000 | Step: 53000 | Train Loss: 0.00027977 |\n",
      "Epoch: 2 | Iter: 13100 | Step: 53100 | Train Loss: 0.07534371 |\n",
      "Epoch: 2 | Iter: 13200 | Step: 53200 | Train Loss: 0.01291620 |\n",
      "Epoch: 2 | Iter: 13300 | Step: 53300 | Train Loss: 0.03276978 |\n",
      "Epoch: 2 | Iter: 13400 | Step: 53400 | Train Loss: 0.01007087 |\n",
      "Epoch: 2 | Iter: 13500 | Step: 53500 | Train Loss: 0.02804961 |\n",
      "Epoch: 2 | Iter: 13600 | Step: 53600 | Train Loss: 0.20426366 |\n",
      "Epoch: 2 | Iter: 13700 | Step: 53700 | Train Loss: 0.00344401 |\n",
      "Epoch: 2 | Iter: 13800 | Step: 53800 | Train Loss: 0.04997232 |\n",
      "Epoch: 2 | Iter: 13900 | Step: 53900 | Train Loss: 0.03255227 |\n",
      "Epoch: 2 | Iter: 14000 | Step: 54000 | Train Loss: 1.54423630 |\n",
      "Epoch: 2 | Iter: 14100 | Step: 54100 | Train Loss: 0.02619030 |\n",
      "Epoch: 2 | Iter: 14200 | Step: 54200 | Train Loss: 0.05155256 |\n",
      "Epoch: 2 | Iter: 14300 | Step: 54300 | Train Loss: 0.18643004 |\n",
      "Epoch: 2 | Iter: 14400 | Step: 54400 | Train Loss: 0.05407150 |\n",
      "Epoch: 2 | Iter: 14500 | Step: 54500 | Train Loss: 0.03734746 |\n",
      "Epoch: 2 | Iter: 14600 | Step: 54600 | Train Loss: 0.02738280 |\n",
      "Epoch: 2 | Iter: 14700 | Step: 54700 | Train Loss: 0.12441320 |\n",
      "Epoch: 2 | Iter: 14800 | Step: 54800 | Train Loss: 0.03235505 |\n",
      "Epoch: 2 | Iter: 14900 | Step: 54900 | Train Loss: 0.02665041 |\n",
      "Epoch: 2 | Iter: 15000 | Step: 55000 | Train Loss: 0.06385566 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:35, 43.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Iter: 15000 | Step: 55000 | Val Loss: 0.02305634\n",
      "Epoch: 2 | Iter: 15100 | Step: 55100 | Train Loss: 0.04716038 |\n",
      "Epoch: 2 | Iter: 15200 | Step: 55200 | Train Loss: 0.03410321 |\n",
      "Epoch: 2 | Iter: 15300 | Step: 55300 | Train Loss: 0.02900368 |\n",
      "Epoch: 2 | Iter: 15400 | Step: 55400 | Train Loss: 0.01704587 |\n",
      "Epoch: 2 | Iter: 15500 | Step: 55500 | Train Loss: 0.03572469 |\n",
      "Epoch: 2 | Iter: 15600 | Step: 55600 | Train Loss: 0.04097014 |\n",
      "Epoch: 2 | Iter: 15700 | Step: 55700 | Train Loss: 0.00497267 |\n",
      "Epoch: 2 | Iter: 15800 | Step: 55800 | Train Loss: 0.00386415 |\n",
      "Epoch: 2 | Iter: 15900 | Step: 55900 | Train Loss: 0.03789058 |\n",
      "Epoch: 2 | Iter: 16000 | Step: 56000 | Train Loss: 0.03133382 |\n",
      "Epoch: 2 | Iter: 16100 | Step: 56100 | Train Loss: 0.06827624 |\n",
      "Epoch: 2 | Iter: 16200 | Step: 56200 | Train Loss: 0.04675393 |\n",
      "Epoch: 2 | Iter: 16300 | Step: 56300 | Train Loss: 0.00025375 |\n",
      "Epoch: 2 | Iter: 16400 | Step: 56400 | Train Loss: 0.04731617 |\n",
      "Epoch: 2 | Iter: 16500 | Step: 56500 | Train Loss: 0.00046525 |\n",
      "Epoch: 2 | Iter: 16600 | Step: 56600 | Train Loss: 0.03202975 |\n",
      "Epoch: 2 | Iter: 16700 | Step: 56700 | Train Loss: 0.00037584 |\n",
      "Epoch: 2 | Iter: 16800 | Step: 56800 | Train Loss: 0.00412219 |\n",
      "Epoch: 2 | Iter: 16900 | Step: 56900 | Train Loss: 0.02635882 |\n",
      "Epoch: 2 | Iter: 17000 | Step: 57000 | Train Loss: 0.03574422 |\n",
      "Epoch: 2 | Iter: 17100 | Step: 57100 | Train Loss: 0.00411833 |\n",
      "Epoch: 2 | Iter: 17200 | Step: 57200 | Train Loss: 0.07198677 |\n",
      "Epoch: 2 | Iter: 17300 | Step: 57300 | Train Loss: 0.51560080 |\n",
      "Epoch: 2 | Iter: 17400 | Step: 57400 | Train Loss: 0.06614517 |\n",
      "Epoch: 2 | Iter: 17500 | Step: 57500 | Train Loss: 0.00006680 |\n",
      "Epoch: 2 | Iter: 17600 | Step: 57600 | Train Loss: 0.00062108 |\n",
      "Epoch: 2 | Iter: 17700 | Step: 57700 | Train Loss: 0.00043056 |\n",
      "Epoch: 2 | Iter: 17800 | Step: 57800 | Train Loss: 0.01132895 |\n",
      "Epoch: 2 | Iter: 17900 | Step: 57900 | Train Loss: 0.00095941 |\n",
      "Epoch: 2 | Iter: 18000 | Step: 58000 | Train Loss: 0.05446093 |\n",
      "Epoch: 2 | Iter: 18100 | Step: 58100 | Train Loss: 0.01416325 |\n",
      "Epoch: 2 | Iter: 18200 | Step: 58200 | Train Loss: 0.00814355 |\n",
      "Epoch: 2 | Iter: 18300 | Step: 58300 | Train Loss: 0.00217669 |\n",
      "Epoch: 2 | Iter: 18400 | Step: 58400 | Train Loss: 0.01213565 |\n",
      "Epoch: 2 | Iter: 18500 | Step: 58500 | Train Loss: 0.00026026 |\n",
      "Epoch: 2 | Iter: 18600 | Step: 58600 | Train Loss: 0.03506901 |\n",
      "Epoch: 2 | Iter: 18700 | Step: 58700 | Train Loss: 0.00424309 |\n",
      "Epoch: 2 | Iter: 18800 | Step: 58800 | Train Loss: 1.30105960 |\n",
      "Epoch: 2 | Iter: 18900 | Step: 58900 | Train Loss: 0.03151930 |\n",
      "Epoch: 2 | Iter: 19000 | Step: 59000 | Train Loss: 0.00000676 |\n",
      "Epoch: 2 | Iter: 19100 | Step: 59100 | Train Loss: 0.03925063 |\n",
      "Epoch: 2 | Iter: 19200 | Step: 59200 | Train Loss: 1.26712394 |\n",
      "Epoch: 2 | Iter: 19300 | Step: 59300 | Train Loss: 0.01590972 |\n",
      "Epoch: 2 | Iter: 19400 | Step: 59400 | Train Loss: 0.02464854 |\n",
      "Epoch: 2 | Iter: 19500 | Step: 59500 | Train Loss: 0.09852194 |\n",
      "Epoch: 2 | Iter: 19600 | Step: 59600 | Train Loss: 0.00081047 |\n",
      "Epoch: 2 | Iter: 19700 | Step: 59700 | Train Loss: 0.03066441 |\n",
      "Epoch: 2 | Iter: 19800 | Step: 59800 | Train Loss: 0.03394692 |\n",
      "Epoch: 2 | Iter: 19900 | Step: 59900 | Train Loss: 0.05790811 |\n",
      "Epoch: 3 | Iter: 0 | Step: 60000 | Train Loss: 0.06455243 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:35, 43.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Iter: 0 | Step: 60000 | Val Loss: 0.02305189\n",
      "Epoch: 3 | Iter: 100 | Step: 60100 | Train Loss: 0.00000772 |\n",
      "Epoch: 3 | Iter: 200 | Step: 60200 | Train Loss: 0.00542315 |\n",
      "Epoch: 3 | Iter: 300 | Step: 60300 | Train Loss: 0.03380617 |\n",
      "Epoch: 3 | Iter: 400 | Step: 60400 | Train Loss: 0.03833790 |\n",
      "Epoch: 3 | Iter: 500 | Step: 60500 | Train Loss: 0.00286830 |\n",
      "Epoch: 3 | Iter: 600 | Step: 60600 | Train Loss: 0.11514292 |\n",
      "Epoch: 3 | Iter: 700 | Step: 60700 | Train Loss: 0.00014681 |\n",
      "Epoch: 3 | Iter: 800 | Step: 60800 | Train Loss: 0.02676021 |\n",
      "Epoch: 3 | Iter: 900 | Step: 60900 | Train Loss: 0.01621720 |\n",
      "Epoch: 3 | Iter: 1000 | Step: 61000 | Train Loss: 0.00413612 |\n",
      "Epoch: 3 | Iter: 1100 | Step: 61100 | Train Loss: 0.04621597 |\n",
      "Epoch: 3 | Iter: 1200 | Step: 61200 | Train Loss: 0.00161688 |\n",
      "Epoch: 3 | Iter: 1300 | Step: 61300 | Train Loss: 0.00136749 |\n",
      "Epoch: 3 | Iter: 1400 | Step: 61400 | Train Loss: 0.00065820 |\n",
      "Epoch: 3 | Iter: 1500 | Step: 61500 | Train Loss: 0.10343844 |\n",
      "Epoch: 3 | Iter: 1600 | Step: 61600 | Train Loss: 0.00001706 |\n",
      "Epoch: 3 | Iter: 1700 | Step: 61700 | Train Loss: 0.14580791 |\n",
      "Epoch: 3 | Iter: 1800 | Step: 61800 | Train Loss: 0.00173316 |\n",
      "Epoch: 3 | Iter: 1900 | Step: 61900 | Train Loss: 0.04186479 |\n",
      "Epoch: 3 | Iter: 2000 | Step: 62000 | Train Loss: 0.06364831 |\n",
      "Epoch: 3 | Iter: 2100 | Step: 62100 | Train Loss: 0.02027092 |\n",
      "Epoch: 3 | Iter: 2200 | Step: 62200 | Train Loss: 0.19064915 |\n",
      "Epoch: 3 | Iter: 2300 | Step: 62300 | Train Loss: 0.03627117 |\n",
      "Epoch: 3 | Iter: 2400 | Step: 62400 | Train Loss: 0.05124922 |\n",
      "Epoch: 3 | Iter: 2500 | Step: 62500 | Train Loss: 0.02424146 |\n",
      "Epoch: 3 | Iter: 2600 | Step: 62600 | Train Loss: 0.00007445 |\n",
      "Epoch: 3 | Iter: 2700 | Step: 62700 | Train Loss: 0.03225401 |\n",
      "Epoch: 3 | Iter: 2800 | Step: 62800 | Train Loss: 0.02927883 |\n",
      "Epoch: 3 | Iter: 2900 | Step: 62900 | Train Loss: 0.03393867 |\n",
      "Epoch: 3 | Iter: 3000 | Step: 63000 | Train Loss: 0.00033060 |\n",
      "Epoch: 3 | Iter: 3100 | Step: 63100 | Train Loss: 0.00319546 |\n",
      "Epoch: 3 | Iter: 3200 | Step: 63200 | Train Loss: 0.16806982 |\n",
      "Epoch: 3 | Iter: 3300 | Step: 63300 | Train Loss: 0.00131347 |\n",
      "Epoch: 3 | Iter: 3400 | Step: 63400 | Train Loss: 0.00053635 |\n",
      "Epoch: 3 | Iter: 3500 | Step: 63500 | Train Loss: 0.02886800 |\n",
      "Epoch: 3 | Iter: 3600 | Step: 63600 | Train Loss: 0.00096790 |\n",
      "Epoch: 3 | Iter: 3700 | Step: 63700 | Train Loss: 0.00348241 |\n",
      "Epoch: 3 | Iter: 3800 | Step: 63800 | Train Loss: 0.19007923 |\n",
      "Epoch: 3 | Iter: 3900 | Step: 63900 | Train Loss: 0.18161708 |\n",
      "Epoch: 3 | Iter: 4000 | Step: 64000 | Train Loss: 0.51475346 |\n",
      "Epoch: 3 | Iter: 4100 | Step: 64100 | Train Loss: 0.00179065 |\n",
      "Epoch: 3 | Iter: 4200 | Step: 64200 | Train Loss: 0.01866933 |\n",
      "Epoch: 3 | Iter: 4300 | Step: 64300 | Train Loss: 0.03806844 |\n",
      "Epoch: 3 | Iter: 4400 | Step: 64400 | Train Loss: 0.00048207 |\n",
      "Epoch: 3 | Iter: 4500 | Step: 64500 | Train Loss: 0.00613783 |\n",
      "Epoch: 3 | Iter: 4600 | Step: 64600 | Train Loss: 0.00000120 |\n",
      "Epoch: 3 | Iter: 4700 | Step: 64700 | Train Loss: 0.13241978 |\n",
      "Epoch: 3 | Iter: 4800 | Step: 64800 | Train Loss: 0.02939186 |\n",
      "Epoch: 3 | Iter: 4900 | Step: 64900 | Train Loss: 0.04441826 |\n",
      "Epoch: 3 | Iter: 5000 | Step: 65000 | Train Loss: 0.00214135 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:28, 44.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Iter: 5000 | Step: 65000 | Val Loss: 0.02452568\n",
      "Epoch: 3 | Iter: 5100 | Step: 65100 | Train Loss: 0.00015050 |\n",
      "Epoch: 3 | Iter: 5200 | Step: 65200 | Train Loss: 0.07409038 |\n",
      "Epoch: 3 | Iter: 5300 | Step: 65300 | Train Loss: 0.00212915 |\n",
      "Epoch: 3 | Iter: 5400 | Step: 65400 | Train Loss: 0.03444282 |\n",
      "Epoch: 3 | Iter: 5500 | Step: 65500 | Train Loss: 0.05410633 |\n",
      "Epoch: 3 | Iter: 5600 | Step: 65600 | Train Loss: 0.42879027 |\n",
      "Epoch: 3 | Iter: 5700 | Step: 65700 | Train Loss: 0.06638706 |\n",
      "Epoch: 3 | Iter: 5800 | Step: 65800 | Train Loss: 0.04115093 |\n",
      "Epoch: 3 | Iter: 5900 | Step: 65900 | Train Loss: 0.01401298 |\n",
      "Epoch: 3 | Iter: 6000 | Step: 66000 | Train Loss: 0.06013784 |\n",
      "Epoch: 3 | Iter: 6100 | Step: 66100 | Train Loss: 0.05374388 |\n",
      "Epoch: 3 | Iter: 6200 | Step: 66200 | Train Loss: 0.01652823 |\n",
      "Epoch: 3 | Iter: 6300 | Step: 66300 | Train Loss: 0.00924601 |\n",
      "Epoch: 3 | Iter: 6400 | Step: 66400 | Train Loss: 0.01938807 |\n",
      "Epoch: 3 | Iter: 6500 | Step: 66500 | Train Loss: 0.05561946 |\n",
      "Epoch: 3 | Iter: 6600 | Step: 66600 | Train Loss: 0.03257943 |\n",
      "Epoch: 3 | Iter: 6700 | Step: 66700 | Train Loss: 0.00006247 |\n",
      "Epoch: 3 | Iter: 6800 | Step: 66800 | Train Loss: 0.38724953 |\n",
      "Epoch: 3 | Iter: 6900 | Step: 66900 | Train Loss: 0.23181380 |\n",
      "Epoch: 3 | Iter: 7000 | Step: 67000 | Train Loss: 0.00420170 |\n",
      "Epoch: 3 | Iter: 7100 | Step: 67100 | Train Loss: 0.00001199 |\n",
      "Epoch: 3 | Iter: 7200 | Step: 67200 | Train Loss: 0.04025253 |\n",
      "Epoch: 3 | Iter: 7300 | Step: 67300 | Train Loss: 0.01791245 |\n",
      "Epoch: 3 | Iter: 7400 | Step: 67400 | Train Loss: 0.00699486 |\n",
      "Epoch: 3 | Iter: 7500 | Step: 67500 | Train Loss: 0.06354173 |\n",
      "Epoch: 3 | Iter: 7600 | Step: 67600 | Train Loss: 0.06624757 |\n",
      "Epoch: 3 | Iter: 7700 | Step: 67700 | Train Loss: 0.03710873 |\n",
      "Epoch: 3 | Iter: 7800 | Step: 67800 | Train Loss: 0.00165868 |\n",
      "Epoch: 3 | Iter: 7900 | Step: 67900 | Train Loss: 0.04737595 |\n",
      "Epoch: 3 | Iter: 8000 | Step: 68000 | Train Loss: 0.00002747 |\n",
      "Epoch: 3 | Iter: 8100 | Step: 68100 | Train Loss: 0.05442256 |\n",
      "Epoch: 3 | Iter: 8200 | Step: 68200 | Train Loss: 0.03477620 |\n",
      "Epoch: 3 | Iter: 8300 | Step: 68300 | Train Loss: 0.00230639 |\n",
      "Epoch: 3 | Iter: 8400 | Step: 68400 | Train Loss: 0.05014917 |\n",
      "Epoch: 3 | Iter: 8500 | Step: 68500 | Train Loss: 1.07139575 |\n",
      "Epoch: 3 | Iter: 8600 | Step: 68600 | Train Loss: 0.31838855 |\n",
      "Epoch: 3 | Iter: 8700 | Step: 68700 | Train Loss: 0.00945048 |\n",
      "Epoch: 3 | Iter: 8800 | Step: 68800 | Train Loss: 0.06656096 |\n",
      "Epoch: 3 | Iter: 8900 | Step: 68900 | Train Loss: 0.03788197 |\n",
      "Epoch: 3 | Iter: 9000 | Step: 69000 | Train Loss: 0.04372117 |\n",
      "Epoch: 3 | Iter: 9100 | Step: 69100 | Train Loss: 0.03467775 |\n",
      "Epoch: 3 | Iter: 9200 | Step: 69200 | Train Loss: 0.00496157 |\n",
      "Epoch: 3 | Iter: 9300 | Step: 69300 | Train Loss: 0.04634595 |\n",
      "Epoch: 3 | Iter: 9400 | Step: 69400 | Train Loss: 0.00376451 |\n",
      "Epoch: 3 | Iter: 9500 | Step: 69500 | Train Loss: 0.03026908 |\n",
      "Epoch: 3 | Iter: 9600 | Step: 69600 | Train Loss: 0.25141433 |\n",
      "Epoch: 3 | Iter: 9700 | Step: 69700 | Train Loss: 0.05239888 |\n",
      "Epoch: 3 | Iter: 9800 | Step: 69800 | Train Loss: 0.00353107 |\n",
      "Epoch: 3 | Iter: 9900 | Step: 69900 | Train Loss: 0.04036576 |\n",
      "Epoch: 3 | Iter: 10000 | Step: 70000 | Train Loss: 0.00191354 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:25, 45.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Iter: 10000 | Step: 70000 | Val Loss: 0.02252637\n",
      "Epoch: 3 | Iter: 10100 | Step: 70100 | Train Loss: 0.00678213 |\n",
      "Epoch: 3 | Iter: 10200 | Step: 70200 | Train Loss: 0.05114030 |\n",
      "Epoch: 3 | Iter: 10300 | Step: 70300 | Train Loss: 0.00044754 |\n",
      "Epoch: 3 | Iter: 10400 | Step: 70400 | Train Loss: 0.29847899 |\n",
      "Epoch: 3 | Iter: 10500 | Step: 70500 | Train Loss: 0.00119553 |\n",
      "Epoch: 3 | Iter: 10600 | Step: 70600 | Train Loss: 0.00023064 |\n",
      "Epoch: 3 | Iter: 10700 | Step: 70700 | Train Loss: 0.01194508 |\n",
      "Epoch: 3 | Iter: 10800 | Step: 70800 | Train Loss: 0.05010822 |\n",
      "Epoch: 3 | Iter: 10900 | Step: 70900 | Train Loss: 0.48316866 |\n",
      "Epoch: 3 | Iter: 11000 | Step: 71000 | Train Loss: 0.03458833 |\n",
      "Epoch: 3 | Iter: 11100 | Step: 71100 | Train Loss: 0.15458331 |\n",
      "Epoch: 3 | Iter: 11200 | Step: 71200 | Train Loss: 0.53963464 |\n",
      "Epoch: 3 | Iter: 11300 | Step: 71300 | Train Loss: 0.03405253 |\n",
      "Epoch: 3 | Iter: 11400 | Step: 71400 | Train Loss: 0.00000002 |\n",
      "Epoch: 3 | Iter: 11500 | Step: 71500 | Train Loss: 0.03801834 |\n",
      "Epoch: 3 | Iter: 11600 | Step: 71600 | Train Loss: 0.05995130 |\n",
      "Epoch: 3 | Iter: 11700 | Step: 71700 | Train Loss: 0.00043099 |\n",
      "Epoch: 3 | Iter: 11800 | Step: 71800 | Train Loss: 0.03121751 |\n",
      "Epoch: 3 | Iter: 11900 | Step: 71900 | Train Loss: 0.01553100 |\n",
      "Epoch: 3 | Iter: 12000 | Step: 72000 | Train Loss: 0.03032925 |\n",
      "Epoch: 3 | Iter: 12100 | Step: 72100 | Train Loss: 0.00211084 |\n",
      "Epoch: 3 | Iter: 12200 | Step: 72200 | Train Loss: 0.03121115 |\n",
      "Epoch: 3 | Iter: 12300 | Step: 72300 | Train Loss: 0.03998453 |\n",
      "Epoch: 3 | Iter: 12400 | Step: 72400 | Train Loss: 0.03686154 |\n",
      "Epoch: 3 | Iter: 12500 | Step: 72500 | Train Loss: 0.01901402 |\n",
      "Epoch: 3 | Iter: 12600 | Step: 72600 | Train Loss: 0.02865659 |\n",
      "Epoch: 3 | Iter: 12700 | Step: 72700 | Train Loss: 0.07991592 |\n",
      "Epoch: 3 | Iter: 12800 | Step: 72800 | Train Loss: 0.00000161 |\n",
      "Epoch: 3 | Iter: 12900 | Step: 72900 | Train Loss: 0.00000996 |\n",
      "Epoch: 3 | Iter: 13000 | Step: 73000 | Train Loss: 0.11353920 |\n",
      "Epoch: 3 | Iter: 13100 | Step: 73100 | Train Loss: 0.08945820 |\n",
      "Epoch: 3 | Iter: 13200 | Step: 73200 | Train Loss: 0.04778931 |\n",
      "Epoch: 3 | Iter: 13300 | Step: 73300 | Train Loss: 0.06246766 |\n",
      "Epoch: 3 | Iter: 13400 | Step: 73400 | Train Loss: 0.02634502 |\n",
      "Epoch: 3 | Iter: 13500 | Step: 73500 | Train Loss: 0.00174218 |\n",
      "Epoch: 3 | Iter: 13600 | Step: 73600 | Train Loss: 0.62378770 |\n",
      "Epoch: 3 | Iter: 13700 | Step: 73700 | Train Loss: 0.00015335 |\n",
      "Epoch: 3 | Iter: 13800 | Step: 73800 | Train Loss: 0.17614441 |\n",
      "Epoch: 3 | Iter: 13900 | Step: 73900 | Train Loss: 0.03482049 |\n",
      "Epoch: 3 | Iter: 14000 | Step: 74000 | Train Loss: 1.09460807 |\n",
      "Epoch: 3 | Iter: 14100 | Step: 74100 | Train Loss: 0.14287305 |\n",
      "Epoch: 3 | Iter: 14200 | Step: 74200 | Train Loss: 0.01634063 |\n",
      "Epoch: 3 | Iter: 14300 | Step: 74300 | Train Loss: 0.30628809 |\n",
      "Epoch: 3 | Iter: 14400 | Step: 74400 | Train Loss: 0.03183289 |\n",
      "Epoch: 3 | Iter: 14500 | Step: 74500 | Train Loss: 0.03672944 |\n",
      "Epoch: 3 | Iter: 14600 | Step: 74600 | Train Loss: 0.04147302 |\n",
      "Epoch: 3 | Iter: 14700 | Step: 74700 | Train Loss: 0.13195741 |\n",
      "Epoch: 3 | Iter: 14800 | Step: 74800 | Train Loss: 0.02305645 |\n",
      "Epoch: 3 | Iter: 14900 | Step: 74900 | Train Loss: 0.00258140 |\n",
      "Epoch: 3 | Iter: 15000 | Step: 75000 | Train Loss: 0.07108806 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:22, 46.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Iter: 15000 | Step: 75000 | Val Loss: 0.02397187\n",
      "Epoch: 3 | Iter: 15100 | Step: 75100 | Train Loss: 0.06122722 |\n",
      "Epoch: 3 | Iter: 15200 | Step: 75200 | Train Loss: 0.00001281 |\n",
      "Epoch: 3 | Iter: 15300 | Step: 75300 | Train Loss: 0.02123648 |\n",
      "Epoch: 3 | Iter: 15400 | Step: 75400 | Train Loss: 0.04653293 |\n",
      "Epoch: 3 | Iter: 15500 | Step: 75500 | Train Loss: 0.02513881 |\n",
      "Epoch: 3 | Iter: 15600 | Step: 75600 | Train Loss: 0.00124030 |\n",
      "Epoch: 3 | Iter: 15700 | Step: 75700 | Train Loss: 0.03538042 |\n",
      "Epoch: 3 | Iter: 15800 | Step: 75800 | Train Loss: 0.01754607 |\n",
      "Epoch: 3 | Iter: 15900 | Step: 75900 | Train Loss: 0.03074198 |\n",
      "Epoch: 3 | Iter: 16000 | Step: 76000 | Train Loss: 0.03887967 |\n",
      "Epoch: 3 | Iter: 16100 | Step: 76100 | Train Loss: 0.03526963 |\n",
      "Epoch: 3 | Iter: 16200 | Step: 76200 | Train Loss: 0.04521166 |\n",
      "Epoch: 3 | Iter: 16300 | Step: 76300 | Train Loss: 0.03599669 |\n",
      "Epoch: 3 | Iter: 16400 | Step: 76400 | Train Loss: 0.00014707 |\n",
      "Epoch: 3 | Iter: 16500 | Step: 76500 | Train Loss: 0.04614465 |\n",
      "Epoch: 3 | Iter: 16600 | Step: 76600 | Train Loss: 0.02768165 |\n",
      "Epoch: 3 | Iter: 16700 | Step: 76700 | Train Loss: 0.03942013 |\n",
      "Epoch: 3 | Iter: 16800 | Step: 76800 | Train Loss: 0.00515863 |\n",
      "Epoch: 3 | Iter: 16900 | Step: 76900 | Train Loss: 0.00113089 |\n",
      "Epoch: 3 | Iter: 17000 | Step: 77000 | Train Loss: 0.00042276 |\n",
      "Epoch: 3 | Iter: 17100 | Step: 77100 | Train Loss: 0.01674957 |\n",
      "Epoch: 3 | Iter: 17200 | Step: 77200 | Train Loss: 0.02587735 |\n",
      "Epoch: 3 | Iter: 17300 | Step: 77300 | Train Loss: 1.15062475 |\n",
      "Epoch: 3 | Iter: 17400 | Step: 77400 | Train Loss: 0.01254867 |\n",
      "Epoch: 3 | Iter: 17500 | Step: 77500 | Train Loss: 0.03604206 |\n",
      "Epoch: 3 | Iter: 17600 | Step: 77600 | Train Loss: 0.00139446 |\n",
      "Epoch: 3 | Iter: 17700 | Step: 77700 | Train Loss: 0.03437008 |\n",
      "Epoch: 3 | Iter: 17800 | Step: 77800 | Train Loss: 0.00852796 |\n",
      "Epoch: 3 | Iter: 17900 | Step: 77900 | Train Loss: 0.01526656 |\n",
      "Epoch: 3 | Iter: 18000 | Step: 78000 | Train Loss: 0.00894758 |\n",
      "Epoch: 3 | Iter: 18100 | Step: 78100 | Train Loss: 0.04098202 |\n",
      "Epoch: 3 | Iter: 18200 | Step: 78200 | Train Loss: 0.10481170 |\n",
      "Epoch: 3 | Iter: 18300 | Step: 78300 | Train Loss: 0.05125547 |\n",
      "Epoch: 3 | Iter: 18400 | Step: 78400 | Train Loss: 0.04039295 |\n",
      "Epoch: 3 | Iter: 18500 | Step: 78500 | Train Loss: 0.04083403 |\n",
      "Epoch: 3 | Iter: 18600 | Step: 78600 | Train Loss: 0.00674849 |\n",
      "Epoch: 3 | Iter: 18700 | Step: 78700 | Train Loss: 0.02522485 |\n",
      "Epoch: 3 | Iter: 18800 | Step: 78800 | Train Loss: 0.43273020 |\n",
      "Epoch: 3 | Iter: 18900 | Step: 78900 | Train Loss: 0.00109283 |\n",
      "Epoch: 3 | Iter: 19000 | Step: 79000 | Train Loss: 0.05209876 |\n",
      "Epoch: 3 | Iter: 19100 | Step: 79100 | Train Loss: 0.00562711 |\n",
      "Epoch: 3 | Iter: 19200 | Step: 79200 | Train Loss: 0.76946837 |\n",
      "Epoch: 3 | Iter: 19300 | Step: 79300 | Train Loss: 0.03322814 |\n",
      "Epoch: 3 | Iter: 19400 | Step: 79400 | Train Loss: 0.03722868 |\n",
      "Epoch: 3 | Iter: 19500 | Step: 79500 | Train Loss: 0.01455759 |\n",
      "Epoch: 3 | Iter: 19600 | Step: 79600 | Train Loss: 0.00300684 |\n",
      "Epoch: 3 | Iter: 19700 | Step: 79700 | Train Loss: 0.00280978 |\n",
      "Epoch: 3 | Iter: 19800 | Step: 79800 | Train Loss: 0.05930393 |\n",
      "Epoch: 3 | Iter: 19900 | Step: 79900 | Train Loss: 0.00275810 |\n",
      "Epoch: 4 | Iter: 0 | Step: 80000 | Train Loss: 0.00000005 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:25, 45.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Iter: 0 | Step: 80000 | Val Loss: 0.02553515\n",
      "Epoch: 4 | Iter: 100 | Step: 80100 | Train Loss: 0.03696810 |\n",
      "Epoch: 4 | Iter: 200 | Step: 80200 | Train Loss: 0.00003749 |\n",
      "Epoch: 4 | Iter: 300 | Step: 80300 | Train Loss: 0.00041844 |\n",
      "Epoch: 4 | Iter: 400 | Step: 80400 | Train Loss: 0.06701028 |\n",
      "Epoch: 4 | Iter: 500 | Step: 80500 | Train Loss: 0.00197150 |\n",
      "Epoch: 4 | Iter: 600 | Step: 80600 | Train Loss: 0.08549642 |\n",
      "Epoch: 4 | Iter: 700 | Step: 80700 | Train Loss: 0.00022691 |\n",
      "Epoch: 4 | Iter: 800 | Step: 80800 | Train Loss: 0.04298870 |\n",
      "Epoch: 4 | Iter: 900 | Step: 80900 | Train Loss: 0.00333937 |\n",
      "Epoch: 4 | Iter: 1000 | Step: 81000 | Train Loss: 0.03302743 |\n",
      "Epoch: 4 | Iter: 1100 | Step: 81100 | Train Loss: 0.00128118 |\n",
      "Epoch: 4 | Iter: 1200 | Step: 81200 | Train Loss: 0.04887023 |\n",
      "Epoch: 4 | Iter: 1300 | Step: 81300 | Train Loss: 0.00053149 |\n",
      "Epoch: 4 | Iter: 1400 | Step: 81400 | Train Loss: 0.00239311 |\n",
      "Epoch: 4 | Iter: 1500 | Step: 81500 | Train Loss: 0.01662928 |\n",
      "Epoch: 4 | Iter: 1600 | Step: 81600 | Train Loss: 0.05242535 |\n",
      "Epoch: 4 | Iter: 1700 | Step: 81700 | Train Loss: 0.05221957 |\n",
      "Epoch: 4 | Iter: 1800 | Step: 81800 | Train Loss: 0.02384647 |\n",
      "Epoch: 4 | Iter: 1900 | Step: 81900 | Train Loss: 0.00348825 |\n",
      "Epoch: 4 | Iter: 2000 | Step: 82000 | Train Loss: 0.04127260 |\n",
      "Epoch: 4 | Iter: 2100 | Step: 82100 | Train Loss: 0.03807043 |\n",
      "Epoch: 4 | Iter: 2200 | Step: 82200 | Train Loss: 0.00035821 |\n",
      "Epoch: 4 | Iter: 2300 | Step: 82300 | Train Loss: 0.03783027 |\n",
      "Epoch: 4 | Iter: 2400 | Step: 82400 | Train Loss: 0.03793921 |\n",
      "Epoch: 4 | Iter: 2500 | Step: 82500 | Train Loss: 0.03192041 |\n",
      "Epoch: 4 | Iter: 2600 | Step: 82600 | Train Loss: 0.06606038 |\n",
      "Epoch: 4 | Iter: 2700 | Step: 82700 | Train Loss: 0.02800017 |\n",
      "Epoch: 4 | Iter: 2800 | Step: 82800 | Train Loss: 0.01359822 |\n",
      "Epoch: 4 | Iter: 2900 | Step: 82900 | Train Loss: 0.00769891 |\n",
      "Epoch: 4 | Iter: 3000 | Step: 83000 | Train Loss: 0.02065662 |\n",
      "Epoch: 4 | Iter: 3100 | Step: 83100 | Train Loss: 0.09954154 |\n",
      "Epoch: 4 | Iter: 3200 | Step: 83200 | Train Loss: 0.00002225 |\n",
      "Epoch: 4 | Iter: 3300 | Step: 83300 | Train Loss: 0.01309933 |\n",
      "Epoch: 4 | Iter: 3400 | Step: 83400 | Train Loss: 0.00545989 |\n",
      "Epoch: 4 | Iter: 3500 | Step: 83500 | Train Loss: 0.00394995 |\n",
      "Epoch: 4 | Iter: 3600 | Step: 83600 | Train Loss: 0.00044722 |\n",
      "Epoch: 4 | Iter: 3700 | Step: 83700 | Train Loss: 0.06300735 |\n",
      "Epoch: 4 | Iter: 3800 | Step: 83800 | Train Loss: 0.02116347 |\n",
      "Epoch: 4 | Iter: 3900 | Step: 83900 | Train Loss: 0.03747561 |\n",
      "Epoch: 4 | Iter: 4000 | Step: 84000 | Train Loss: 0.42214322 |\n",
      "Epoch: 4 | Iter: 4100 | Step: 84100 | Train Loss: 0.04506827 |\n",
      "Epoch: 4 | Iter: 4200 | Step: 84200 | Train Loss: 0.00665470 |\n",
      "Epoch: 4 | Iter: 4300 | Step: 84300 | Train Loss: 0.03513886 |\n",
      "Epoch: 4 | Iter: 4400 | Step: 84400 | Train Loss: 0.04604472 |\n",
      "Epoch: 4 | Iter: 4500 | Step: 84500 | Train Loss: 0.02095536 |\n",
      "Epoch: 4 | Iter: 4600 | Step: 84600 | Train Loss: 0.00058143 |\n",
      "Epoch: 4 | Iter: 4700 | Step: 84700 | Train Loss: 0.04857348 |\n",
      "Epoch: 4 | Iter: 4800 | Step: 84800 | Train Loss: 0.00112262 |\n",
      "Epoch: 4 | Iter: 4900 | Step: 84900 | Train Loss: 0.00000106 |\n",
      "Epoch: 4 | Iter: 5000 | Step: 85000 | Train Loss: 0.02598708 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:43, 41.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Iter: 5000 | Step: 85000 | Val Loss: 0.02445919\n",
      "Epoch: 4 | Iter: 5100 | Step: 85100 | Train Loss: 0.00427510 |\n",
      "Epoch: 4 | Iter: 5200 | Step: 85200 | Train Loss: 0.00031729 |\n",
      "Epoch: 4 | Iter: 5300 | Step: 85300 | Train Loss: 0.05772812 |\n",
      "Epoch: 4 | Iter: 5400 | Step: 85400 | Train Loss: 0.00063097 |\n",
      "Epoch: 4 | Iter: 5500 | Step: 85500 | Train Loss: 0.03068936 |\n",
      "Epoch: 4 | Iter: 5600 | Step: 85600 | Train Loss: 0.69238472 |\n",
      "Epoch: 4 | Iter: 5700 | Step: 85700 | Train Loss: 0.00446740 |\n",
      "Epoch: 4 | Iter: 5800 | Step: 85800 | Train Loss: 0.02074273 |\n",
      "Epoch: 4 | Iter: 5900 | Step: 85900 | Train Loss: 0.02528245 |\n",
      "Epoch: 4 | Iter: 6000 | Step: 86000 | Train Loss: 0.00801137 |\n",
      "Epoch: 4 | Iter: 6100 | Step: 86100 | Train Loss: 0.00077233 |\n",
      "Epoch: 4 | Iter: 6200 | Step: 86200 | Train Loss: 0.00206337 |\n",
      "Epoch: 4 | Iter: 6300 | Step: 86300 | Train Loss: 0.09361816 |\n",
      "Epoch: 4 | Iter: 6400 | Step: 86400 | Train Loss: 0.01323107 |\n",
      "Epoch: 4 | Iter: 6500 | Step: 86500 | Train Loss: 0.07436627 |\n",
      "Epoch: 4 | Iter: 6600 | Step: 86600 | Train Loss: 0.05806521 |\n",
      "Epoch: 4 | Iter: 6700 | Step: 86700 | Train Loss: 0.03611597 |\n",
      "Epoch: 4 | Iter: 6800 | Step: 86800 | Train Loss: 0.06126968 |\n",
      "Epoch: 4 | Iter: 6900 | Step: 86900 | Train Loss: 0.56387138 |\n",
      "Epoch: 4 | Iter: 7000 | Step: 87000 | Train Loss: 0.06465506 |\n",
      "Epoch: 4 | Iter: 7100 | Step: 87100 | Train Loss: 0.00026989 |\n",
      "Epoch: 4 | Iter: 7200 | Step: 87200 | Train Loss: 0.02026440 |\n",
      "Epoch: 4 | Iter: 7300 | Step: 87300 | Train Loss: 0.00050632 |\n",
      "Epoch: 4 | Iter: 7400 | Step: 87400 | Train Loss: 0.02918450 |\n",
      "Epoch: 4 | Iter: 7500 | Step: 87500 | Train Loss: 0.01116807 |\n",
      "Epoch: 4 | Iter: 7600 | Step: 87600 | Train Loss: 0.00000023 |\n",
      "Epoch: 4 | Iter: 7700 | Step: 87700 | Train Loss: 0.01718898 |\n",
      "Epoch: 4 | Iter: 7800 | Step: 87800 | Train Loss: 0.02253206 |\n",
      "Epoch: 4 | Iter: 7900 | Step: 87900 | Train Loss: 0.01757497 |\n",
      "Epoch: 4 | Iter: 8000 | Step: 88000 | Train Loss: 0.00082930 |\n",
      "Epoch: 4 | Iter: 8100 | Step: 88100 | Train Loss: 0.01540042 |\n",
      "Epoch: 4 | Iter: 8200 | Step: 88200 | Train Loss: 0.03246477 |\n",
      "Epoch: 4 | Iter: 8300 | Step: 88300 | Train Loss: 0.08718590 |\n",
      "Epoch: 4 | Iter: 8400 | Step: 88400 | Train Loss: 0.03476221 |\n",
      "Epoch: 4 | Iter: 8500 | Step: 88500 | Train Loss: 0.92093349 |\n",
      "Epoch: 4 | Iter: 8600 | Step: 88600 | Train Loss: 0.17792116 |\n",
      "Epoch: 4 | Iter: 8700 | Step: 88700 | Train Loss: 0.04090876 |\n",
      "Epoch: 4 | Iter: 8800 | Step: 88800 | Train Loss: 0.00017924 |\n",
      "Epoch: 4 | Iter: 8900 | Step: 88900 | Train Loss: 0.00014990 |\n",
      "Epoch: 4 | Iter: 9000 | Step: 89000 | Train Loss: 0.03371852 |\n",
      "Epoch: 4 | Iter: 9100 | Step: 89100 | Train Loss: 0.04658396 |\n",
      "Epoch: 4 | Iter: 9200 | Step: 89200 | Train Loss: 0.01742055 |\n",
      "Epoch: 4 | Iter: 9300 | Step: 89300 | Train Loss: 0.00001030 |\n",
      "Epoch: 4 | Iter: 9400 | Step: 89400 | Train Loss: 0.20370124 |\n",
      "Epoch: 4 | Iter: 9500 | Step: 89500 | Train Loss: 0.00028348 |\n",
      "Epoch: 4 | Iter: 9600 | Step: 89600 | Train Loss: 1.00380778 |\n",
      "Epoch: 4 | Iter: 9700 | Step: 89700 | Train Loss: 0.00605832 |\n",
      "Epoch: 4 | Iter: 9800 | Step: 89800 | Train Loss: 0.00614378 |\n",
      "Epoch: 4 | Iter: 9900 | Step: 89900 | Train Loss: 0.00002402 |\n",
      "Epoch: 4 | Iter: 10000 | Step: 90000 | Train Loss: 0.06791984 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:15, 47.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Iter: 10000 | Step: 90000 | Val Loss: 0.02382821\n",
      "Epoch: 4 | Iter: 10100 | Step: 90100 | Train Loss: 0.04987216 |\n",
      "Epoch: 4 | Iter: 10200 | Step: 90200 | Train Loss: 0.00007353 |\n",
      "Epoch: 4 | Iter: 10300 | Step: 90300 | Train Loss: 0.01540658 |\n",
      "Epoch: 4 | Iter: 10400 | Step: 90400 | Train Loss: 0.04191998 |\n",
      "Epoch: 4 | Iter: 10500 | Step: 90500 | Train Loss: 0.05996630 |\n",
      "Epoch: 4 | Iter: 10600 | Step: 90600 | Train Loss: 0.00002246 |\n",
      "Epoch: 4 | Iter: 10700 | Step: 90700 | Train Loss: 0.02542534 |\n",
      "Epoch: 4 | Iter: 10800 | Step: 90800 | Train Loss: 0.00583183 |\n",
      "Epoch: 4 | Iter: 10900 | Step: 90900 | Train Loss: 0.09985776 |\n",
      "Epoch: 4 | Iter: 11000 | Step: 91000 | Train Loss: 0.02849961 |\n",
      "Epoch: 4 | Iter: 11100 | Step: 91100 | Train Loss: 0.35743433 |\n",
      "Epoch: 4 | Iter: 11200 | Step: 91200 | Train Loss: 0.89765358 |\n",
      "Epoch: 4 | Iter: 11300 | Step: 91300 | Train Loss: 0.04333216 |\n",
      "Epoch: 4 | Iter: 11400 | Step: 91400 | Train Loss: 0.00000281 |\n",
      "Epoch: 4 | Iter: 11500 | Step: 91500 | Train Loss: 0.01648570 |\n",
      "Epoch: 4 | Iter: 11600 | Step: 91600 | Train Loss: 0.00004073 |\n",
      "Epoch: 4 | Iter: 11700 | Step: 91700 | Train Loss: 0.04167127 |\n",
      "Epoch: 4 | Iter: 11800 | Step: 91800 | Train Loss: 0.00052510 |\n",
      "Epoch: 4 | Iter: 11900 | Step: 91900 | Train Loss: 0.01509157 |\n",
      "Epoch: 4 | Iter: 12000 | Step: 92000 | Train Loss: 0.00018712 |\n",
      "Epoch: 4 | Iter: 12100 | Step: 92100 | Train Loss: 0.00718046 |\n",
      "Epoch: 4 | Iter: 12200 | Step: 92200 | Train Loss: 0.08404652 |\n",
      "Epoch: 4 | Iter: 12300 | Step: 92300 | Train Loss: 0.00090561 |\n",
      "Epoch: 4 | Iter: 12400 | Step: 92400 | Train Loss: 0.05606454 |\n",
      "Epoch: 4 | Iter: 12500 | Step: 92500 | Train Loss: 0.04743979 |\n",
      "Epoch: 4 | Iter: 12600 | Step: 92600 | Train Loss: 0.02629449 |\n",
      "Epoch: 4 | Iter: 12700 | Step: 92700 | Train Loss: 0.00009806 |\n",
      "Epoch: 4 | Iter: 12800 | Step: 92800 | Train Loss: 0.01740131 |\n",
      "Epoch: 4 | Iter: 12900 | Step: 92900 | Train Loss: 0.06958985 |\n",
      "Epoch: 4 | Iter: 13000 | Step: 93000 | Train Loss: 0.00672874 |\n",
      "Epoch: 4 | Iter: 13100 | Step: 93100 | Train Loss: 0.00638511 |\n",
      "Epoch: 4 | Iter: 13200 | Step: 93200 | Train Loss: 0.00111696 |\n",
      "Epoch: 4 | Iter: 13300 | Step: 93300 | Train Loss: 0.00005013 |\n",
      "Epoch: 4 | Iter: 13400 | Step: 93400 | Train Loss: 0.14021584 |\n",
      "Epoch: 4 | Iter: 13500 | Step: 93500 | Train Loss: 0.02701674 |\n",
      "Epoch: 4 | Iter: 13600 | Step: 93600 | Train Loss: 0.22879727 |\n",
      "Epoch: 4 | Iter: 13700 | Step: 93700 | Train Loss: 0.00896977 |\n",
      "Epoch: 4 | Iter: 13800 | Step: 93800 | Train Loss: 0.23525883 |\n",
      "Epoch: 4 | Iter: 13900 | Step: 93900 | Train Loss: 0.03040716 |\n",
      "Epoch: 4 | Iter: 14000 | Step: 94000 | Train Loss: 1.44064021 |\n",
      "Epoch: 4 | Iter: 14100 | Step: 94100 | Train Loss: 0.02615731 |\n",
      "Epoch: 4 | Iter: 14200 | Step: 94200 | Train Loss: 0.02344729 |\n",
      "Epoch: 4 | Iter: 14300 | Step: 94300 | Train Loss: 0.00277585 |\n",
      "Epoch: 4 | Iter: 14400 | Step: 94400 | Train Loss: 0.06085874 |\n",
      "Epoch: 4 | Iter: 14500 | Step: 94500 | Train Loss: 0.05934044 |\n",
      "Epoch: 4 | Iter: 14600 | Step: 94600 | Train Loss: 0.02808086 |\n",
      "Epoch: 4 | Iter: 14700 | Step: 94700 | Train Loss: 0.24593724 |\n",
      "Epoch: 4 | Iter: 14800 | Step: 94800 | Train Loss: 0.00236262 |\n",
      "Epoch: 4 | Iter: 14900 | Step: 94900 | Train Loss: 0.04821524 |\n",
      "Epoch: 4 | Iter: 15000 | Step: 95000 | Train Loss: 0.00512384 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:16, 47.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Iter: 15000 | Step: 95000 | Val Loss: 0.02505268\n",
      "Epoch: 4 | Iter: 15100 | Step: 95100 | Train Loss: 0.02093351 |\n",
      "Epoch: 4 | Iter: 15200 | Step: 95200 | Train Loss: 0.00004480 |\n",
      "Epoch: 4 | Iter: 15300 | Step: 95300 | Train Loss: 0.00028878 |\n",
      "Epoch: 4 | Iter: 15400 | Step: 95400 | Train Loss: 0.02855309 |\n",
      "Epoch: 4 | Iter: 15500 | Step: 95500 | Train Loss: 0.00148324 |\n",
      "Epoch: 4 | Iter: 15600 | Step: 95600 | Train Loss: 0.06216514 |\n",
      "Epoch: 4 | Iter: 15700 | Step: 95700 | Train Loss: 0.01832551 |\n",
      "Epoch: 4 | Iter: 15800 | Step: 95800 | Train Loss: 0.00593638 |\n",
      "Epoch: 4 | Iter: 15900 | Step: 95900 | Train Loss: 0.00058163 |\n",
      "Epoch: 4 | Iter: 16000 | Step: 96000 | Train Loss: 0.00006959 |\n",
      "Epoch: 4 | Iter: 16100 | Step: 96100 | Train Loss: 0.19479465 |\n",
      "Epoch: 4 | Iter: 16200 | Step: 96200 | Train Loss: 0.03118745 |\n",
      "Epoch: 4 | Iter: 16300 | Step: 96300 | Train Loss: 0.00020060 |\n",
      "Epoch: 4 | Iter: 16400 | Step: 96400 | Train Loss: 0.00064678 |\n",
      "Epoch: 4 | Iter: 16500 | Step: 96500 | Train Loss: 0.05238090 |\n",
      "Epoch: 4 | Iter: 16600 | Step: 96600 | Train Loss: 0.03212305 |\n",
      "Epoch: 4 | Iter: 16700 | Step: 96700 | Train Loss: 0.03044055 |\n",
      "Epoch: 4 | Iter: 16800 | Step: 96800 | Train Loss: 0.20653145 |\n",
      "Epoch: 4 | Iter: 16900 | Step: 96900 | Train Loss: 0.03201553 |\n",
      "Epoch: 4 | Iter: 17000 | Step: 97000 | Train Loss: 0.04566723 |\n",
      "Epoch: 4 | Iter: 17100 | Step: 97100 | Train Loss: 0.01596686 |\n",
      "Epoch: 4 | Iter: 17200 | Step: 97200 | Train Loss: 0.05657692 |\n",
      "Epoch: 4 | Iter: 17300 | Step: 97300 | Train Loss: 1.18865240 |\n",
      "Epoch: 4 | Iter: 17400 | Step: 97400 | Train Loss: 0.12692350 |\n",
      "Epoch: 4 | Iter: 17500 | Step: 97500 | Train Loss: 0.01503383 |\n",
      "Epoch: 4 | Iter: 17600 | Step: 97600 | Train Loss: 0.01092022 |\n",
      "Epoch: 4 | Iter: 17700 | Step: 97700 | Train Loss: 0.01940524 |\n",
      "Epoch: 4 | Iter: 17800 | Step: 97800 | Train Loss: 0.06949363 |\n",
      "Epoch: 4 | Iter: 17900 | Step: 97900 | Train Loss: 0.16048285 |\n",
      "Epoch: 4 | Iter: 18000 | Step: 98000 | Train Loss: 0.00000627 |\n",
      "Epoch: 4 | Iter: 18100 | Step: 98100 | Train Loss: 0.00006291 |\n",
      "Epoch: 4 | Iter: 18200 | Step: 98200 | Train Loss: 0.00827074 |\n",
      "Epoch: 4 | Iter: 18300 | Step: 98300 | Train Loss: 0.00013370 |\n",
      "Epoch: 4 | Iter: 18400 | Step: 98400 | Train Loss: 0.00005374 |\n",
      "Epoch: 4 | Iter: 18500 | Step: 98500 | Train Loss: 0.00059945 |\n",
      "Epoch: 4 | Iter: 18600 | Step: 98600 | Train Loss: 0.00009890 |\n",
      "Epoch: 4 | Iter: 18700 | Step: 98700 | Train Loss: 0.05911177 |\n",
      "Epoch: 4 | Iter: 18800 | Step: 98800 | Train Loss: 1.78417075 |\n",
      "Epoch: 4 | Iter: 18900 | Step: 98900 | Train Loss: 0.05179440 |\n",
      "Epoch: 4 | Iter: 19000 | Step: 99000 | Train Loss: 0.03943155 |\n",
      "Epoch: 4 | Iter: 19100 | Step: 99100 | Train Loss: 0.03636174 |\n",
      "Epoch: 4 | Iter: 19200 | Step: 99200 | Train Loss: 0.99394667 |\n",
      "Epoch: 4 | Iter: 19300 | Step: 99300 | Train Loss: 0.03116353 |\n",
      "Epoch: 4 | Iter: 19400 | Step: 99400 | Train Loss: 0.02694674 |\n",
      "Epoch: 4 | Iter: 19500 | Step: 99500 | Train Loss: 0.11800940 |\n",
      "Epoch: 4 | Iter: 19600 | Step: 99600 | Train Loss: 0.00322281 |\n",
      "Epoch: 4 | Iter: 19700 | Step: 99700 | Train Loss: 0.09170916 |\n",
      "Epoch: 4 | Iter: 19800 | Step: 99800 | Train Loss: 0.00141278 |\n",
      "Epoch: 4 | Iter: 19900 | Step: 99900 | Train Loss: 0.06306762 |\n",
      "Epoch: 5 | Iter: 0 | Step: 100000 | Train Loss: 0.03471101 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:23, 45.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Iter: 0 | Step: 100000 | Val Loss: 0.02334317\n",
      "Epoch: 5 | Iter: 100 | Step: 100100 | Train Loss: 0.01957796 |\n",
      "Epoch: 5 | Iter: 200 | Step: 100200 | Train Loss: 0.02286526 |\n",
      "Epoch: 5 | Iter: 300 | Step: 100300 | Train Loss: 0.00126320 |\n",
      "Epoch: 5 | Iter: 400 | Step: 100400 | Train Loss: 0.04494100 |\n",
      "Epoch: 5 | Iter: 500 | Step: 100500 | Train Loss: 0.22096863 |\n",
      "Epoch: 5 | Iter: 600 | Step: 100600 | Train Loss: 0.02940943 |\n",
      "Epoch: 5 | Iter: 700 | Step: 100700 | Train Loss: 0.00176693 |\n",
      "Epoch: 5 | Iter: 800 | Step: 100800 | Train Loss: 0.03642932 |\n",
      "Epoch: 5 | Iter: 900 | Step: 100900 | Train Loss: 0.00073685 |\n",
      "Epoch: 5 | Iter: 1000 | Step: 101000 | Train Loss: 0.03185358 |\n",
      "Epoch: 5 | Iter: 1100 | Step: 101100 | Train Loss: 0.03354687 |\n",
      "Epoch: 5 | Iter: 1200 | Step: 101200 | Train Loss: 0.00000174 |\n",
      "Epoch: 5 | Iter: 1300 | Step: 101300 | Train Loss: 0.05740089 |\n",
      "Epoch: 5 | Iter: 1400 | Step: 101400 | Train Loss: 0.04234956 |\n",
      "Epoch: 5 | Iter: 1500 | Step: 101500 | Train Loss: 0.00007005 |\n",
      "Epoch: 5 | Iter: 1600 | Step: 101600 | Train Loss: 0.01412091 |\n",
      "Epoch: 5 | Iter: 1700 | Step: 101700 | Train Loss: 0.09976342 |\n",
      "Epoch: 5 | Iter: 1800 | Step: 101800 | Train Loss: 0.01310040 |\n",
      "Epoch: 5 | Iter: 1900 | Step: 101900 | Train Loss: 0.05040932 |\n",
      "Epoch: 5 | Iter: 2000 | Step: 102000 | Train Loss: 0.04445858 |\n",
      "Epoch: 5 | Iter: 2100 | Step: 102100 | Train Loss: 0.01088459 |\n",
      "Epoch: 5 | Iter: 2200 | Step: 102200 | Train Loss: 0.00222417 |\n",
      "Epoch: 5 | Iter: 2300 | Step: 102300 | Train Loss: 0.03822533 |\n",
      "Epoch: 5 | Iter: 2400 | Step: 102400 | Train Loss: 0.05846900 |\n",
      "Epoch: 5 | Iter: 2500 | Step: 102500 | Train Loss: 0.00020307 |\n",
      "Epoch: 5 | Iter: 2600 | Step: 102600 | Train Loss: 0.03950957 |\n",
      "Epoch: 5 | Iter: 2700 | Step: 102700 | Train Loss: 0.00059646 |\n",
      "Epoch: 5 | Iter: 2800 | Step: 102800 | Train Loss: 0.00122892 |\n",
      "Epoch: 5 | Iter: 2900 | Step: 102900 | Train Loss: 0.06025351 |\n",
      "Epoch: 5 | Iter: 3000 | Step: 103000 | Train Loss: 0.03874555 |\n",
      "Epoch: 5 | Iter: 3100 | Step: 103100 | Train Loss: 0.03530945 |\n",
      "Epoch: 5 | Iter: 3200 | Step: 103200 | Train Loss: 0.02728759 |\n",
      "Epoch: 5 | Iter: 3300 | Step: 103300 | Train Loss: 0.03493428 |\n",
      "Epoch: 5 | Iter: 3400 | Step: 103400 | Train Loss: 0.03629176 |\n",
      "Epoch: 5 | Iter: 3500 | Step: 103500 | Train Loss: 0.04972350 |\n",
      "Epoch: 5 | Iter: 3600 | Step: 103600 | Train Loss: 0.00024937 |\n",
      "Epoch: 5 | Iter: 3700 | Step: 103700 | Train Loss: 0.02213587 |\n",
      "Epoch: 5 | Iter: 3800 | Step: 103800 | Train Loss: 0.07201284 |\n",
      "Epoch: 5 | Iter: 3900 | Step: 103900 | Train Loss: 0.09679966 |\n",
      "Epoch: 5 | Iter: 4000 | Step: 104000 | Train Loss: 0.29867089 |\n",
      "Epoch: 5 | Iter: 4100 | Step: 104100 | Train Loss: 0.03705273 |\n",
      "Epoch: 5 | Iter: 4200 | Step: 104200 | Train Loss: 0.00107172 |\n",
      "Epoch: 5 | Iter: 4300 | Step: 104300 | Train Loss: 0.00003543 |\n",
      "Epoch: 5 | Iter: 4400 | Step: 104400 | Train Loss: 0.00586853 |\n",
      "Epoch: 5 | Iter: 4500 | Step: 104500 | Train Loss: 0.00670231 |\n",
      "Epoch: 5 | Iter: 4600 | Step: 104600 | Train Loss: 0.00391124 |\n",
      "Epoch: 5 | Iter: 4700 | Step: 104700 | Train Loss: 0.13665698 |\n",
      "Epoch: 5 | Iter: 4800 | Step: 104800 | Train Loss: 0.01593043 |\n",
      "Epoch: 5 | Iter: 4900 | Step: 104900 | Train Loss: 0.00132335 |\n",
      "Epoch: 5 | Iter: 5000 | Step: 105000 | Train Loss: 0.04802847 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:26, 45.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Iter: 5000 | Step: 105000 | Val Loss: 0.02447559\n",
      "Epoch: 5 | Iter: 5100 | Step: 105100 | Train Loss: 0.01962682 |\n",
      "Epoch: 5 | Iter: 5200 | Step: 105200 | Train Loss: 0.03297362 |\n",
      "Epoch: 5 | Iter: 5300 | Step: 105300 | Train Loss: 0.00278325 |\n",
      "Epoch: 5 | Iter: 5400 | Step: 105400 | Train Loss: 0.00975966 |\n",
      "Epoch: 5 | Iter: 5500 | Step: 105500 | Train Loss: 0.05412839 |\n",
      "Epoch: 5 | Iter: 5600 | Step: 105600 | Train Loss: 0.65664744 |\n",
      "Epoch: 5 | Iter: 5700 | Step: 105700 | Train Loss: 0.01825169 |\n",
      "Epoch: 5 | Iter: 5800 | Step: 105800 | Train Loss: 0.02455198 |\n",
      "Epoch: 5 | Iter: 5900 | Step: 105900 | Train Loss: 0.01767063 |\n",
      "Epoch: 5 | Iter: 6000 | Step: 106000 | Train Loss: 0.03277534 |\n",
      "Epoch: 5 | Iter: 6100 | Step: 106100 | Train Loss: 0.00049416 |\n",
      "Epoch: 5 | Iter: 6200 | Step: 106200 | Train Loss: 0.00779246 |\n",
      "Epoch: 5 | Iter: 6300 | Step: 106300 | Train Loss: 0.06420943 |\n",
      "Epoch: 5 | Iter: 6400 | Step: 106400 | Train Loss: 0.01803196 |\n",
      "Epoch: 5 | Iter: 6500 | Step: 106500 | Train Loss: 0.00578691 |\n",
      "Epoch: 5 | Iter: 6600 | Step: 106600 | Train Loss: 0.00026822 |\n",
      "Epoch: 5 | Iter: 6700 | Step: 106700 | Train Loss: 0.03457373 |\n",
      "Epoch: 5 | Iter: 6800 | Step: 106800 | Train Loss: 0.32679513 |\n",
      "Epoch: 5 | Iter: 6900 | Step: 106900 | Train Loss: 0.26855525 |\n",
      "Epoch: 5 | Iter: 7000 | Step: 107000 | Train Loss: 0.00001682 |\n",
      "Epoch: 5 | Iter: 7100 | Step: 107100 | Train Loss: 0.02582266 |\n",
      "Epoch: 5 | Iter: 7200 | Step: 107200 | Train Loss: 0.03681884 |\n",
      "Epoch: 5 | Iter: 7300 | Step: 107300 | Train Loss: 0.03843660 |\n",
      "Epoch: 5 | Iter: 7400 | Step: 107400 | Train Loss: 0.02818688 |\n",
      "Epoch: 5 | Iter: 7500 | Step: 107500 | Train Loss: 0.00031843 |\n",
      "Epoch: 5 | Iter: 7600 | Step: 107600 | Train Loss: 0.05851953 |\n",
      "Epoch: 5 | Iter: 7700 | Step: 107700 | Train Loss: 0.00039926 |\n",
      "Epoch: 5 | Iter: 7800 | Step: 107800 | Train Loss: 0.08641453 |\n",
      "Epoch: 5 | Iter: 7900 | Step: 107900 | Train Loss: 0.02891020 |\n",
      "Epoch: 5 | Iter: 8000 | Step: 108000 | Train Loss: 0.02357906 |\n",
      "Epoch: 5 | Iter: 8100 | Step: 108100 | Train Loss: 0.01204743 |\n",
      "Epoch: 5 | Iter: 8200 | Step: 108200 | Train Loss: 0.04642453 |\n",
      "Epoch: 5 | Iter: 8300 | Step: 108300 | Train Loss: 0.00230076 |\n",
      "Epoch: 5 | Iter: 8400 | Step: 108400 | Train Loss: 0.03040189 |\n",
      "Epoch: 5 | Iter: 8500 | Step: 108500 | Train Loss: 0.84246868 |\n",
      "Epoch: 5 | Iter: 8600 | Step: 108600 | Train Loss: 0.14418194 |\n",
      "Epoch: 5 | Iter: 8700 | Step: 108700 | Train Loss: 0.03052838 |\n",
      "Epoch: 5 | Iter: 8800 | Step: 108800 | Train Loss: 0.04247390 |\n",
      "Epoch: 5 | Iter: 8900 | Step: 108900 | Train Loss: 0.01240679 |\n",
      "Epoch: 5 | Iter: 9000 | Step: 109000 | Train Loss: 0.00703399 |\n",
      "Epoch: 5 | Iter: 9100 | Step: 109100 | Train Loss: 0.03046906 |\n",
      "Epoch: 5 | Iter: 9200 | Step: 109200 | Train Loss: 0.00284796 |\n",
      "Epoch: 5 | Iter: 9300 | Step: 109300 | Train Loss: 0.00114260 |\n",
      "Epoch: 5 | Iter: 9400 | Step: 109400 | Train Loss: 0.19140868 |\n",
      "Epoch: 5 | Iter: 9500 | Step: 109500 | Train Loss: 0.01719126 |\n",
      "Epoch: 5 | Iter: 9600 | Step: 109600 | Train Loss: 0.96828043 |\n",
      "Epoch: 5 | Iter: 9700 | Step: 109700 | Train Loss: 0.00852590 |\n",
      "Epoch: 5 | Iter: 9800 | Step: 109800 | Train Loss: 0.00050470 |\n",
      "Epoch: 5 | Iter: 9900 | Step: 109900 | Train Loss: 0.02606391 |\n",
      "Epoch: 5 | Iter: 10000 | Step: 110000 | Train Loss: 0.00064405 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:46, 41.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Iter: 10000 | Step: 110000 | Val Loss: 0.02433873\n",
      "Epoch: 5 | Iter: 10100 | Step: 110100 | Train Loss: 0.00077304 |\n",
      "Epoch: 5 | Iter: 10200 | Step: 110200 | Train Loss: 0.00000004 |\n",
      "Epoch: 5 | Iter: 10300 | Step: 110300 | Train Loss: 0.00027757 |\n",
      "Epoch: 5 | Iter: 10400 | Step: 110400 | Train Loss: 0.12720284 |\n",
      "Epoch: 5 | Iter: 10500 | Step: 110500 | Train Loss: 0.00668194 |\n",
      "Epoch: 5 | Iter: 10600 | Step: 110600 | Train Loss: 0.04400678 |\n",
      "Epoch: 5 | Iter: 10700 | Step: 110700 | Train Loss: 0.02249544 |\n",
      "Epoch: 5 | Iter: 10800 | Step: 110800 | Train Loss: 0.04582298 |\n",
      "Epoch: 5 | Iter: 10900 | Step: 110900 | Train Loss: 0.12564883 |\n",
      "Epoch: 5 | Iter: 11000 | Step: 111000 | Train Loss: 0.03721115 |\n",
      "Epoch: 5 | Iter: 11100 | Step: 111100 | Train Loss: 0.21466115 |\n",
      "Epoch: 5 | Iter: 11200 | Step: 111200 | Train Loss: 0.24036507 |\n",
      "Epoch: 5 | Iter: 11300 | Step: 111300 | Train Loss: 0.00119347 |\n",
      "Epoch: 5 | Iter: 11400 | Step: 111400 | Train Loss: 0.00062296 |\n",
      "Epoch: 5 | Iter: 11500 | Step: 111500 | Train Loss: 0.00226347 |\n",
      "Epoch: 5 | Iter: 11600 | Step: 111600 | Train Loss: 0.00447117 |\n",
      "Epoch: 5 | Iter: 11700 | Step: 111700 | Train Loss: 0.03476302 |\n",
      "Epoch: 5 | Iter: 11800 | Step: 111800 | Train Loss: 0.04093630 |\n",
      "Epoch: 5 | Iter: 11900 | Step: 111900 | Train Loss: 0.03923923 |\n",
      "Epoch: 5 | Iter: 12000 | Step: 112000 | Train Loss: 0.03595187 |\n",
      "Epoch: 5 | Iter: 12100 | Step: 112100 | Train Loss: 0.00249137 |\n",
      "Epoch: 5 | Iter: 12200 | Step: 112200 | Train Loss: 0.00708857 |\n",
      "Epoch: 5 | Iter: 12300 | Step: 112300 | Train Loss: 0.05775585 |\n",
      "Epoch: 5 | Iter: 12400 | Step: 112400 | Train Loss: 0.00788515 |\n",
      "Epoch: 5 | Iter: 12500 | Step: 112500 | Train Loss: 0.00003701 |\n",
      "Epoch: 5 | Iter: 12600 | Step: 112600 | Train Loss: 0.00098304 |\n",
      "Epoch: 5 | Iter: 12700 | Step: 112700 | Train Loss: 0.06025276 |\n",
      "Epoch: 5 | Iter: 12800 | Step: 112800 | Train Loss: 0.03461985 |\n",
      "Epoch: 5 | Iter: 12900 | Step: 112900 | Train Loss: 0.02170742 |\n",
      "Epoch: 5 | Iter: 13000 | Step: 113000 | Train Loss: 0.01636042 |\n",
      "Epoch: 5 | Iter: 13100 | Step: 113100 | Train Loss: 0.00283536 |\n",
      "Epoch: 5 | Iter: 13200 | Step: 113200 | Train Loss: 0.03379796 |\n",
      "Epoch: 5 | Iter: 13300 | Step: 113300 | Train Loss: 0.02471347 |\n",
      "Epoch: 5 | Iter: 13400 | Step: 113400 | Train Loss: 0.09127647 |\n",
      "Epoch: 5 | Iter: 13500 | Step: 113500 | Train Loss: 0.00009674 |\n",
      "Epoch: 5 | Iter: 13600 | Step: 113600 | Train Loss: 0.45550603 |\n",
      "Epoch: 5 | Iter: 13700 | Step: 113700 | Train Loss: 0.01752940 |\n",
      "Epoch: 5 | Iter: 13800 | Step: 113800 | Train Loss: 0.09727377 |\n",
      "Epoch: 5 | Iter: 13900 | Step: 113900 | Train Loss: 0.03937814 |\n",
      "Epoch: 5 | Iter: 14000 | Step: 114000 | Train Loss: 0.62763792 |\n",
      "Epoch: 5 | Iter: 14100 | Step: 114100 | Train Loss: 0.11325471 |\n",
      "Epoch: 5 | Iter: 14200 | Step: 114200 | Train Loss: 0.04642430 |\n",
      "Epoch: 5 | Iter: 14300 | Step: 114300 | Train Loss: 0.02071127 |\n",
      "Epoch: 5 | Iter: 14400 | Step: 114400 | Train Loss: 0.06361813 |\n",
      "Epoch: 5 | Iter: 14500 | Step: 114500 | Train Loss: 0.03244448 |\n",
      "Epoch: 5 | Iter: 14600 | Step: 114600 | Train Loss: 0.03318711 |\n",
      "Epoch: 5 | Iter: 14700 | Step: 114700 | Train Loss: 0.53366947 |\n",
      "Epoch: 5 | Iter: 14800 | Step: 114800 | Train Loss: 0.00053162 |\n",
      "Epoch: 5 | Iter: 14900 | Step: 114900 | Train Loss: 0.00012350 |\n",
      "Epoch: 5 | Iter: 15000 | Step: 115000 | Train Loss: 0.04573025 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:29, 44.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Iter: 15000 | Step: 115000 | Val Loss: 0.02414057\n",
      "Epoch: 5 | Iter: 15100 | Step: 115100 | Train Loss: 0.02272119 |\n",
      "Epoch: 5 | Iter: 15200 | Step: 115200 | Train Loss: 0.03982887 |\n",
      "Epoch: 5 | Iter: 15300 | Step: 115300 | Train Loss: 0.00111685 |\n",
      "Epoch: 5 | Iter: 15400 | Step: 115400 | Train Loss: 0.05966685 |\n",
      "Epoch: 5 | Iter: 15500 | Step: 115500 | Train Loss: 0.05650239 |\n",
      "Epoch: 5 | Iter: 15600 | Step: 115600 | Train Loss: 0.02364249 |\n",
      "Epoch: 5 | Iter: 15700 | Step: 115700 | Train Loss: 0.04569716 |\n",
      "Epoch: 5 | Iter: 15800 | Step: 115800 | Train Loss: 0.07842518 |\n",
      "Epoch: 5 | Iter: 15900 | Step: 115900 | Train Loss: 0.02488230 |\n",
      "Epoch: 5 | Iter: 16000 | Step: 116000 | Train Loss: 0.03256321 |\n",
      "Epoch: 5 | Iter: 16100 | Step: 116100 | Train Loss: 0.01272732 |\n",
      "Epoch: 5 | Iter: 16200 | Step: 116200 | Train Loss: 0.02650324 |\n",
      "Epoch: 5 | Iter: 16300 | Step: 116300 | Train Loss: 0.03690861 |\n",
      "Epoch: 5 | Iter: 16400 | Step: 116400 | Train Loss: 0.03244460 |\n",
      "Epoch: 5 | Iter: 16500 | Step: 116500 | Train Loss: 0.00017458 |\n",
      "Epoch: 5 | Iter: 16600 | Step: 116600 | Train Loss: 0.00001194 |\n",
      "Epoch: 5 | Iter: 16700 | Step: 116700 | Train Loss: 0.02968827 |\n",
      "Epoch: 5 | Iter: 16800 | Step: 116800 | Train Loss: 0.00038923 |\n",
      "Epoch: 5 | Iter: 16900 | Step: 116900 | Train Loss: 0.01702585 |\n",
      "Epoch: 5 | Iter: 17000 | Step: 117000 | Train Loss: 0.03731984 |\n",
      "Epoch: 5 | Iter: 17100 | Step: 117100 | Train Loss: 0.00066294 |\n",
      "Epoch: 5 | Iter: 17200 | Step: 117200 | Train Loss: 0.01768623 |\n",
      "Epoch: 5 | Iter: 17300 | Step: 117300 | Train Loss: 0.82252967 |\n",
      "Epoch: 5 | Iter: 17400 | Step: 117400 | Train Loss: 0.00982226 |\n",
      "Epoch: 5 | Iter: 17500 | Step: 117500 | Train Loss: 0.00132585 |\n",
      "Epoch: 5 | Iter: 17600 | Step: 117600 | Train Loss: 0.00010393 |\n",
      "Epoch: 5 | Iter: 17700 | Step: 117700 | Train Loss: 0.00090848 |\n",
      "Epoch: 5 | Iter: 17800 | Step: 117800 | Train Loss: 0.00307345 |\n",
      "Epoch: 5 | Iter: 17900 | Step: 117900 | Train Loss: 0.09720722 |\n",
      "Epoch: 5 | Iter: 18000 | Step: 118000 | Train Loss: 0.04151968 |\n",
      "Epoch: 5 | Iter: 18100 | Step: 118100 | Train Loss: 0.05154857 |\n",
      "Epoch: 5 | Iter: 18200 | Step: 118200 | Train Loss: 0.01333961 |\n",
      "Epoch: 5 | Iter: 18300 | Step: 118300 | Train Loss: 0.00008618 |\n",
      "Epoch: 5 | Iter: 18400 | Step: 118400 | Train Loss: 0.03072491 |\n",
      "Epoch: 5 | Iter: 18500 | Step: 118500 | Train Loss: 0.00504462 |\n",
      "Epoch: 5 | Iter: 18600 | Step: 118600 | Train Loss: 0.04166137 |\n",
      "Epoch: 5 | Iter: 18700 | Step: 118700 | Train Loss: 0.01521794 |\n",
      "Epoch: 5 | Iter: 18800 | Step: 118800 | Train Loss: 1.13430810 |\n",
      "Epoch: 5 | Iter: 18900 | Step: 118900 | Train Loss: 0.02976316 |\n",
      "Epoch: 5 | Iter: 19000 | Step: 119000 | Train Loss: 0.06222317 |\n",
      "Epoch: 5 | Iter: 19100 | Step: 119100 | Train Loss: 0.02969587 |\n",
      "Epoch: 5 | Iter: 19200 | Step: 119200 | Train Loss: 1.43407416 |\n",
      "Epoch: 5 | Iter: 19300 | Step: 119300 | Train Loss: 0.03723477 |\n",
      "Epoch: 5 | Iter: 19400 | Step: 119400 | Train Loss: 0.04507504 |\n",
      "Epoch: 5 | Iter: 19500 | Step: 119500 | Train Loss: 0.17715763 |\n",
      "Epoch: 5 | Iter: 19600 | Step: 119600 | Train Loss: 0.00101931 |\n",
      "Epoch: 5 | Iter: 19700 | Step: 119700 | Train Loss: 0.01712164 |\n",
      "Epoch: 5 | Iter: 19800 | Step: 119800 | Train Loss: 0.05258323 |\n",
      "Epoch: 5 | Iter: 19900 | Step: 119900 | Train Loss: 0.24437764 |\n",
      "Epoch: 6 | Iter: 0 | Step: 120000 | Train Loss: 0.00588518 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:27, 44.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Iter: 0 | Step: 120000 | Val Loss: 0.02217950\n",
      "Epoch: 6 | Iter: 100 | Step: 120100 | Train Loss: 0.03443396 |\n",
      "Epoch: 6 | Iter: 200 | Step: 120200 | Train Loss: 0.04368765 |\n",
      "Epoch: 6 | Iter: 300 | Step: 120300 | Train Loss: 0.02370217 |\n",
      "Epoch: 6 | Iter: 400 | Step: 120400 | Train Loss: 0.02898224 |\n",
      "Epoch: 6 | Iter: 500 | Step: 120500 | Train Loss: 0.00929543 |\n",
      "Epoch: 6 | Iter: 600 | Step: 120600 | Train Loss: 0.00017131 |\n",
      "Epoch: 6 | Iter: 700 | Step: 120700 | Train Loss: 0.02735311 |\n",
      "Epoch: 6 | Iter: 800 | Step: 120800 | Train Loss: 0.01745017 |\n",
      "Epoch: 6 | Iter: 900 | Step: 120900 | Train Loss: 0.00240735 |\n",
      "Epoch: 6 | Iter: 1000 | Step: 121000 | Train Loss: 0.04050361 |\n",
      "Epoch: 6 | Iter: 1100 | Step: 121100 | Train Loss: 0.01088991 |\n",
      "Epoch: 6 | Iter: 1200 | Step: 121200 | Train Loss: 0.06640468 |\n",
      "Epoch: 6 | Iter: 1300 | Step: 121300 | Train Loss: 0.03546833 |\n",
      "Epoch: 6 | Iter: 1400 | Step: 121400 | Train Loss: 0.04800387 |\n",
      "Epoch: 6 | Iter: 1500 | Step: 121500 | Train Loss: 0.01951113 |\n",
      "Epoch: 6 | Iter: 1600 | Step: 121600 | Train Loss: 0.00986093 |\n",
      "Epoch: 6 | Iter: 1700 | Step: 121700 | Train Loss: 0.01772861 |\n",
      "Epoch: 6 | Iter: 1800 | Step: 121800 | Train Loss: 0.05919940 |\n",
      "Epoch: 6 | Iter: 1900 | Step: 121900 | Train Loss: 0.04909321 |\n",
      "Epoch: 6 | Iter: 2000 | Step: 122000 | Train Loss: 0.03436754 |\n",
      "Epoch: 6 | Iter: 2100 | Step: 122100 | Train Loss: 0.02543703 |\n",
      "Epoch: 6 | Iter: 2200 | Step: 122200 | Train Loss: 0.00171102 |\n",
      "Epoch: 6 | Iter: 2300 | Step: 122300 | Train Loss: 0.00113635 |\n",
      "Epoch: 6 | Iter: 2400 | Step: 122400 | Train Loss: 0.01629563 |\n",
      "Epoch: 6 | Iter: 2500 | Step: 122500 | Train Loss: 0.00011294 |\n",
      "Epoch: 6 | Iter: 2600 | Step: 122600 | Train Loss: 0.03502787 |\n",
      "Epoch: 6 | Iter: 2700 | Step: 122700 | Train Loss: 0.00063318 |\n",
      "Epoch: 6 | Iter: 2800 | Step: 122800 | Train Loss: 0.00008408 |\n",
      "Epoch: 6 | Iter: 2900 | Step: 122900 | Train Loss: 0.04368402 |\n",
      "Epoch: 6 | Iter: 3000 | Step: 123000 | Train Loss: 0.00603136 |\n",
      "Epoch: 6 | Iter: 3100 | Step: 123100 | Train Loss: 0.05201397 |\n",
      "Epoch: 6 | Iter: 3200 | Step: 123200 | Train Loss: 0.00570976 |\n",
      "Epoch: 6 | Iter: 3300 | Step: 123300 | Train Loss: 0.02022347 |\n",
      "Epoch: 6 | Iter: 3400 | Step: 123400 | Train Loss: 0.01231546 |\n",
      "Epoch: 6 | Iter: 3500 | Step: 123500 | Train Loss: 0.02756153 |\n",
      "Epoch: 6 | Iter: 3600 | Step: 123600 | Train Loss: 0.03785755 |\n",
      "Epoch: 6 | Iter: 3700 | Step: 123700 | Train Loss: 0.05965067 |\n",
      "Epoch: 6 | Iter: 3800 | Step: 123800 | Train Loss: 0.20085138 |\n",
      "Epoch: 6 | Iter: 3900 | Step: 123900 | Train Loss: 0.00028800 |\n",
      "Epoch: 6 | Iter: 4000 | Step: 124000 | Train Loss: 1.37616277 |\n",
      "Epoch: 6 | Iter: 4100 | Step: 124100 | Train Loss: 0.03774203 |\n",
      "Epoch: 6 | Iter: 4200 | Step: 124200 | Train Loss: 0.01901801 |\n",
      "Epoch: 6 | Iter: 4300 | Step: 124300 | Train Loss: 0.00265663 |\n",
      "Epoch: 6 | Iter: 4400 | Step: 124400 | Train Loss: 0.02146353 |\n",
      "Epoch: 6 | Iter: 4500 | Step: 124500 | Train Loss: 0.07474714 |\n",
      "Epoch: 6 | Iter: 4600 | Step: 124600 | Train Loss: 0.02941937 |\n",
      "Epoch: 6 | Iter: 4700 | Step: 124700 | Train Loss: 0.11563026 |\n",
      "Epoch: 6 | Iter: 4800 | Step: 124800 | Train Loss: 0.03722451 |\n",
      "Epoch: 6 | Iter: 4900 | Step: 124900 | Train Loss: 0.00095777 |\n",
      "Epoch: 6 | Iter: 5000 | Step: 125000 | Train Loss: 0.05984025 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:25, 45.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Iter: 5000 | Step: 125000 | Val Loss: 0.02354964\n",
      "Epoch: 6 | Iter: 5100 | Step: 125100 | Train Loss: 0.01622259 |\n",
      "Epoch: 6 | Iter: 5200 | Step: 125200 | Train Loss: 0.00895120 |\n",
      "Epoch: 6 | Iter: 5300 | Step: 125300 | Train Loss: 0.05167037 |\n",
      "Epoch: 6 | Iter: 5400 | Step: 125400 | Train Loss: 0.00000173 |\n",
      "Epoch: 6 | Iter: 5500 | Step: 125500 | Train Loss: 0.00003067 |\n",
      "Epoch: 6 | Iter: 5600 | Step: 125600 | Train Loss: 1.00369751 |\n",
      "Epoch: 6 | Iter: 5700 | Step: 125700 | Train Loss: 0.02980949 |\n",
      "Epoch: 6 | Iter: 5800 | Step: 125800 | Train Loss: 0.02421186 |\n",
      "Epoch: 6 | Iter: 5900 | Step: 125900 | Train Loss: 0.13132252 |\n",
      "Epoch: 6 | Iter: 6000 | Step: 126000 | Train Loss: 0.03032793 |\n",
      "Epoch: 6 | Iter: 6100 | Step: 126100 | Train Loss: 0.00015590 |\n",
      "Epoch: 6 | Iter: 6200 | Step: 126200 | Train Loss: 0.00774791 |\n",
      "Epoch: 6 | Iter: 6300 | Step: 126300 | Train Loss: 0.02349452 |\n",
      "Epoch: 6 | Iter: 6400 | Step: 126400 | Train Loss: 0.04434239 |\n",
      "Epoch: 6 | Iter: 6500 | Step: 126500 | Train Loss: 0.07048474 |\n",
      "Epoch: 6 | Iter: 6600 | Step: 126600 | Train Loss: 0.00341969 |\n",
      "Epoch: 6 | Iter: 6700 | Step: 126700 | Train Loss: 0.00081954 |\n",
      "Epoch: 6 | Iter: 6800 | Step: 126800 | Train Loss: 0.41343835 |\n",
      "Epoch: 6 | Iter: 6900 | Step: 126900 | Train Loss: 0.19082467 |\n",
      "Epoch: 6 | Iter: 7000 | Step: 127000 | Train Loss: 0.00092841 |\n",
      "Epoch: 6 | Iter: 7100 | Step: 127100 | Train Loss: 0.02935463 |\n",
      "Epoch: 6 | Iter: 7200 | Step: 127200 | Train Loss: 0.00015093 |\n",
      "Epoch: 6 | Iter: 7300 | Step: 127300 | Train Loss: 0.00221245 |\n",
      "Epoch: 6 | Iter: 7400 | Step: 127400 | Train Loss: 0.00009556 |\n",
      "Epoch: 6 | Iter: 7500 | Step: 127500 | Train Loss: 0.00110408 |\n",
      "Epoch: 6 | Iter: 7600 | Step: 127600 | Train Loss: 0.06038801 |\n",
      "Epoch: 6 | Iter: 7700 | Step: 127700 | Train Loss: 0.04625633 |\n",
      "Epoch: 6 | Iter: 7800 | Step: 127800 | Train Loss: 0.01560009 |\n",
      "Epoch: 6 | Iter: 7900 | Step: 127900 | Train Loss: 0.02218453 |\n",
      "Epoch: 6 | Iter: 8000 | Step: 128000 | Train Loss: 0.02470441 |\n",
      "Epoch: 6 | Iter: 8100 | Step: 128100 | Train Loss: 0.00523727 |\n",
      "Epoch: 6 | Iter: 8200 | Step: 128200 | Train Loss: 0.01913996 |\n",
      "Epoch: 6 | Iter: 8300 | Step: 128300 | Train Loss: 0.03719014 |\n",
      "Epoch: 6 | Iter: 8400 | Step: 128400 | Train Loss: 0.00142974 |\n",
      "Epoch: 6 | Iter: 8500 | Step: 128500 | Train Loss: 1.19067216 |\n",
      "Epoch: 6 | Iter: 8600 | Step: 128600 | Train Loss: 0.33180323 |\n",
      "Epoch: 6 | Iter: 8700 | Step: 128700 | Train Loss: 0.01187706 |\n",
      "Epoch: 6 | Iter: 8800 | Step: 128800 | Train Loss: 0.06664550 |\n",
      "Epoch: 6 | Iter: 8900 | Step: 128900 | Train Loss: 0.01589605 |\n",
      "Epoch: 6 | Iter: 9000 | Step: 129000 | Train Loss: 0.00053158 |\n",
      "Epoch: 6 | Iter: 9100 | Step: 129100 | Train Loss: 0.03011097 |\n",
      "Epoch: 6 | Iter: 9200 | Step: 129200 | Train Loss: 0.00251268 |\n",
      "Epoch: 6 | Iter: 9300 | Step: 129300 | Train Loss: 0.04327844 |\n",
      "Epoch: 6 | Iter: 9400 | Step: 129400 | Train Loss: 0.17943622 |\n",
      "Epoch: 6 | Iter: 9500 | Step: 129500 | Train Loss: 0.04113105 |\n",
      "Epoch: 6 | Iter: 9600 | Step: 129600 | Train Loss: 0.59985656 |\n",
      "Epoch: 6 | Iter: 9700 | Step: 129700 | Train Loss: 0.00725572 |\n",
      "Epoch: 6 | Iter: 9800 | Step: 129800 | Train Loss: 0.00508626 |\n",
      "Epoch: 6 | Iter: 9900 | Step: 129900 | Train Loss: 0.03272992 |\n",
      "Epoch: 6 | Iter: 10000 | Step: 130000 | Train Loss: 0.00249036 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:14, 47.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Iter: 10000 | Step: 130000 | Val Loss: 0.02432544\n",
      "Epoch: 6 | Iter: 10100 | Step: 130100 | Train Loss: 0.02787825 |\n",
      "Epoch: 6 | Iter: 10200 | Step: 130200 | Train Loss: 0.03644800 |\n",
      "Epoch: 6 | Iter: 10300 | Step: 130300 | Train Loss: 0.01603701 |\n",
      "Epoch: 6 | Iter: 10400 | Step: 130400 | Train Loss: 0.10215729 |\n",
      "Epoch: 6 | Iter: 10500 | Step: 130500 | Train Loss: 0.07368632 |\n",
      "Epoch: 6 | Iter: 10600 | Step: 130600 | Train Loss: 0.04324239 |\n",
      "Epoch: 6 | Iter: 10700 | Step: 130700 | Train Loss: 0.00738260 |\n",
      "Epoch: 6 | Iter: 10800 | Step: 130800 | Train Loss: 0.00039932 |\n",
      "Epoch: 6 | Iter: 10900 | Step: 130900 | Train Loss: 0.08861024 |\n",
      "Epoch: 6 | Iter: 11000 | Step: 131000 | Train Loss: 0.02627972 |\n",
      "Epoch: 6 | Iter: 11100 | Step: 131100 | Train Loss: 0.03980336 |\n",
      "Epoch: 6 | Iter: 11200 | Step: 131200 | Train Loss: 0.78501940 |\n",
      "Epoch: 6 | Iter: 11300 | Step: 131300 | Train Loss: 0.00781246 |\n",
      "Epoch: 6 | Iter: 11400 | Step: 131400 | Train Loss: 0.00317727 |\n",
      "Epoch: 6 | Iter: 11500 | Step: 131500 | Train Loss: 0.02928955 |\n",
      "Epoch: 6 | Iter: 11600 | Step: 131600 | Train Loss: 0.01660660 |\n",
      "Epoch: 6 | Iter: 11700 | Step: 131700 | Train Loss: 0.03785533 |\n",
      "Epoch: 6 | Iter: 11800 | Step: 131800 | Train Loss: 0.00082721 |\n",
      "Epoch: 6 | Iter: 11900 | Step: 131900 | Train Loss: 0.03198759 |\n",
      "Epoch: 6 | Iter: 12000 | Step: 132000 | Train Loss: 0.01977270 |\n",
      "Epoch: 6 | Iter: 12100 | Step: 132100 | Train Loss: 0.04119495 |\n",
      "Epoch: 6 | Iter: 12200 | Step: 132200 | Train Loss: 0.02061136 |\n",
      "Epoch: 6 | Iter: 12300 | Step: 132300 | Train Loss: 0.01067455 |\n",
      "Epoch: 6 | Iter: 12400 | Step: 132400 | Train Loss: 0.02186912 |\n",
      "Epoch: 6 | Iter: 12500 | Step: 132500 | Train Loss: 0.00396240 |\n",
      "Epoch: 6 | Iter: 12600 | Step: 132600 | Train Loss: 0.01847707 |\n",
      "Epoch: 6 | Iter: 12700 | Step: 132700 | Train Loss: 0.00003753 |\n",
      "Epoch: 6 | Iter: 12800 | Step: 132800 | Train Loss: 0.04171461 |\n",
      "Epoch: 6 | Iter: 12900 | Step: 132900 | Train Loss: 0.03683877 |\n",
      "Epoch: 6 | Iter: 13000 | Step: 133000 | Train Loss: 0.01995260 |\n",
      "Epoch: 6 | Iter: 13100 | Step: 133100 | Train Loss: 0.00702598 |\n",
      "Epoch: 6 | Iter: 13200 | Step: 133200 | Train Loss: 0.03690848 |\n",
      "Epoch: 6 | Iter: 13300 | Step: 133300 | Train Loss: 0.01773080 |\n",
      "Epoch: 6 | Iter: 13400 | Step: 133400 | Train Loss: 0.11101742 |\n",
      "Epoch: 6 | Iter: 13500 | Step: 133500 | Train Loss: 0.00027249 |\n",
      "Epoch: 6 | Iter: 13600 | Step: 133600 | Train Loss: 1.17502344 |\n",
      "Epoch: 6 | Iter: 13700 | Step: 133700 | Train Loss: 0.02644608 |\n",
      "Epoch: 6 | Iter: 13800 | Step: 133800 | Train Loss: 0.10640905 |\n",
      "Epoch: 6 | Iter: 13900 | Step: 133900 | Train Loss: 0.04202964 |\n",
      "Epoch: 6 | Iter: 14000 | Step: 134000 | Train Loss: 1.52776802 |\n",
      "Epoch: 6 | Iter: 14100 | Step: 134100 | Train Loss: 0.30072758 |\n",
      "Epoch: 6 | Iter: 14200 | Step: 134200 | Train Loss: 0.04926123 |\n",
      "Epoch: 6 | Iter: 14300 | Step: 134300 | Train Loss: 0.34361911 |\n",
      "Epoch: 6 | Iter: 14400 | Step: 134400 | Train Loss: 0.00020988 |\n",
      "Epoch: 6 | Iter: 14500 | Step: 134500 | Train Loss: 0.02209224 |\n",
      "Epoch: 6 | Iter: 14600 | Step: 134600 | Train Loss: 0.00084286 |\n",
      "Epoch: 6 | Iter: 14700 | Step: 134700 | Train Loss: 0.31881797 |\n",
      "Epoch: 6 | Iter: 14800 | Step: 134800 | Train Loss: 0.03336050 |\n",
      "Epoch: 6 | Iter: 14900 | Step: 134900 | Train Loss: 0.02310780 |\n",
      "Epoch: 6 | Iter: 15000 | Step: 135000 | Train Loss: 0.01516383 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:16, 47.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Iter: 15000 | Step: 135000 | Val Loss: 0.02284154\n",
      "Epoch: 6 | Iter: 15100 | Step: 135100 | Train Loss: 0.03002103 |\n",
      "Epoch: 6 | Iter: 15200 | Step: 135200 | Train Loss: 0.00093236 |\n",
      "Epoch: 6 | Iter: 15300 | Step: 135300 | Train Loss: 0.00236844 |\n",
      "Epoch: 6 | Iter: 15400 | Step: 135400 | Train Loss: 0.02449486 |\n",
      "Epoch: 6 | Iter: 15500 | Step: 135500 | Train Loss: 0.03119055 |\n",
      "Epoch: 6 | Iter: 15600 | Step: 135600 | Train Loss: 0.03362565 |\n",
      "Epoch: 6 | Iter: 15700 | Step: 135700 | Train Loss: 0.04529477 |\n",
      "Epoch: 6 | Iter: 15800 | Step: 135800 | Train Loss: 0.02475824 |\n",
      "Epoch: 6 | Iter: 15900 | Step: 135900 | Train Loss: 0.04169244 |\n",
      "Epoch: 6 | Iter: 16000 | Step: 136000 | Train Loss: 0.00070005 |\n",
      "Epoch: 6 | Iter: 16100 | Step: 136100 | Train Loss: 0.07487611 |\n",
      "Epoch: 6 | Iter: 16200 | Step: 136200 | Train Loss: 0.03733794 |\n",
      "Epoch: 6 | Iter: 16300 | Step: 136300 | Train Loss: 0.00022388 |\n",
      "Epoch: 6 | Iter: 16400 | Step: 136400 | Train Loss: 0.00066593 |\n",
      "Epoch: 6 | Iter: 16500 | Step: 136500 | Train Loss: 0.00010515 |\n",
      "Epoch: 6 | Iter: 16600 | Step: 136600 | Train Loss: 0.03381594 |\n",
      "Epoch: 6 | Iter: 16700 | Step: 136700 | Train Loss: 0.02558641 |\n",
      "Epoch: 6 | Iter: 16800 | Step: 136800 | Train Loss: 0.03660936 |\n",
      "Epoch: 6 | Iter: 16900 | Step: 136900 | Train Loss: 0.01407220 |\n",
      "Epoch: 6 | Iter: 17000 | Step: 137000 | Train Loss: 0.00121930 |\n",
      "Epoch: 6 | Iter: 17100 | Step: 137100 | Train Loss: 0.06247770 |\n",
      "Epoch: 6 | Iter: 17200 | Step: 137200 | Train Loss: 0.03625819 |\n",
      "Epoch: 6 | Iter: 17300 | Step: 137300 | Train Loss: 0.81121212 |\n",
      "Epoch: 6 | Iter: 17400 | Step: 137400 | Train Loss: 0.23060325 |\n",
      "Epoch: 6 | Iter: 17500 | Step: 137500 | Train Loss: 0.02972677 |\n",
      "Epoch: 6 | Iter: 17600 | Step: 137600 | Train Loss: 0.09063238 |\n",
      "Epoch: 6 | Iter: 17700 | Step: 137700 | Train Loss: 0.00426325 |\n",
      "Epoch: 6 | Iter: 17800 | Step: 137800 | Train Loss: 0.00438264 |\n",
      "Epoch: 6 | Iter: 17900 | Step: 137900 | Train Loss: 0.04298732 |\n",
      "Epoch: 6 | Iter: 18000 | Step: 138000 | Train Loss: 0.01236119 |\n",
      "Epoch: 6 | Iter: 18100 | Step: 138100 | Train Loss: 0.03545909 |\n",
      "Epoch: 6 | Iter: 18200 | Step: 138200 | Train Loss: 0.02979756 |\n",
      "Epoch: 6 | Iter: 18300 | Step: 138300 | Train Loss: 0.00000007 |\n",
      "Epoch: 6 | Iter: 18400 | Step: 138400 | Train Loss: 0.03174924 |\n",
      "Epoch: 6 | Iter: 18500 | Step: 138500 | Train Loss: 0.02648824 |\n",
      "Epoch: 6 | Iter: 18600 | Step: 138600 | Train Loss: 0.05473393 |\n",
      "Epoch: 6 | Iter: 18700 | Step: 138700 | Train Loss: 0.00062720 |\n",
      "Epoch: 6 | Iter: 18800 | Step: 138800 | Train Loss: 0.93357342 |\n",
      "Epoch: 6 | Iter: 18900 | Step: 138900 | Train Loss: 0.04048393 |\n",
      "Epoch: 6 | Iter: 19000 | Step: 139000 | Train Loss: 0.00310017 |\n",
      "Epoch: 6 | Iter: 19100 | Step: 139100 | Train Loss: 0.06732681 |\n",
      "Epoch: 6 | Iter: 19200 | Step: 139200 | Train Loss: 0.84312564 |\n",
      "Epoch: 6 | Iter: 19300 | Step: 139300 | Train Loss: 0.03860359 |\n",
      "Epoch: 6 | Iter: 19400 | Step: 139400 | Train Loss: 0.01422791 |\n",
      "Epoch: 6 | Iter: 19500 | Step: 139500 | Train Loss: 0.13296197 |\n",
      "Epoch: 6 | Iter: 19600 | Step: 139600 | Train Loss: 0.02144724 |\n",
      "Epoch: 6 | Iter: 19700 | Step: 139700 | Train Loss: 0.02801586 |\n",
      "Epoch: 6 | Iter: 19800 | Step: 139800 | Train Loss: 0.05481546 |\n",
      "Epoch: 6 | Iter: 19900 | Step: 139900 | Train Loss: 0.02797291 |\n",
      "Epoch: 7 | Iter: 0 | Step: 140000 | Train Loss: 0.00043507 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:52, 40.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Iter: 0 | Step: 140000 | Val Loss: 0.02297527\n",
      "Epoch: 7 | Iter: 100 | Step: 140100 | Train Loss: 0.00008779 |\n",
      "Epoch: 7 | Iter: 200 | Step: 140200 | Train Loss: 0.05719403 |\n",
      "Epoch: 7 | Iter: 300 | Step: 140300 | Train Loss: 0.01099283 |\n",
      "Epoch: 7 | Iter: 400 | Step: 140400 | Train Loss: 0.00127493 |\n",
      "Epoch: 7 | Iter: 500 | Step: 140500 | Train Loss: 0.01306496 |\n",
      "Epoch: 7 | Iter: 600 | Step: 140600 | Train Loss: 0.00128564 |\n",
      "Epoch: 7 | Iter: 700 | Step: 140700 | Train Loss: 0.02798847 |\n",
      "Epoch: 7 | Iter: 800 | Step: 140800 | Train Loss: 0.01751792 |\n",
      "Epoch: 7 | Iter: 900 | Step: 140900 | Train Loss: 0.00031196 |\n",
      "Epoch: 7 | Iter: 1000 | Step: 141000 | Train Loss: 0.00381027 |\n",
      "Epoch: 7 | Iter: 1100 | Step: 141100 | Train Loss: 0.05477101 |\n",
      "Epoch: 7 | Iter: 1200 | Step: 141200 | Train Loss: 0.05156887 |\n",
      "Epoch: 7 | Iter: 1300 | Step: 141300 | Train Loss: 0.03014197 |\n",
      "Epoch: 7 | Iter: 1400 | Step: 141400 | Train Loss: 0.04166702 |\n",
      "Epoch: 7 | Iter: 1500 | Step: 141500 | Train Loss: 0.00816113 |\n",
      "Epoch: 7 | Iter: 1600 | Step: 141600 | Train Loss: 0.01634674 |\n",
      "Epoch: 7 | Iter: 1700 | Step: 141700 | Train Loss: 0.00355928 |\n",
      "Epoch: 7 | Iter: 1800 | Step: 141800 | Train Loss: 0.00097753 |\n",
      "Epoch: 7 | Iter: 1900 | Step: 141900 | Train Loss: 0.04544383 |\n",
      "Epoch: 7 | Iter: 2000 | Step: 142000 | Train Loss: 0.06938155 |\n",
      "Epoch: 7 | Iter: 2100 | Step: 142100 | Train Loss: 0.01652838 |\n",
      "Epoch: 7 | Iter: 2200 | Step: 142200 | Train Loss: 0.11929618 |\n",
      "Epoch: 7 | Iter: 2300 | Step: 142300 | Train Loss: 0.00024808 |\n",
      "Epoch: 7 | Iter: 2400 | Step: 142400 | Train Loss: 0.00814943 |\n",
      "Epoch: 7 | Iter: 2500 | Step: 142500 | Train Loss: 0.05077414 |\n",
      "Epoch: 7 | Iter: 2600 | Step: 142600 | Train Loss: 0.01377392 |\n",
      "Epoch: 7 | Iter: 2700 | Step: 142700 | Train Loss: 0.00016577 |\n",
      "Epoch: 7 | Iter: 2800 | Step: 142800 | Train Loss: 0.00000041 |\n",
      "Epoch: 7 | Iter: 2900 | Step: 142900 | Train Loss: 0.00723427 |\n",
      "Epoch: 7 | Iter: 3000 | Step: 143000 | Train Loss: 0.01226719 |\n",
      "Epoch: 7 | Iter: 3100 | Step: 143100 | Train Loss: 0.00066726 |\n",
      "Epoch: 7 | Iter: 3200 | Step: 143200 | Train Loss: 0.00028407 |\n",
      "Epoch: 7 | Iter: 3300 | Step: 143300 | Train Loss: 0.00523278 |\n",
      "Epoch: 7 | Iter: 3400 | Step: 143400 | Train Loss: 0.02460078 |\n",
      "Epoch: 7 | Iter: 3500 | Step: 143500 | Train Loss: 0.00002332 |\n",
      "Epoch: 7 | Iter: 3600 | Step: 143600 | Train Loss: 0.00022743 |\n",
      "Epoch: 7 | Iter: 3700 | Step: 143700 | Train Loss: 0.01965174 |\n",
      "Epoch: 7 | Iter: 3800 | Step: 143800 | Train Loss: 0.07563984 |\n",
      "Epoch: 7 | Iter: 3900 | Step: 143900 | Train Loss: 0.00288090 |\n",
      "Epoch: 7 | Iter: 4000 | Step: 144000 | Train Loss: 0.55934602 |\n",
      "Epoch: 7 | Iter: 4100 | Step: 144100 | Train Loss: 0.00807294 |\n",
      "Epoch: 7 | Iter: 4200 | Step: 144200 | Train Loss: 0.02764550 |\n",
      "Epoch: 7 | Iter: 4300 | Step: 144300 | Train Loss: 0.00017175 |\n",
      "Epoch: 7 | Iter: 4400 | Step: 144400 | Train Loss: 0.00035060 |\n",
      "Epoch: 7 | Iter: 4500 | Step: 144500 | Train Loss: 0.04229182 |\n",
      "Epoch: 7 | Iter: 4600 | Step: 144600 | Train Loss: 0.01625856 |\n",
      "Epoch: 7 | Iter: 4700 | Step: 144700 | Train Loss: 0.01071858 |\n",
      "Epoch: 7 | Iter: 4800 | Step: 144800 | Train Loss: 0.00117382 |\n",
      "Epoch: 7 | Iter: 4900 | Step: 144900 | Train Loss: 0.00671506 |\n",
      "Epoch: 7 | Iter: 5000 | Step: 145000 | Train Loss: 0.00974066 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:10<03:56, 39.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Iter: 5000 | Step: 145000 | Val Loss: 0.02319438\n",
      "Epoch: 7 | Iter: 5100 | Step: 145100 | Train Loss: 0.00001034 |\n",
      "Epoch: 7 | Iter: 5200 | Step: 145200 | Train Loss: 0.00006283 |\n",
      "Epoch: 7 | Iter: 5300 | Step: 145300 | Train Loss: 0.02018405 |\n",
      "Epoch: 7 | Iter: 5400 | Step: 145400 | Train Loss: 0.00064201 |\n",
      "Epoch: 7 | Iter: 5500 | Step: 145500 | Train Loss: 0.04299816 |\n",
      "Epoch: 7 | Iter: 5600 | Step: 145600 | Train Loss: 1.10756433 |\n",
      "Epoch: 7 | Iter: 5700 | Step: 145700 | Train Loss: 0.00954752 |\n",
      "Epoch: 7 | Iter: 5800 | Step: 145800 | Train Loss: 0.02775675 |\n",
      "Epoch: 7 | Iter: 5900 | Step: 145900 | Train Loss: 0.01516390 |\n",
      "Epoch: 7 | Iter: 6000 | Step: 146000 | Train Loss: 0.00126080 |\n",
      "Epoch: 7 | Iter: 6100 | Step: 146100 | Train Loss: 0.00035686 |\n",
      "Epoch: 7 | Iter: 6200 | Step: 146200 | Train Loss: 0.04572282 |\n",
      "Epoch: 7 | Iter: 6300 | Step: 146300 | Train Loss: 0.00010214 |\n",
      "Epoch: 7 | Iter: 6400 | Step: 146400 | Train Loss: 0.07120208 |\n",
      "Epoch: 7 | Iter: 6500 | Step: 146500 | Train Loss: 0.00894189 |\n",
      "Epoch: 7 | Iter: 6600 | Step: 146600 | Train Loss: 0.02838387 |\n",
      "Epoch: 7 | Iter: 6700 | Step: 146700 | Train Loss: 0.04716239 |\n",
      "Epoch: 7 | Iter: 6800 | Step: 146800 | Train Loss: 0.06382204 |\n",
      "Epoch: 7 | Iter: 6900 | Step: 146900 | Train Loss: 0.80223769 |\n",
      "Epoch: 7 | Iter: 7000 | Step: 147000 | Train Loss: 0.11378472 |\n",
      "Epoch: 7 | Iter: 7100 | Step: 147100 | Train Loss: 0.00176684 |\n",
      "Epoch: 7 | Iter: 7200 | Step: 147200 | Train Loss: 0.00157638 |\n",
      "Epoch: 7 | Iter: 7300 | Step: 147300 | Train Loss: 0.02231601 |\n",
      "Epoch: 7 | Iter: 7400 | Step: 147400 | Train Loss: 0.04374433 |\n",
      "Epoch: 7 | Iter: 7500 | Step: 147500 | Train Loss: 0.04340857 |\n",
      "Epoch: 7 | Iter: 7600 | Step: 147600 | Train Loss: 0.00000751 |\n",
      "Epoch: 7 | Iter: 7700 | Step: 147700 | Train Loss: 0.00041244 |\n",
      "Epoch: 7 | Iter: 7800 | Step: 147800 | Train Loss: 0.12915029 |\n",
      "Epoch: 7 | Iter: 7900 | Step: 147900 | Train Loss: 0.00050763 |\n",
      "Epoch: 7 | Iter: 8000 | Step: 148000 | Train Loss: 0.03225245 |\n",
      "Epoch: 7 | Iter: 8100 | Step: 148100 | Train Loss: 0.05373302 |\n",
      "Epoch: 7 | Iter: 8200 | Step: 148200 | Train Loss: 0.06542869 |\n",
      "Epoch: 7 | Iter: 8300 | Step: 148300 | Train Loss: 0.06713540 |\n",
      "Epoch: 7 | Iter: 8400 | Step: 148400 | Train Loss: 0.00002037 |\n",
      "Epoch: 7 | Iter: 8500 | Step: 148500 | Train Loss: 1.28440392 |\n",
      "Epoch: 7 | Iter: 8600 | Step: 148600 | Train Loss: 0.08599482 |\n",
      "Epoch: 7 | Iter: 8700 | Step: 148700 | Train Loss: 0.00123292 |\n",
      "Epoch: 7 | Iter: 8800 | Step: 148800 | Train Loss: 0.05428036 |\n",
      "Epoch: 7 | Iter: 8900 | Step: 148900 | Train Loss: 0.00000277 |\n",
      "Epoch: 7 | Iter: 9000 | Step: 149000 | Train Loss: 0.01023331 |\n",
      "Epoch: 7 | Iter: 9100 | Step: 149100 | Train Loss: 0.02929150 |\n",
      "Epoch: 7 | Iter: 9200 | Step: 149200 | Train Loss: 0.01240417 |\n",
      "Epoch: 7 | Iter: 9300 | Step: 149300 | Train Loss: 0.06324650 |\n",
      "Epoch: 7 | Iter: 9400 | Step: 149400 | Train Loss: 0.01580115 |\n",
      "Epoch: 7 | Iter: 9500 | Step: 149500 | Train Loss: 0.00271939 |\n",
      "Epoch: 7 | Iter: 9600 | Step: 149600 | Train Loss: 0.17063500 |\n",
      "Epoch: 7 | Iter: 9700 | Step: 149700 | Train Loss: 0.00737557 |\n",
      "Epoch: 7 | Iter: 9800 | Step: 149800 | Train Loss: 0.11002732 |\n",
      "Epoch: 7 | Iter: 9900 | Step: 149900 | Train Loss: 0.00091408 |\n",
      "Epoch: 7 | Iter: 10000 | Step: 150000 | Train Loss: 0.02436540 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:25, 45.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Iter: 10000 | Step: 150000 | Val Loss: 0.02227199\n",
      "Epoch: 7 | Iter: 10100 | Step: 150100 | Train Loss: 0.01859988 |\n",
      "Epoch: 7 | Iter: 10200 | Step: 150200 | Train Loss: 0.07796779 |\n",
      "Epoch: 7 | Iter: 10300 | Step: 150300 | Train Loss: 0.02055372 |\n",
      "Epoch: 7 | Iter: 10400 | Step: 150400 | Train Loss: 0.18556778 |\n",
      "Epoch: 7 | Iter: 10500 | Step: 150500 | Train Loss: 0.05016128 |\n",
      "Epoch: 7 | Iter: 10600 | Step: 150600 | Train Loss: 0.00089396 |\n",
      "Epoch: 7 | Iter: 10700 | Step: 150700 | Train Loss: 0.00380427 |\n",
      "Epoch: 7 | Iter: 10800 | Step: 150800 | Train Loss: 0.11539017 |\n",
      "Epoch: 7 | Iter: 10900 | Step: 150900 | Train Loss: 0.06944906 |\n",
      "Epoch: 7 | Iter: 11000 | Step: 151000 | Train Loss: 0.00001131 |\n",
      "Epoch: 7 | Iter: 11100 | Step: 151100 | Train Loss: 0.39732400 |\n",
      "Epoch: 7 | Iter: 11200 | Step: 151200 | Train Loss: 0.52259034 |\n",
      "Epoch: 7 | Iter: 11300 | Step: 151300 | Train Loss: 0.02888986 |\n",
      "Epoch: 7 | Iter: 11400 | Step: 151400 | Train Loss: 0.03681109 |\n",
      "Epoch: 7 | Iter: 11500 | Step: 151500 | Train Loss: 0.01122104 |\n",
      "Epoch: 7 | Iter: 11600 | Step: 151600 | Train Loss: 0.04714245 |\n",
      "Epoch: 7 | Iter: 11700 | Step: 151700 | Train Loss: 0.03417144 |\n",
      "Epoch: 7 | Iter: 11800 | Step: 151800 | Train Loss: 0.00006273 |\n",
      "Epoch: 7 | Iter: 11900 | Step: 151900 | Train Loss: 0.03484605 |\n",
      "Epoch: 7 | Iter: 12000 | Step: 152000 | Train Loss: 0.04953388 |\n",
      "Epoch: 7 | Iter: 12100 | Step: 152100 | Train Loss: 0.03986928 |\n",
      "Epoch: 7 | Iter: 12200 | Step: 152200 | Train Loss: 0.04116998 |\n",
      "Epoch: 7 | Iter: 12300 | Step: 152300 | Train Loss: 0.10538369 |\n",
      "Epoch: 7 | Iter: 12400 | Step: 152400 | Train Loss: 0.01051703 |\n",
      "Epoch: 7 | Iter: 12500 | Step: 152500 | Train Loss: 0.02205672 |\n",
      "Epoch: 7 | Iter: 12600 | Step: 152600 | Train Loss: 0.00291014 |\n",
      "Epoch: 7 | Iter: 12700 | Step: 152700 | Train Loss: 0.06434356 |\n",
      "Epoch: 7 | Iter: 12800 | Step: 152800 | Train Loss: 0.02401797 |\n",
      "Epoch: 7 | Iter: 12900 | Step: 152900 | Train Loss: 0.05131590 |\n",
      "Epoch: 7 | Iter: 13000 | Step: 153000 | Train Loss: 0.00241488 |\n",
      "Epoch: 7 | Iter: 13100 | Step: 153100 | Train Loss: 0.02298128 |\n",
      "Epoch: 7 | Iter: 13200 | Step: 153200 | Train Loss: 0.00040966 |\n",
      "Epoch: 7 | Iter: 13300 | Step: 153300 | Train Loss: 0.00401227 |\n",
      "Epoch: 7 | Iter: 13400 | Step: 153400 | Train Loss: 0.10807430 |\n",
      "Epoch: 7 | Iter: 13500 | Step: 153500 | Train Loss: 0.03367854 |\n",
      "Epoch: 7 | Iter: 13600 | Step: 153600 | Train Loss: 0.47857982 |\n",
      "Epoch: 7 | Iter: 13700 | Step: 153700 | Train Loss: 0.02933120 |\n",
      "Epoch: 7 | Iter: 13800 | Step: 153800 | Train Loss: 0.01495136 |\n",
      "Epoch: 7 | Iter: 13900 | Step: 153900 | Train Loss: 0.00070232 |\n",
      "Epoch: 7 | Iter: 14000 | Step: 154000 | Train Loss: 1.09744930 |\n",
      "Epoch: 7 | Iter: 14100 | Step: 154100 | Train Loss: 0.26741526 |\n",
      "Epoch: 7 | Iter: 14200 | Step: 154200 | Train Loss: 0.00330279 |\n",
      "Epoch: 7 | Iter: 14300 | Step: 154300 | Train Loss: 0.18008006 |\n",
      "Epoch: 7 | Iter: 14400 | Step: 154400 | Train Loss: 0.11170269 |\n",
      "Epoch: 7 | Iter: 14500 | Step: 154500 | Train Loss: 0.00098563 |\n",
      "Epoch: 7 | Iter: 14600 | Step: 154600 | Train Loss: 0.00000040 |\n",
      "Epoch: 7 | Iter: 14700 | Step: 154700 | Train Loss: 0.56701350 |\n",
      "Epoch: 7 | Iter: 14800 | Step: 154800 | Train Loss: 0.01544307 |\n",
      "Epoch: 7 | Iter: 14900 | Step: 154900 | Train Loss: 0.00022119 |\n",
      "Epoch: 7 | Iter: 15000 | Step: 155000 | Train Loss: 0.03087964 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:22, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Iter: 15000 | Step: 155000 | Val Loss: 0.02415889\n",
      "Epoch: 7 | Iter: 15100 | Step: 155100 | Train Loss: 0.00103376 |\n",
      "Epoch: 7 | Iter: 15200 | Step: 155200 | Train Loss: 0.02960454 |\n",
      "Epoch: 7 | Iter: 15300 | Step: 155300 | Train Loss: 0.01408334 |\n",
      "Epoch: 7 | Iter: 15400 | Step: 155400 | Train Loss: 0.01235276 |\n",
      "Epoch: 7 | Iter: 15500 | Step: 155500 | Train Loss: 0.06459157 |\n",
      "Epoch: 7 | Iter: 15600 | Step: 155600 | Train Loss: 0.00830478 |\n",
      "Epoch: 7 | Iter: 15700 | Step: 155700 | Train Loss: 0.02665764 |\n",
      "Epoch: 7 | Iter: 15800 | Step: 155800 | Train Loss: 0.01728288 |\n",
      "Epoch: 7 | Iter: 15900 | Step: 155900 | Train Loss: 0.03000791 |\n",
      "Epoch: 7 | Iter: 16000 | Step: 156000 | Train Loss: 0.01603192 |\n",
      "Epoch: 7 | Iter: 16100 | Step: 156100 | Train Loss: 0.18419513 |\n",
      "Epoch: 7 | Iter: 16200 | Step: 156200 | Train Loss: 0.00000001 |\n",
      "Epoch: 7 | Iter: 16300 | Step: 156300 | Train Loss: 0.00037977 |\n",
      "Epoch: 7 | Iter: 16400 | Step: 156400 | Train Loss: 0.04453331 |\n",
      "Epoch: 7 | Iter: 16500 | Step: 156500 | Train Loss: 0.01987272 |\n",
      "Epoch: 7 | Iter: 16600 | Step: 156600 | Train Loss: 0.02242146 |\n",
      "Epoch: 7 | Iter: 16700 | Step: 156700 | Train Loss: 0.00162214 |\n",
      "Epoch: 7 | Iter: 16800 | Step: 156800 | Train Loss: 0.03971605 |\n",
      "Epoch: 7 | Iter: 16900 | Step: 156900 | Train Loss: 0.04788934 |\n",
      "Epoch: 7 | Iter: 17000 | Step: 157000 | Train Loss: 0.00125394 |\n",
      "Epoch: 7 | Iter: 17100 | Step: 157100 | Train Loss: 0.02595878 |\n",
      "Epoch: 7 | Iter: 17200 | Step: 157200 | Train Loss: 0.01847704 |\n",
      "Epoch: 7 | Iter: 17300 | Step: 157300 | Train Loss: 1.15806687 |\n",
      "Epoch: 7 | Iter: 17400 | Step: 157400 | Train Loss: 0.27212831 |\n",
      "Epoch: 7 | Iter: 17500 | Step: 157500 | Train Loss: 0.00639891 |\n",
      "Epoch: 7 | Iter: 17600 | Step: 157600 | Train Loss: 0.13173707 |\n",
      "Epoch: 7 | Iter: 17700 | Step: 157700 | Train Loss: 0.04942143 |\n",
      "Epoch: 7 | Iter: 17800 | Step: 157800 | Train Loss: 0.00751214 |\n",
      "Epoch: 7 | Iter: 17900 | Step: 157900 | Train Loss: 0.00559992 |\n",
      "Epoch: 7 | Iter: 18000 | Step: 158000 | Train Loss: 0.02659291 |\n",
      "Epoch: 7 | Iter: 18100 | Step: 158100 | Train Loss: 0.03932542 |\n",
      "Epoch: 7 | Iter: 18200 | Step: 158200 | Train Loss: 0.02354935 |\n",
      "Epoch: 7 | Iter: 18300 | Step: 158300 | Train Loss: 0.02651692 |\n",
      "Epoch: 7 | Iter: 18400 | Step: 158400 | Train Loss: 0.04509512 |\n",
      "Epoch: 7 | Iter: 18500 | Step: 158500 | Train Loss: 0.02166063 |\n",
      "Epoch: 7 | Iter: 18600 | Step: 158600 | Train Loss: 0.00663779 |\n",
      "Epoch: 7 | Iter: 18700 | Step: 158700 | Train Loss: 0.00600201 |\n",
      "Epoch: 7 | Iter: 18800 | Step: 158800 | Train Loss: 1.56877100 |\n",
      "Epoch: 7 | Iter: 18900 | Step: 158900 | Train Loss: 0.00197539 |\n",
      "Epoch: 7 | Iter: 19000 | Step: 159000 | Train Loss: 0.00361750 |\n",
      "Epoch: 7 | Iter: 19100 | Step: 159100 | Train Loss: 0.02082689 |\n",
      "Epoch: 7 | Iter: 19200 | Step: 159200 | Train Loss: 0.46973944 |\n",
      "Epoch: 7 | Iter: 19300 | Step: 159300 | Train Loss: 0.00113660 |\n",
      "Epoch: 7 | Iter: 19400 | Step: 159400 | Train Loss: 0.04279733 |\n",
      "Epoch: 7 | Iter: 19500 | Step: 159500 | Train Loss: 0.00445411 |\n",
      "Epoch: 7 | Iter: 19600 | Step: 159600 | Train Loss: 0.01655730 |\n",
      "Epoch: 7 | Iter: 19700 | Step: 159700 | Train Loss: 0.00915114 |\n",
      "Epoch: 7 | Iter: 19800 | Step: 159800 | Train Loss: 0.00000230 |\n",
      "Epoch: 7 | Iter: 19900 | Step: 159900 | Train Loss: 0.07311878 |\n",
      "Epoch: 8 | Iter: 0 | Step: 160000 | Train Loss: 0.00064190 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:26, 45.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Iter: 0 | Step: 160000 | Val Loss: 0.02347665\n",
      "Epoch: 8 | Iter: 100 | Step: 160100 | Train Loss: 0.02195477 |\n",
      "Epoch: 8 | Iter: 200 | Step: 160200 | Train Loss: 0.02422625 |\n",
      "Epoch: 8 | Iter: 300 | Step: 160300 | Train Loss: 0.03460157 |\n",
      "Epoch: 8 | Iter: 400 | Step: 160400 | Train Loss: 0.03126102 |\n",
      "Epoch: 8 | Iter: 500 | Step: 160500 | Train Loss: 0.00866375 |\n",
      "Epoch: 8 | Iter: 600 | Step: 160600 | Train Loss: 0.00303990 |\n",
      "Epoch: 8 | Iter: 700 | Step: 160700 | Train Loss: 0.02274145 |\n",
      "Epoch: 8 | Iter: 800 | Step: 160800 | Train Loss: 0.02363230 |\n",
      "Epoch: 8 | Iter: 900 | Step: 160900 | Train Loss: 0.00000236 |\n",
      "Epoch: 8 | Iter: 1000 | Step: 161000 | Train Loss: 0.00733098 |\n",
      "Epoch: 8 | Iter: 1100 | Step: 161100 | Train Loss: 0.06956875 |\n",
      "Epoch: 8 | Iter: 1200 | Step: 161200 | Train Loss: 0.03208273 |\n",
      "Epoch: 8 | Iter: 1300 | Step: 161300 | Train Loss: 0.00107068 |\n",
      "Epoch: 8 | Iter: 1400 | Step: 161400 | Train Loss: 0.01092969 |\n",
      "Epoch: 8 | Iter: 1500 | Step: 161500 | Train Loss: 0.07015472 |\n",
      "Epoch: 8 | Iter: 1600 | Step: 161600 | Train Loss: 0.00016175 |\n",
      "Epoch: 8 | Iter: 1700 | Step: 161700 | Train Loss: 0.16641103 |\n",
      "Epoch: 8 | Iter: 1800 | Step: 161800 | Train Loss: 0.02271546 |\n",
      "Epoch: 8 | Iter: 1900 | Step: 161900 | Train Loss: 0.02262887 |\n",
      "Epoch: 8 | Iter: 2000 | Step: 162000 | Train Loss: 0.00040334 |\n",
      "Epoch: 8 | Iter: 2100 | Step: 162100 | Train Loss: 0.03802703 |\n",
      "Epoch: 8 | Iter: 2200 | Step: 162200 | Train Loss: 0.06151877 |\n",
      "Epoch: 8 | Iter: 2300 | Step: 162300 | Train Loss: 0.00028375 |\n",
      "Epoch: 8 | Iter: 2400 | Step: 162400 | Train Loss: 0.03186624 |\n",
      "Epoch: 8 | Iter: 2500 | Step: 162500 | Train Loss: 0.00209036 |\n",
      "Epoch: 8 | Iter: 2600 | Step: 162600 | Train Loss: 0.00455158 |\n",
      "Epoch: 8 | Iter: 2700 | Step: 162700 | Train Loss: 0.04853677 |\n",
      "Epoch: 8 | Iter: 2800 | Step: 162800 | Train Loss: 0.00735167 |\n",
      "Epoch: 8 | Iter: 2900 | Step: 162900 | Train Loss: 0.02284347 |\n",
      "Epoch: 8 | Iter: 3000 | Step: 163000 | Train Loss: 0.00965716 |\n",
      "Epoch: 8 | Iter: 3100 | Step: 163100 | Train Loss: 0.00020935 |\n",
      "Epoch: 8 | Iter: 3200 | Step: 163200 | Train Loss: 0.00261978 |\n",
      "Epoch: 8 | Iter: 3300 | Step: 163300 | Train Loss: 0.09446293 |\n",
      "Epoch: 8 | Iter: 3400 | Step: 163400 | Train Loss: 0.04458398 |\n",
      "Epoch: 8 | Iter: 3500 | Step: 163500 | Train Loss: 0.00107391 |\n",
      "Epoch: 8 | Iter: 3600 | Step: 163600 | Train Loss: 0.01620525 |\n",
      "Epoch: 8 | Iter: 3700 | Step: 163700 | Train Loss: 0.05333611 |\n",
      "Epoch: 8 | Iter: 3800 | Step: 163800 | Train Loss: 0.00385376 |\n",
      "Epoch: 8 | Iter: 3900 | Step: 163900 | Train Loss: 0.04067449 |\n",
      "Epoch: 8 | Iter: 4000 | Step: 164000 | Train Loss: 0.32954183 |\n",
      "Epoch: 8 | Iter: 4100 | Step: 164100 | Train Loss: 0.01178533 |\n",
      "Epoch: 8 | Iter: 4200 | Step: 164200 | Train Loss: 0.05095461 |\n",
      "Epoch: 8 | Iter: 4300 | Step: 164300 | Train Loss: 0.04655519 |\n",
      "Epoch: 8 | Iter: 4400 | Step: 164400 | Train Loss: 0.02016496 |\n",
      "Epoch: 8 | Iter: 4500 | Step: 164500 | Train Loss: 0.00334366 |\n",
      "Epoch: 8 | Iter: 4600 | Step: 164600 | Train Loss: 0.03072207 |\n",
      "Epoch: 8 | Iter: 4700 | Step: 164700 | Train Loss: 0.43921986 |\n",
      "Epoch: 8 | Iter: 4800 | Step: 164800 | Train Loss: 0.02451775 |\n",
      "Epoch: 8 | Iter: 4900 | Step: 164900 | Train Loss: 0.00001703 |\n",
      "Epoch: 8 | Iter: 5000 | Step: 165000 | Train Loss: 0.00302667 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:29, 44.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Iter: 5000 | Step: 165000 | Val Loss: 0.02321683\n",
      "Epoch: 8 | Iter: 5100 | Step: 165100 | Train Loss: 0.04337275 |\n",
      "Epoch: 8 | Iter: 5200 | Step: 165200 | Train Loss: 0.07286787 |\n",
      "Epoch: 8 | Iter: 5300 | Step: 165300 | Train Loss: 0.00287244 |\n",
      "Epoch: 8 | Iter: 5400 | Step: 165400 | Train Loss: 0.01908953 |\n",
      "Epoch: 8 | Iter: 5500 | Step: 165500 | Train Loss: 0.00880911 |\n",
      "Epoch: 8 | Iter: 5600 | Step: 165600 | Train Loss: 1.15098917 |\n",
      "Epoch: 8 | Iter: 5700 | Step: 165700 | Train Loss: 0.00160741 |\n",
      "Epoch: 8 | Iter: 5800 | Step: 165800 | Train Loss: 0.03440943 |\n",
      "Epoch: 8 | Iter: 5900 | Step: 165900 | Train Loss: 0.01991634 |\n",
      "Epoch: 8 | Iter: 6000 | Step: 166000 | Train Loss: 0.00028308 |\n",
      "Epoch: 8 | Iter: 6100 | Step: 166100 | Train Loss: 0.04140894 |\n",
      "Epoch: 8 | Iter: 6200 | Step: 166200 | Train Loss: 0.04277595 |\n",
      "Epoch: 8 | Iter: 6300 | Step: 166300 | Train Loss: 0.00025839 |\n",
      "Epoch: 8 | Iter: 6400 | Step: 166400 | Train Loss: 0.00172370 |\n",
      "Epoch: 8 | Iter: 6500 | Step: 166500 | Train Loss: 0.01627922 |\n",
      "Epoch: 8 | Iter: 6600 | Step: 166600 | Train Loss: 0.03571690 |\n",
      "Epoch: 8 | Iter: 6700 | Step: 166700 | Train Loss: 0.00015166 |\n",
      "Epoch: 8 | Iter: 6800 | Step: 166800 | Train Loss: 0.21724169 |\n",
      "Epoch: 8 | Iter: 6900 | Step: 166900 | Train Loss: 0.29763761 |\n",
      "Epoch: 8 | Iter: 7000 | Step: 167000 | Train Loss: 0.00360721 |\n",
      "Epoch: 8 | Iter: 7100 | Step: 167100 | Train Loss: 0.01298483 |\n",
      "Epoch: 8 | Iter: 7200 | Step: 167200 | Train Loss: 0.07225909 |\n",
      "Epoch: 8 | Iter: 7300 | Step: 167300 | Train Loss: 0.01938724 |\n",
      "Epoch: 8 | Iter: 7400 | Step: 167400 | Train Loss: 0.00742894 |\n",
      "Epoch: 8 | Iter: 7500 | Step: 167500 | Train Loss: 0.01258171 |\n",
      "Epoch: 8 | Iter: 7600 | Step: 167600 | Train Loss: 0.05829265 |\n",
      "Epoch: 8 | Iter: 7700 | Step: 167700 | Train Loss: 0.00053566 |\n",
      "Epoch: 8 | Iter: 7800 | Step: 167800 | Train Loss: 0.01724101 |\n",
      "Epoch: 8 | Iter: 7900 | Step: 167900 | Train Loss: 0.01597853 |\n",
      "Epoch: 8 | Iter: 8000 | Step: 168000 | Train Loss: 0.00613117 |\n",
      "Epoch: 8 | Iter: 8100 | Step: 168100 | Train Loss: 0.00301034 |\n",
      "Epoch: 8 | Iter: 8200 | Step: 168200 | Train Loss: 0.04720335 |\n",
      "Epoch: 8 | Iter: 8300 | Step: 168300 | Train Loss: 0.06934077 |\n",
      "Epoch: 8 | Iter: 8400 | Step: 168400 | Train Loss: 0.01700133 |\n",
      "Epoch: 8 | Iter: 8500 | Step: 168500 | Train Loss: 0.58604181 |\n",
      "Epoch: 8 | Iter: 8600 | Step: 168600 | Train Loss: 0.36501127 |\n",
      "Epoch: 8 | Iter: 8700 | Step: 168700 | Train Loss: 0.06344839 |\n",
      "Epoch: 8 | Iter: 8800 | Step: 168800 | Train Loss: 0.04659728 |\n",
      "Epoch: 8 | Iter: 8900 | Step: 168900 | Train Loss: 0.00028694 |\n",
      "Epoch: 8 | Iter: 9000 | Step: 169000 | Train Loss: 0.03481399 |\n",
      "Epoch: 8 | Iter: 9100 | Step: 169100 | Train Loss: 0.00002515 |\n",
      "Epoch: 8 | Iter: 9200 | Step: 169200 | Train Loss: 0.00244816 |\n",
      "Epoch: 8 | Iter: 9300 | Step: 169300 | Train Loss: 0.03730820 |\n",
      "Epoch: 8 | Iter: 9400 | Step: 169400 | Train Loss: 0.08857749 |\n",
      "Epoch: 8 | Iter: 9500 | Step: 169500 | Train Loss: 0.03031391 |\n",
      "Epoch: 8 | Iter: 9600 | Step: 169600 | Train Loss: 0.68755054 |\n",
      "Epoch: 8 | Iter: 9700 | Step: 169700 | Train Loss: 0.00652877 |\n",
      "Epoch: 8 | Iter: 9800 | Step: 169800 | Train Loss: 0.00888355 |\n",
      "Epoch: 8 | Iter: 9900 | Step: 169900 | Train Loss: 0.00201256 |\n",
      "Epoch: 8 | Iter: 10000 | Step: 170000 | Train Loss: 0.04606413 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:48, 40.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Iter: 10000 | Step: 170000 | Val Loss: 0.02314995\n",
      "Epoch: 8 | Iter: 10100 | Step: 170100 | Train Loss: 0.04502143 |\n",
      "Epoch: 8 | Iter: 10200 | Step: 170200 | Train Loss: 0.00077889 |\n",
      "Epoch: 8 | Iter: 10300 | Step: 170300 | Train Loss: 0.00589135 |\n",
      "Epoch: 8 | Iter: 10400 | Step: 170400 | Train Loss: 0.18686634 |\n",
      "Epoch: 8 | Iter: 10500 | Step: 170500 | Train Loss: 0.00214084 |\n",
      "Epoch: 8 | Iter: 10600 | Step: 170600 | Train Loss: 0.06119755 |\n",
      "Epoch: 8 | Iter: 10700 | Step: 170700 | Train Loss: 0.00777943 |\n",
      "Epoch: 8 | Iter: 10800 | Step: 170800 | Train Loss: 0.01738340 |\n",
      "Epoch: 8 | Iter: 10900 | Step: 170900 | Train Loss: 0.43632543 |\n",
      "Epoch: 8 | Iter: 11000 | Step: 171000 | Train Loss: 0.15239622 |\n",
      "Epoch: 8 | Iter: 11100 | Step: 171100 | Train Loss: 0.42854723 |\n",
      "Epoch: 8 | Iter: 11200 | Step: 171200 | Train Loss: 0.73090833 |\n",
      "Epoch: 8 | Iter: 11300 | Step: 171300 | Train Loss: 0.00302626 |\n",
      "Epoch: 8 | Iter: 11400 | Step: 171400 | Train Loss: 0.00449312 |\n",
      "Epoch: 8 | Iter: 11500 | Step: 171500 | Train Loss: 0.02679859 |\n",
      "Epoch: 8 | Iter: 11600 | Step: 171600 | Train Loss: 0.07327793 |\n",
      "Epoch: 8 | Iter: 11700 | Step: 171700 | Train Loss: 0.00000000 |\n",
      "Epoch: 8 | Iter: 11800 | Step: 171800 | Train Loss: 0.03606888 |\n",
      "Epoch: 8 | Iter: 11900 | Step: 171900 | Train Loss: 0.00077530 |\n",
      "Epoch: 8 | Iter: 12000 | Step: 172000 | Train Loss: 0.00078879 |\n",
      "Epoch: 8 | Iter: 12100 | Step: 172100 | Train Loss: 0.04226641 |\n",
      "Epoch: 8 | Iter: 12200 | Step: 172200 | Train Loss: 0.00602612 |\n",
      "Epoch: 8 | Iter: 12300 | Step: 172300 | Train Loss: 0.01381237 |\n",
      "Epoch: 8 | Iter: 12400 | Step: 172400 | Train Loss: 0.00227225 |\n",
      "Epoch: 8 | Iter: 12500 | Step: 172500 | Train Loss: 0.00101090 |\n",
      "Epoch: 8 | Iter: 12600 | Step: 172600 | Train Loss: 0.00001375 |\n",
      "Epoch: 8 | Iter: 12700 | Step: 172700 | Train Loss: 0.03970304 |\n",
      "Epoch: 8 | Iter: 12800 | Step: 172800 | Train Loss: 0.00180310 |\n",
      "Epoch: 8 | Iter: 12900 | Step: 172900 | Train Loss: 0.00028498 |\n",
      "Epoch: 8 | Iter: 13000 | Step: 173000 | Train Loss: 0.00528165 |\n",
      "Epoch: 8 | Iter: 13100 | Step: 173100 | Train Loss: 0.02641032 |\n",
      "Epoch: 8 | Iter: 13200 | Step: 173200 | Train Loss: 0.04410820 |\n",
      "Epoch: 8 | Iter: 13300 | Step: 173300 | Train Loss: 0.03860182 |\n",
      "Epoch: 8 | Iter: 13400 | Step: 173400 | Train Loss: 0.02639630 |\n",
      "Epoch: 8 | Iter: 13500 | Step: 173500 | Train Loss: 0.05656412 |\n",
      "Epoch: 8 | Iter: 13600 | Step: 173600 | Train Loss: 0.80638611 |\n",
      "Epoch: 8 | Iter: 13700 | Step: 173700 | Train Loss: 0.02847064 |\n",
      "Epoch: 8 | Iter: 13800 | Step: 173800 | Train Loss: 0.15262470 |\n",
      "Epoch: 8 | Iter: 13900 | Step: 173900 | Train Loss: 0.00016227 |\n",
      "Epoch: 8 | Iter: 14000 | Step: 174000 | Train Loss: 0.89161301 |\n",
      "Epoch: 8 | Iter: 14100 | Step: 174100 | Train Loss: 0.24657808 |\n",
      "Epoch: 8 | Iter: 14200 | Step: 174200 | Train Loss: 0.02253397 |\n",
      "Epoch: 8 | Iter: 14300 | Step: 174300 | Train Loss: 0.14359845 |\n",
      "Epoch: 8 | Iter: 14400 | Step: 174400 | Train Loss: 0.00007455 |\n",
      "Epoch: 8 | Iter: 14500 | Step: 174500 | Train Loss: 0.00000004 |\n",
      "Epoch: 8 | Iter: 14600 | Step: 174600 | Train Loss: 0.00900705 |\n",
      "Epoch: 8 | Iter: 14700 | Step: 174700 | Train Loss: 0.21465251 |\n",
      "Epoch: 8 | Iter: 14800 | Step: 174800 | Train Loss: 0.03850871 |\n",
      "Epoch: 8 | Iter: 14900 | Step: 174900 | Train Loss: 0.02626017 |\n",
      "Epoch: 8 | Iter: 15000 | Step: 175000 | Train Loss: 0.05128642 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:46, 41.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Iter: 15000 | Step: 175000 | Val Loss: 0.02544511\n",
      "Epoch: 8 | Iter: 15100 | Step: 175100 | Train Loss: 0.03084098 |\n",
      "Epoch: 8 | Iter: 15200 | Step: 175200 | Train Loss: 0.00763642 |\n",
      "Epoch: 8 | Iter: 15300 | Step: 175300 | Train Loss: 0.05499656 |\n",
      "Epoch: 8 | Iter: 15400 | Step: 175400 | Train Loss: 0.01816362 |\n",
      "Epoch: 8 | Iter: 15500 | Step: 175500 | Train Loss: 0.02589669 |\n",
      "Epoch: 8 | Iter: 15600 | Step: 175600 | Train Loss: 0.03182609 |\n",
      "Epoch: 8 | Iter: 15700 | Step: 175700 | Train Loss: 0.02431643 |\n",
      "Epoch: 8 | Iter: 15800 | Step: 175800 | Train Loss: 0.06272840 |\n",
      "Epoch: 8 | Iter: 15900 | Step: 175900 | Train Loss: 0.00848948 |\n",
      "Epoch: 8 | Iter: 16000 | Step: 176000 | Train Loss: 0.00257646 |\n",
      "Epoch: 8 | Iter: 16100 | Step: 176100 | Train Loss: 0.19969152 |\n",
      "Epoch: 8 | Iter: 16200 | Step: 176200 | Train Loss: 0.03096134 |\n",
      "Epoch: 8 | Iter: 16300 | Step: 176300 | Train Loss: 0.04694382 |\n",
      "Epoch: 8 | Iter: 16400 | Step: 176400 | Train Loss: 0.02535289 |\n",
      "Epoch: 8 | Iter: 16500 | Step: 176500 | Train Loss: 0.02668165 |\n",
      "Epoch: 8 | Iter: 16600 | Step: 176600 | Train Loss: 0.01692203 |\n",
      "Epoch: 8 | Iter: 16700 | Step: 176700 | Train Loss: 0.02898401 |\n",
      "Epoch: 8 | Iter: 16800 | Step: 176800 | Train Loss: 0.00970895 |\n",
      "Epoch: 8 | Iter: 16900 | Step: 176900 | Train Loss: 0.03580843 |\n",
      "Epoch: 8 | Iter: 17000 | Step: 177000 | Train Loss: 0.00391524 |\n",
      "Epoch: 8 | Iter: 17100 | Step: 177100 | Train Loss: 0.00646247 |\n",
      "Epoch: 8 | Iter: 17200 | Step: 177200 | Train Loss: 0.03516294 |\n",
      "Epoch: 8 | Iter: 17300 | Step: 177300 | Train Loss: 1.13164222 |\n",
      "Epoch: 8 | Iter: 17400 | Step: 177400 | Train Loss: 0.00377202 |\n",
      "Epoch: 8 | Iter: 17500 | Step: 177500 | Train Loss: 0.02118015 |\n",
      "Epoch: 8 | Iter: 17600 | Step: 177600 | Train Loss: 0.02161569 |\n",
      "Epoch: 8 | Iter: 17700 | Step: 177700 | Train Loss: 0.00048802 |\n",
      "Epoch: 8 | Iter: 17800 | Step: 177800 | Train Loss: 0.15520866 |\n",
      "Epoch: 8 | Iter: 17900 | Step: 177900 | Train Loss: 0.02157053 |\n",
      "Epoch: 8 | Iter: 18000 | Step: 178000 | Train Loss: 0.02611814 |\n",
      "Epoch: 8 | Iter: 18100 | Step: 178100 | Train Loss: 0.04524119 |\n",
      "Epoch: 8 | Iter: 18200 | Step: 178200 | Train Loss: 0.06761593 |\n",
      "Epoch: 8 | Iter: 18300 | Step: 178300 | Train Loss: 0.00648145 |\n",
      "Epoch: 8 | Iter: 18400 | Step: 178400 | Train Loss: 0.01948216 |\n",
      "Epoch: 8 | Iter: 18500 | Step: 178500 | Train Loss: 0.00431275 |\n",
      "Epoch: 8 | Iter: 18600 | Step: 178600 | Train Loss: 0.00108722 |\n",
      "Epoch: 8 | Iter: 18700 | Step: 178700 | Train Loss: 0.04081774 |\n",
      "Epoch: 8 | Iter: 18800 | Step: 178800 | Train Loss: 1.05097568 |\n",
      "Epoch: 8 | Iter: 18900 | Step: 178900 | Train Loss: 0.02076085 |\n",
      "Epoch: 8 | Iter: 19000 | Step: 179000 | Train Loss: 0.01053994 |\n",
      "Epoch: 8 | Iter: 19100 | Step: 179100 | Train Loss: 0.14820150 |\n",
      "Epoch: 8 | Iter: 19200 | Step: 179200 | Train Loss: 0.72942609 |\n",
      "Epoch: 8 | Iter: 19300 | Step: 179300 | Train Loss: 0.02026050 |\n",
      "Epoch: 8 | Iter: 19400 | Step: 179400 | Train Loss: 0.06397551 |\n",
      "Epoch: 8 | Iter: 19500 | Step: 179500 | Train Loss: 0.01386357 |\n",
      "Epoch: 8 | Iter: 19600 | Step: 179600 | Train Loss: 0.05226889 |\n",
      "Epoch: 8 | Iter: 19700 | Step: 179700 | Train Loss: 0.08467719 |\n",
      "Epoch: 8 | Iter: 19800 | Step: 179800 | Train Loss: 0.07734644 |\n",
      "Epoch: 8 | Iter: 19900 | Step: 179900 | Train Loss: 0.11457052 |\n",
      "Epoch: 9 | Iter: 0 | Step: 180000 | Train Loss: 0.01720235 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:17, 47.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Iter: 0 | Step: 180000 | Val Loss: 0.02389984\n",
      "Epoch: 9 | Iter: 100 | Step: 180100 | Train Loss: 0.02754398 |\n",
      "Epoch: 9 | Iter: 200 | Step: 180200 | Train Loss: 0.06530076 |\n",
      "Epoch: 9 | Iter: 300 | Step: 180300 | Train Loss: 0.02657189 |\n",
      "Epoch: 9 | Iter: 400 | Step: 180400 | Train Loss: 0.02845303 |\n",
      "Epoch: 9 | Iter: 500 | Step: 180500 | Train Loss: 0.02667350 |\n",
      "Epoch: 9 | Iter: 600 | Step: 180600 | Train Loss: 0.00324958 |\n",
      "Epoch: 9 | Iter: 700 | Step: 180700 | Train Loss: 0.04835387 |\n",
      "Epoch: 9 | Iter: 800 | Step: 180800 | Train Loss: 0.00113049 |\n",
      "Epoch: 9 | Iter: 900 | Step: 180900 | Train Loss: 0.03777624 |\n",
      "Epoch: 9 | Iter: 1000 | Step: 181000 | Train Loss: 0.00005203 |\n",
      "Epoch: 9 | Iter: 1100 | Step: 181100 | Train Loss: 0.00251037 |\n",
      "Epoch: 9 | Iter: 1200 | Step: 181200 | Train Loss: 0.01229463 |\n",
      "Epoch: 9 | Iter: 1300 | Step: 181300 | Train Loss: 0.03384933 |\n",
      "Epoch: 9 | Iter: 1400 | Step: 181400 | Train Loss: 0.02052749 |\n",
      "Epoch: 9 | Iter: 1500 | Step: 181500 | Train Loss: 0.00151999 |\n",
      "Epoch: 9 | Iter: 1600 | Step: 181600 | Train Loss: 0.02000487 |\n",
      "Epoch: 9 | Iter: 1700 | Step: 181700 | Train Loss: 0.22963795 |\n",
      "Epoch: 9 | Iter: 1800 | Step: 181800 | Train Loss: 0.00003932 |\n",
      "Epoch: 9 | Iter: 1900 | Step: 181900 | Train Loss: 0.00001939 |\n",
      "Epoch: 9 | Iter: 2000 | Step: 182000 | Train Loss: 0.08298619 |\n",
      "Epoch: 9 | Iter: 2100 | Step: 182100 | Train Loss: 0.02690656 |\n",
      "Epoch: 9 | Iter: 2200 | Step: 182200 | Train Loss: 0.00000035 |\n",
      "Epoch: 9 | Iter: 2300 | Step: 182300 | Train Loss: 0.03767424 |\n",
      "Epoch: 9 | Iter: 2400 | Step: 182400 | Train Loss: 0.00050342 |\n",
      "Epoch: 9 | Iter: 2500 | Step: 182500 | Train Loss: 0.00449462 |\n",
      "Epoch: 9 | Iter: 2600 | Step: 182600 | Train Loss: 0.00040804 |\n",
      "Epoch: 9 | Iter: 2700 | Step: 182700 | Train Loss: 0.03892426 |\n",
      "Epoch: 9 | Iter: 2800 | Step: 182800 | Train Loss: 0.00204180 |\n",
      "Epoch: 9 | Iter: 2900 | Step: 182900 | Train Loss: 0.00962923 |\n",
      "Epoch: 9 | Iter: 3000 | Step: 183000 | Train Loss: 0.00461046 |\n",
      "Epoch: 9 | Iter: 3100 | Step: 183100 | Train Loss: 0.00321812 |\n",
      "Epoch: 9 | Iter: 3200 | Step: 183200 | Train Loss: 0.02866303 |\n",
      "Epoch: 9 | Iter: 3300 | Step: 183300 | Train Loss: 0.00763408 |\n",
      "Epoch: 9 | Iter: 3400 | Step: 183400 | Train Loss: 0.01568932 |\n",
      "Epoch: 9 | Iter: 3500 | Step: 183500 | Train Loss: 0.02181941 |\n",
      "Epoch: 9 | Iter: 3600 | Step: 183600 | Train Loss: 0.00124414 |\n",
      "Epoch: 9 | Iter: 3700 | Step: 183700 | Train Loss: 0.05280329 |\n",
      "Epoch: 9 | Iter: 3800 | Step: 183800 | Train Loss: 0.00953005 |\n",
      "Epoch: 9 | Iter: 3900 | Step: 183900 | Train Loss: 0.01975883 |\n",
      "Epoch: 9 | Iter: 4000 | Step: 184000 | Train Loss: 0.53053546 |\n",
      "Epoch: 9 | Iter: 4100 | Step: 184100 | Train Loss: 0.02223823 |\n",
      "Epoch: 9 | Iter: 4200 | Step: 184200 | Train Loss: 0.00258652 |\n",
      "Epoch: 9 | Iter: 4300 | Step: 184300 | Train Loss: 0.00188237 |\n",
      "Epoch: 9 | Iter: 4400 | Step: 184400 | Train Loss: 0.10583937 |\n",
      "Epoch: 9 | Iter: 4500 | Step: 184500 | Train Loss: 0.01749247 |\n",
      "Epoch: 9 | Iter: 4600 | Step: 184600 | Train Loss: 0.02988701 |\n",
      "Epoch: 9 | Iter: 4700 | Step: 184700 | Train Loss: 0.00088228 |\n",
      "Epoch: 9 | Iter: 4800 | Step: 184800 | Train Loss: 0.00213239 |\n",
      "Epoch: 9 | Iter: 4900 | Step: 184900 | Train Loss: 0.02692462 |\n",
      "Epoch: 9 | Iter: 5000 | Step: 185000 | Train Loss: 0.03146701 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:20, 46.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Iter: 5000 | Step: 185000 | Val Loss: 0.02165252\n",
      "Epoch: 9 | Iter: 5100 | Step: 185100 | Train Loss: 0.00059107 |\n",
      "Epoch: 9 | Iter: 5200 | Step: 185200 | Train Loss: 0.01888059 |\n",
      "Epoch: 9 | Iter: 5300 | Step: 185300 | Train Loss: 0.01078933 |\n",
      "Epoch: 9 | Iter: 5400 | Step: 185400 | Train Loss: 0.02887005 |\n",
      "Epoch: 9 | Iter: 5500 | Step: 185500 | Train Loss: 0.05707163 |\n",
      "Epoch: 9 | Iter: 5600 | Step: 185600 | Train Loss: 0.91486257 |\n",
      "Epoch: 9 | Iter: 5700 | Step: 185700 | Train Loss: 0.03085741 |\n",
      "Epoch: 9 | Iter: 5800 | Step: 185800 | Train Loss: 0.00004373 |\n",
      "Epoch: 9 | Iter: 5900 | Step: 185900 | Train Loss: 0.02122996 |\n",
      "Epoch: 9 | Iter: 6000 | Step: 186000 | Train Loss: 0.03128805 |\n",
      "Epoch: 9 | Iter: 6100 | Step: 186100 | Train Loss: 0.02366346 |\n",
      "Epoch: 9 | Iter: 6200 | Step: 186200 | Train Loss: 0.09398586 |\n",
      "Epoch: 9 | Iter: 6300 | Step: 186300 | Train Loss: 0.00605900 |\n",
      "Epoch: 9 | Iter: 6400 | Step: 186400 | Train Loss: 0.00559253 |\n",
      "Epoch: 9 | Iter: 6500 | Step: 186500 | Train Loss: 0.00001637 |\n",
      "Epoch: 9 | Iter: 6600 | Step: 186600 | Train Loss: 0.01837291 |\n",
      "Epoch: 9 | Iter: 6700 | Step: 186700 | Train Loss: 0.00388415 |\n",
      "Epoch: 9 | Iter: 6800 | Step: 186800 | Train Loss: 0.08554450 |\n",
      "Epoch: 9 | Iter: 6900 | Step: 186900 | Train Loss: 0.58935654 |\n",
      "Epoch: 9 | Iter: 7000 | Step: 187000 | Train Loss: 0.01867290 |\n",
      "Epoch: 9 | Iter: 7100 | Step: 187100 | Train Loss: 0.02823404 |\n",
      "Epoch: 9 | Iter: 7200 | Step: 187200 | Train Loss: 0.00655433 |\n",
      "Epoch: 9 | Iter: 7300 | Step: 187300 | Train Loss: 0.02575648 |\n",
      "Epoch: 9 | Iter: 7400 | Step: 187400 | Train Loss: 0.01713832 |\n",
      "Epoch: 9 | Iter: 7500 | Step: 187500 | Train Loss: 0.00501565 |\n",
      "Epoch: 9 | Iter: 7600 | Step: 187600 | Train Loss: 0.00021769 |\n",
      "Epoch: 9 | Iter: 7700 | Step: 187700 | Train Loss: 0.05596421 |\n",
      "Epoch: 9 | Iter: 7800 | Step: 187800 | Train Loss: 0.00190642 |\n",
      "Epoch: 9 | Iter: 7900 | Step: 187900 | Train Loss: 0.01870560 |\n",
      "Epoch: 9 | Iter: 8000 | Step: 188000 | Train Loss: 0.01899155 |\n",
      "Epoch: 9 | Iter: 8100 | Step: 188100 | Train Loss: 0.01728432 |\n",
      "Epoch: 9 | Iter: 8200 | Step: 188200 | Train Loss: 0.05974976 |\n",
      "Epoch: 9 | Iter: 8300 | Step: 188300 | Train Loss: 0.03564783 |\n",
      "Epoch: 9 | Iter: 8400 | Step: 188400 | Train Loss: 0.00012031 |\n",
      "Epoch: 9 | Iter: 8500 | Step: 188500 | Train Loss: 1.26278484 |\n",
      "Epoch: 9 | Iter: 8600 | Step: 188600 | Train Loss: 0.23786177 |\n",
      "Epoch: 9 | Iter: 8700 | Step: 188700 | Train Loss: 0.06106676 |\n",
      "Epoch: 9 | Iter: 8800 | Step: 188800 | Train Loss: 0.03727039 |\n",
      "Epoch: 9 | Iter: 8900 | Step: 188900 | Train Loss: 0.05428848 |\n",
      "Epoch: 9 | Iter: 9000 | Step: 189000 | Train Loss: 0.03205477 |\n",
      "Epoch: 9 | Iter: 9100 | Step: 189100 | Train Loss: 0.00092263 |\n",
      "Epoch: 9 | Iter: 9200 | Step: 189200 | Train Loss: 0.04422667 |\n",
      "Epoch: 9 | Iter: 9300 | Step: 189300 | Train Loss: 0.05645157 |\n",
      "Epoch: 9 | Iter: 9400 | Step: 189400 | Train Loss: 0.02852647 |\n",
      "Epoch: 9 | Iter: 9500 | Step: 189500 | Train Loss: 0.05340816 |\n",
      "Epoch: 9 | Iter: 9600 | Step: 189600 | Train Loss: 0.27128577 |\n",
      "Epoch: 9 | Iter: 9700 | Step: 189700 | Train Loss: 0.09528873 |\n",
      "Epoch: 9 | Iter: 9800 | Step: 189800 | Train Loss: 0.03289573 |\n",
      "Epoch: 9 | Iter: 9900 | Step: 189900 | Train Loss: 0.01316158 |\n",
      "Epoch: 9 | Iter: 10000 | Step: 190000 | Train Loss: 0.01410642 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:08<03:22, 46.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Iter: 10000 | Step: 190000 | Val Loss: 0.02294962\n",
      "Epoch: 9 | Iter: 10100 | Step: 190100 | Train Loss: 0.01818977 |\n",
      "Epoch: 9 | Iter: 10200 | Step: 190200 | Train Loss: 0.00030598 |\n",
      "Epoch: 9 | Iter: 10300 | Step: 190300 | Train Loss: 0.00607410 |\n",
      "Epoch: 9 | Iter: 10400 | Step: 190400 | Train Loss: 0.08093021 |\n",
      "Epoch: 9 | Iter: 10500 | Step: 190500 | Train Loss: 0.05979888 |\n",
      "Epoch: 9 | Iter: 10600 | Step: 190600 | Train Loss: 0.00026379 |\n",
      "Epoch: 9 | Iter: 10700 | Step: 190700 | Train Loss: 0.05558337 |\n",
      "Epoch: 9 | Iter: 10800 | Step: 190800 | Train Loss: 0.00851479 |\n",
      "Epoch: 9 | Iter: 10900 | Step: 190900 | Train Loss: 0.21559575 |\n",
      "Epoch: 9 | Iter: 11000 | Step: 191000 | Train Loss: 0.03599737 |\n",
      "Epoch: 9 | Iter: 11100 | Step: 191100 | Train Loss: 0.34714410 |\n",
      "Epoch: 9 | Iter: 11200 | Step: 191200 | Train Loss: 0.82248867 |\n",
      "Epoch: 9 | Iter: 11300 | Step: 191300 | Train Loss: 0.01198023 |\n",
      "Epoch: 9 | Iter: 11400 | Step: 191400 | Train Loss: 0.00019370 |\n",
      "Epoch: 9 | Iter: 11500 | Step: 191500 | Train Loss: 0.01967723 |\n",
      "Epoch: 9 | Iter: 11600 | Step: 191600 | Train Loss: 0.00661351 |\n",
      "Epoch: 9 | Iter: 11700 | Step: 191700 | Train Loss: 0.00259302 |\n",
      "Epoch: 9 | Iter: 11800 | Step: 191800 | Train Loss: 0.03989913 |\n",
      "Epoch: 9 | Iter: 11900 | Step: 191900 | Train Loss: 0.00011362 |\n",
      "Epoch: 9 | Iter: 12000 | Step: 192000 | Train Loss: 0.01643631 |\n",
      "Epoch: 9 | Iter: 12100 | Step: 192100 | Train Loss: 0.00067544 |\n",
      "Epoch: 9 | Iter: 12200 | Step: 192200 | Train Loss: 0.05235233 |\n",
      "Epoch: 9 | Iter: 12300 | Step: 192300 | Train Loss: 0.02443683 |\n",
      "Epoch: 9 | Iter: 12400 | Step: 192400 | Train Loss: 0.00009523 |\n",
      "Epoch: 9 | Iter: 12500 | Step: 192500 | Train Loss: 0.04911144 |\n",
      "Epoch: 9 | Iter: 12600 | Step: 192600 | Train Loss: 0.02320725 |\n",
      "Epoch: 9 | Iter: 12700 | Step: 192700 | Train Loss: 0.00005314 |\n",
      "Epoch: 9 | Iter: 12800 | Step: 192800 | Train Loss: 0.00381713 |\n",
      "Epoch: 9 | Iter: 12900 | Step: 192900 | Train Loss: 0.04757317 |\n",
      "Epoch: 9 | Iter: 13000 | Step: 193000 | Train Loss: 0.12785195 |\n",
      "Epoch: 9 | Iter: 13100 | Step: 193100 | Train Loss: 0.01024028 |\n",
      "Epoch: 9 | Iter: 13200 | Step: 193200 | Train Loss: 0.00097657 |\n",
      "Epoch: 9 | Iter: 13300 | Step: 193300 | Train Loss: 0.01218713 |\n",
      "Epoch: 9 | Iter: 13400 | Step: 193400 | Train Loss: 0.04406703 |\n",
      "Epoch: 9 | Iter: 13500 | Step: 193500 | Train Loss: 0.00135040 |\n",
      "Epoch: 9 | Iter: 13600 | Step: 193600 | Train Loss: 0.83406383 |\n",
      "Epoch: 9 | Iter: 13700 | Step: 193700 | Train Loss: 0.02333595 |\n",
      "Epoch: 9 | Iter: 13800 | Step: 193800 | Train Loss: 0.06088287 |\n",
      "Epoch: 9 | Iter: 13900 | Step: 193900 | Train Loss: 0.01765355 |\n",
      "Epoch: 9 | Iter: 14000 | Step: 194000 | Train Loss: 0.50406170 |\n",
      "Epoch: 9 | Iter: 14100 | Step: 194100 | Train Loss: 0.02537933 |\n",
      "Epoch: 9 | Iter: 14200 | Step: 194200 | Train Loss: 0.04868669 |\n",
      "Epoch: 9 | Iter: 14300 | Step: 194300 | Train Loss: 0.00744198 |\n",
      "Epoch: 9 | Iter: 14400 | Step: 194400 | Train Loss: 0.12477202 |\n",
      "Epoch: 9 | Iter: 14500 | Step: 194500 | Train Loss: 0.00127353 |\n",
      "Epoch: 9 | Iter: 14600 | Step: 194600 | Train Loss: 0.00004385 |\n",
      "Epoch: 9 | Iter: 14700 | Step: 194700 | Train Loss: 0.26452091 |\n",
      "Epoch: 9 | Iter: 14800 | Step: 194800 | Train Loss: 0.00047625 |\n",
      "Epoch: 9 | Iter: 14900 | Step: 194900 | Train Loss: 0.00429598 |\n",
      "Epoch: 9 | Iter: 15000 | Step: 195000 | Train Loss: 0.00211091 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 400/9717 [00:09<03:43, 41.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Iter: 15000 | Step: 195000 | Val Loss: 0.02311711\n",
      "Epoch: 9 | Iter: 15100 | Step: 195100 | Train Loss: 0.01697779 |\n",
      "Epoch: 9 | Iter: 15200 | Step: 195200 | Train Loss: 0.03194617 |\n",
      "Epoch: 9 | Iter: 15300 | Step: 195300 | Train Loss: 0.02021872 |\n",
      "Epoch: 9 | Iter: 15400 | Step: 195400 | Train Loss: 0.00018223 |\n",
      "Epoch: 9 | Iter: 15500 | Step: 195500 | Train Loss: 0.02953218 |\n",
      "Epoch: 9 | Iter: 15600 | Step: 195600 | Train Loss: 0.05770582 |\n",
      "Epoch: 9 | Iter: 15700 | Step: 195700 | Train Loss: 0.00372554 |\n",
      "Epoch: 9 | Iter: 15800 | Step: 195800 | Train Loss: 0.00022447 |\n",
      "Epoch: 9 | Iter: 15900 | Step: 195900 | Train Loss: 0.02039076 |\n",
      "Epoch: 9 | Iter: 16000 | Step: 196000 | Train Loss: 0.01451526 |\n",
      "Epoch: 9 | Iter: 16100 | Step: 196100 | Train Loss: 0.25468522 |\n",
      "Epoch: 9 | Iter: 16200 | Step: 196200 | Train Loss: 0.00143164 |\n",
      "Epoch: 9 | Iter: 16300 | Step: 196300 | Train Loss: 0.00786744 |\n",
      "Epoch: 9 | Iter: 16400 | Step: 196400 | Train Loss: 0.05057612 |\n",
      "Epoch: 9 | Iter: 16500 | Step: 196500 | Train Loss: 0.02132276 |\n",
      "Epoch: 9 | Iter: 16600 | Step: 196600 | Train Loss: 0.00019536 |\n",
      "Epoch: 9 | Iter: 16700 | Step: 196700 | Train Loss: 0.00060423 |\n",
      "Epoch: 9 | Iter: 16800 | Step: 196800 | Train Loss: 0.02311443 |\n",
      "Epoch: 9 | Iter: 16900 | Step: 196900 | Train Loss: 0.01402469 |\n",
      "Epoch: 9 | Iter: 17000 | Step: 197000 | Train Loss: 0.10790773 |\n",
      "Epoch: 9 | Iter: 17100 | Step: 197100 | Train Loss: 0.00059508 |\n",
      "Epoch: 9 | Iter: 17200 | Step: 197200 | Train Loss: 0.00510053 |\n",
      "Epoch: 9 | Iter: 17300 | Step: 197300 | Train Loss: 0.47136271 |\n",
      "Epoch: 9 | Iter: 17400 | Step: 197400 | Train Loss: 0.01703257 |\n",
      "Epoch: 9 | Iter: 17500 | Step: 197500 | Train Loss: 0.04054924 |\n",
      "Epoch: 9 | Iter: 17600 | Step: 197600 | Train Loss: 0.16674353 |\n",
      "Epoch: 9 | Iter: 17700 | Step: 197700 | Train Loss: 0.04859848 |\n",
      "Epoch: 9 | Iter: 17800 | Step: 197800 | Train Loss: 0.07898763 |\n",
      "Epoch: 9 | Iter: 17900 | Step: 197900 | Train Loss: 0.01595970 |\n",
      "Epoch: 9 | Iter: 18000 | Step: 198000 | Train Loss: 0.00082041 |\n",
      "Epoch: 9 | Iter: 18100 | Step: 198100 | Train Loss: 0.00345420 |\n",
      "Epoch: 9 | Iter: 18200 | Step: 198200 | Train Loss: 0.03392492 |\n",
      "Epoch: 9 | Iter: 18300 | Step: 198300 | Train Loss: 0.03070045 |\n",
      "Epoch: 9 | Iter: 18400 | Step: 198400 | Train Loss: 0.00810836 |\n",
      "Epoch: 9 | Iter: 18500 | Step: 198500 | Train Loss: 0.00001875 |\n",
      "Epoch: 9 | Iter: 18600 | Step: 198600 | Train Loss: 0.00503388 |\n",
      "Epoch: 9 | Iter: 18700 | Step: 198700 | Train Loss: 0.01915403 |\n",
      "Epoch: 9 | Iter: 18800 | Step: 198800 | Train Loss: 0.78734201 |\n",
      "Epoch: 9 | Iter: 18900 | Step: 198900 | Train Loss: 0.03494685 |\n",
      "Epoch: 9 | Iter: 19000 | Step: 199000 | Train Loss: 0.03742684 |\n",
      "Epoch: 9 | Iter: 19100 | Step: 199100 | Train Loss: 0.07850870 |\n",
      "Epoch: 9 | Iter: 19200 | Step: 199200 | Train Loss: 1.17523408 |\n",
      "Epoch: 9 | Iter: 19300 | Step: 199300 | Train Loss: 0.04781073 |\n",
      "Epoch: 9 | Iter: 19400 | Step: 199400 | Train Loss: 0.02136543 |\n",
      "Epoch: 9 | Iter: 19500 | Step: 199500 | Train Loss: 0.07545157 |\n",
      "Epoch: 9 | Iter: 19600 | Step: 199600 | Train Loss: 0.06518472 |\n",
      "Epoch: 9 | Iter: 19700 | Step: 199700 | Train Loss: 0.03286954 |\n",
      "Epoch: 9 | Iter: 19800 | Step: 199800 | Train Loss: 0.01247472 |\n",
      "Epoch: 9 | Iter: 19900 | Step: 199900 | Train Loss: 0.15611039 |\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model_dir = \"./models/\"\n",
    "step = 0 \n",
    "imgs_per_batch = 40\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(10):\n",
    "    sampler = RandomSampler(train_data, replacement=True, num_samples=20000)\n",
    "    for i, sample_id in enumerate(sampler):\n",
    "        data = train_data[sample_id]\n",
    "        \n",
    "        label = data[\"steering_angle\"]\n",
    "        img_pth, label = choose_image(label)\n",
    "#         print(\"img_pth\" ,img_pth)\n",
    "#         print(\"data[pth]\",data[img_pth])\n",
    "        rectify(data,img_pth)\n",
    "#         print(\"img_pth\" ,img_pth)\n",
    "#         print(\"data[pth]\",data[img_pth])\n",
    "        img = cv2.imread(data[img_pth])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = preprocess(img)\n",
    "        img, label = random_flip(img, label)\n",
    "        img, label = random_translate(img, label, 100, 10)\n",
    "        img = random_shadow(img)\n",
    "        img = random_brightness(img)\n",
    "        img = Variable(torch.cuda.FloatTensor([img]))\n",
    "        label = np.array([label]).astype(float)\n",
    "        label = Variable(torch.cuda.FloatTensor(label))\n",
    "        img = img.permute(0,3,1,2)\n",
    "        \n",
    "        out_vec = model(img)\n",
    "        loss = criterion(out_vec,label)\n",
    "\n",
    "        loss.backward()\n",
    "        if step%imgs_per_batch==0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        if step%100==0:\n",
    "            log_str = \\\n",
    "                'Epoch: {} | Iter: {} | Step: {} | ' + \\\n",
    "                'Train Loss: {:.8f} |'\n",
    "            log_str = log_str.format(\n",
    "                epoch,\n",
    "                i,\n",
    "                step,\n",
    "                loss.item())\n",
    "            print(log_str)\n",
    "\n",
    "#         if step%100==0:\n",
    "#             log_value('train_loss',loss.item(),step)\n",
    "\n",
    "        if step%5000==0:\n",
    "            val_loss = eval_model(model,val_data, num_samples=400)\n",
    "#             log_value('val_loss',val_loss,step)\n",
    "            log_str = \\\n",
    "                'Epoch: {} | Iter: {} | Step: {} | Val Loss: {:.8f}'\n",
    "            log_str = log_str.format(\n",
    "                epoch,\n",
    "                i,\n",
    "                step,\n",
    "                val_loss)\n",
    "            print(log_str)\n",
    "            model.train()\n",
    "\n",
    "        if step%5000==0:\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)\n",
    "\n",
    "            model_pth = os.path.join(\n",
    "                model_dir,\n",
    "                'model_{}'.format(step))\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                model_pth)\n",
    "\n",
    "        step += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5422.621899,
   "end_time": "2022-08-30T22:31:56.879342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-30T21:01:34.257443",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
